{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f56491",
   "metadata": {},
   "source": [
    "# PostgreSQL データベース検査\n",
    "\n",
    "以下のスクリプトを実行して、データベース（`dataset`）の格納状況を確認します。\n",
    "\n",
    "**主な機能:**\n",
    "1. **テーブル一覧取得**: 指定スキーマ（`public`）内の全テーブルをリストアップ\n",
    "2. **件数カウント**: 各テーブルの総レコード数を計算\n",
    "3. **データプレビュー**: 最新データを10件取得し、以下のフォーマットで表示\n",
    "   - 数値のカンマ区切り\n",
    "   - 値の大きさに応じた背景色グラデーション（ヒートマップ）\n",
    "\n",
    "**使用ライブラリ:** `pandas`, `sqlalchemy`, `IPython.display`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dab6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path: /mnt/e/env/ts/tslib/ds/src\n",
      "Files in target path: ['exog', 'anomalies', '3', '10', '__pycache__', 'db_viewer_patch.zip', 'db_viewer_patch.zip:Zone.Identifier', 'base', 'db']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce5c6c052044bd880953d3ccd2ea700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), HTML(value=\"\\n        <div style='background:#ffffff; padding: 8px 10px; border-radiu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine = Engine(postgresql+psycopg://postgres:***@127.0.0.1:5432/foundation_model)\n",
      "db_version = PostgreSQL 16.11 (Ubuntu 16.11-0ubuntu0.24.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# 1. 現在のディレクトリ (tslib) を取得\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 2. 'db' パッケージが入っている親ディレクトリ (ds/src) のパスを作成\n",
    "#    実体: /mnt/e/env/ts/tslib/ds/src/db  -> 追加すべきパス: /mnt/e/env/ts/tslib/ds/src\n",
    "target_path = os.path.join(current_dir, 'ds', 'src')\n",
    "\n",
    "# 3. パスが通っていなければ追加\n",
    "if target_path not in sys.path:\n",
    "    sys.path.append(target_path)\n",
    "\n",
    "# 確認用出力\n",
    "print(f\"Added path: {target_path}\")\n",
    "if os.path.exists(target_path):\n",
    "    print(f\"Files in target path: {os.listdir(target_path)}\") # ここに 'db' が表示されるはず\n",
    "else:\n",
    "    print(\"⚠️ 警告: 指定したパスが存在しません。ディレクトリ構成を確認してください。\")\n",
    "\n",
    "# 4. パスが通った状態でインポート\n",
    "import db.view.db_app\n",
    "importlib.reload(db.view.db_app)\n",
    "from db.view.db_app import DBViewerApp\n",
    "\n",
    "# DB設定\n",
    "DB_HOST = os.environ.get(\"DB_HOST\", \"127.0.0.1\")\n",
    "DB_PORT = os.environ.get(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.environ.get(\"DB_NAME\", \"foundation_model\")\n",
    "DB_USER = os.environ.get(\"DB_USER\", \"postgres\")\n",
    "DB_PASS = os.environ.get(\"DB_PASS\", \"z\")\n",
    "DB_SCHEMA = os.environ.get(\"DB_SCHEMA\", \"public\")\n",
    "\n",
    "# アプリ起動\n",
    "app = DBViewerApp(DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASS, DB_SCHEMA)\n",
    "app.ui.display()\n",
    "\n",
    "print(\"engine =\", getattr(app.state, \"engine\", None))\n",
    "print(\"db_version =\", getattr(app.state, \"db_version\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86a06f",
   "metadata": {},
   "source": [
    "# 0. Notebook Setup\n",
    "\n",
    "この章の目的：\n",
    "- Notebookを上から再実行しても副作用が残りにくい（再現性が高い）\n",
    "- `src/` 配下のコード変更が即反映される（autoreload）\n",
    "- プロジェクトパスが常に通っている（import安定）\n",
    "\n",
    "実行ルール：\n",
    "- この章のセルは、Notebook起動後に最初に必ず実行する\n",
    "- 以後のセルは「この章に依存」する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a85357",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-1. 環境情報とカレント確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9af37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.14 (main, Oct 21 2025, 18:31:21) [GCC 11.2.0]\n",
      "Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "CWD: /mnt/e/env/ts/tslib\n",
      "PROJECT_ROOT exists: True\n",
      "SRC_ROOT exists: True\n",
      "NOTEBOOK_PATH: /mnt/e/env/ts/tslib/neuralforecast.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_PATH = Path(\"/mnt/e/env/ts/tslib/neuralforecast.ipynb\")\n",
    "PROJECT_ROOT  = Path(\"/mnt/e/env/ts/tslib\")\n",
    "SRC_ROOT      = PROJECT_ROOT / \"src\"\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"PROJECT_ROOT exists:\", PROJECT_ROOT.exists())\n",
    "print(\"SRC_ROOT exists:\", SRC_ROOT.exists())\n",
    "print(\"NOTEBOOK_PATH:\", NOTEBOOK_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce3ca7",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-2. src/ を import パスに追加（重複なし）＋ autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97837bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path[0]: /mnt/e/env/ts/tslib/src\n"
     ]
    }
   ],
   "source": [
    "# 1) src を sys.path の先頭に入れて import を安定化（重複は避ける）\n",
    "src_str = str(SRC_ROOT)\n",
    "if src_str not in sys.path:\n",
    "    sys.path.insert(0, src_str)\n",
    "\n",
    "print(\"sys.path[0]:\", sys.path[0])\n",
    "\n",
    "# 2) Notebook上で src 配下の変更を自動反映\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d82114",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-3. “副作用を残しにくい”ための基本設定（乱数・警告・ログ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437c6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 08:03:25 | INFO | nf_app | Notebook logging initialized. SEED=42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ---- 再現性（まずは最小）----\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---- 警告ノイズを抑える（必要に応じて調整）----\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# ---- ログ：Notebook再実行でハンドラが増殖しがちなので、毎回初期化 ----\n",
    "import logging\n",
    "\n",
    "def setup_notebook_logging(level=logging.INFO) -> logging.Logger:\n",
    "    logger = logging.getLogger(\"nf_app\")\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # 既存ハンドラをクリア（再実行しても二重出力にならない）\n",
    "    if logger.handlers:\n",
    "        for h in list(logger.handlers):\n",
    "            logger.removeHandler(h)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        fmt=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "logger = setup_notebook_logging()\n",
    "logger.info(\"Notebook logging initialized. SEED=%s\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26549bc",
   "metadata": {},
   "source": [
    "# 0. Package Bootstrap（nf_app 作成）\n",
    "\n",
    "目的：\n",
    "- `src/nf_app` を Python package として成立させる（import 可能にする）\n",
    "- 最小の `__init__.py` を置いて “import の起点” を作る\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724eb523",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-4. nf_app/__init__.py を作成（コマンド）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53784aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/__init__.py\n",
      "\"\"\"\n",
      "nf_app package.\n",
      "\n",
      "Notebook駆動での実験・運用を支えるアプリ層。\n",
      "- 実体の処理は src/nf_app 以下のモジュールに切り出す\n",
      "- Notebookはオーケストレーション（実行・確認）に集中する\n",
      "\"\"\"\n",
      "\n",
      "__all__ = [\"__version__\"]\n",
      "__version__ = \"0.1.0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "init_path = Path(\"/mnt/e/env/ts/tslib/src/nf_app/__init__.py\")\n",
    "init_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "init_code = '''\\\n",
    "\"\"\"\n",
    "nf_app package.\n",
    "\n",
    "Notebook駆動での実験・運用を支えるアプリ層。\n",
    "- 実体の処理は src/nf_app 以下のモジュールに切り出す\n",
    "- Notebookはオーケストレーション（実行・確認）に集中する\n",
    "\"\"\"\n",
    "\n",
    "__all__ = [\"__version__\"]\n",
    "__version__ = \"0.1.0\"\n",
    "'''\n",
    "\n",
    "init_path.write_text(init_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", init_path)\n",
    "print(init_path.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42654d34",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-5. import 動作確認（autoreloadの効きもここで確認）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b6b92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf_app imported: <module 'nf_app' from '/mnt/e/env/ts/tslib/src/nf_app/__init__.py'>\n",
      "nf_app.__version__: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "import nf_app\n",
    "print(\"nf_app imported:\", nf_app)\n",
    "print(\"nf_app.__version__:\", nf_app.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41183ab4",
   "metadata": {},
   "source": [
    "# 0. 再実行安全のための「リセット関数」\n",
    "\n",
    "Notebookでは同じセルを何度も回すため、次を標準化する：\n",
    "- ロガーの再初期化（ハンドラ増殖防止）\n",
    "- 大きいキャッシュ/グローバル変数を明示的に破棄できる口を用意\n",
    "\n",
    "DB接続などは後工程でここに“閉じる処理”を追加する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d2c18",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-6. リセット関数（後でDB/セッションをここに登録）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d0ed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 08:04:55 | INFO | nf_app | Notebook reset done.\n"
     ]
    }
   ],
   "source": [
    "def notebook_reset():\n",
    "    \"\"\"\n",
    "    Notebookの再実行前に呼ぶ想定のリセット。\n",
    "    後工程で、DB接続/セッション/一時ファイルなどもここで閉じる。\n",
    "    \"\"\"\n",
    "    global logger\n",
    "    logger = setup_notebook_logging()\n",
    "    logger.info(\"Notebook reset done.\")\n",
    "\n",
    "notebook_reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87a2b7",
   "metadata": {},
   "source": [
    "# 0. Config\n",
    "\n",
    "目的：\n",
    "- DB接続URL（dataset/model）・成果物ルート（artifact root）・実行モード（run mode）を一元管理する\n",
    "- `.env` と環境変数で切り替え可能にする（環境変数が優先）\n",
    "- pytestで設定の読み取り・優先順位・妥当性を検証する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549891b",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Config-1. config.py を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb266b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/config.py\n",
      "\\\n",
      "from __future__ import annotations\n",
      "\n",
      "import os\n",
      "from dataclasses import dataclass\n",
      "from enum import Enum\n",
      "from pathlib import Path\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "class RunMode(str, Enum):\n",
      "    \"\"\"\n",
      "    Notebook/CLIでの実行モード。\n",
      "    - DRY_RUN: DB書込みやartifact保存を抑制（読み取りと計算の検証）\n",
      "    - TRAIN_SAVE_PREDICT: 学習→保存→予測→登録まで実施\n",
      "    - TRAIN_PREDICT: 学習→予測（保存はしない）\n",
      "    - PREDICT_ONLY: 既存モデルをロードして予測のみ\n",
      "    \"\"\"\n",
      "    DRY_RUN = \"DRY_RUN\"\n",
      "    TRAIN_SAVE_PREDICT = \"TRAIN_SAVE_PREDICT\"\n",
      "    TRAIN_PREDICT = \"TRAIN_PREDICT\"\n",
      "    PREDICT_ONLY = \"PREDICT_ONLY\"\n",
      "\n",
      "\n",
      "def _parse_bool(v: Optional[str], default: bool = False) -> bool:\n",
      "    if v is None:\n",
      "        return default\n",
      "    s = v.strip().lower()\n",
      "    return s in (\"1\", \"true\", \"t\", \"yes\", \"y\", \"on\")\n",
      "\n",
      "\n",
      "def _load_dotenv_if_present(dotenv_path: Path) -> None:\n",
      "    \"\"\"\n",
      "    `.env` を読み込む。環境変 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "config_path = Path(\"/mnt/e/env/ts/tslib/src/nf_app/config.py\")\n",
    "config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config_code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class RunMode(str, Enum):\n",
    "    \"\"\"\n",
    "    Notebook/CLIでの実行モード。\n",
    "    - DRY_RUN: DB書込みやartifact保存を抑制（読み取りと計算の検証）\n",
    "    - TRAIN_SAVE_PREDICT: 学習→保存→予測→登録まで実施\n",
    "    - TRAIN_PREDICT: 学習→予測（保存はしない）\n",
    "    - PREDICT_ONLY: 既存モデルをロードして予測のみ\n",
    "    \"\"\"\n",
    "    DRY_RUN = \"DRY_RUN\"\n",
    "    TRAIN_SAVE_PREDICT = \"TRAIN_SAVE_PREDICT\"\n",
    "    TRAIN_PREDICT = \"TRAIN_PREDICT\"\n",
    "    PREDICT_ONLY = \"PREDICT_ONLY\"\n",
    "\n",
    "\n",
    "def _parse_bool(v: Optional[str], default: bool = False) -> bool:\n",
    "    if v is None:\n",
    "        return default\n",
    "    s = v.strip().lower()\n",
    "    return s in (\"1\", \"true\", \"t\", \"yes\", \"y\", \"on\")\n",
    "\n",
    "\n",
    "def _load_dotenv_if_present(dotenv_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    `.env` を読み込む。環境変数が既に存在する場合は上書きしない（env優先）。\n",
    "    - python-dotenv があればそれを使う\n",
    "    - なければ最小パーサで読み込む\n",
    "    \"\"\"\n",
    "    if not dotenv_path.exists():\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        from dotenv import load_dotenv  # type: ignore\n",
    "        load_dotenv(dotenv_path=str(dotenv_path), override=False)\n",
    "        return\n",
    "    except Exception:\n",
    "        # フォールバック：KEY=VALUE のみ対応（簡易）\n",
    "        for line in dotenv_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if \"=\" not in line:\n",
    "                continue\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            k = k.strip()\n",
    "            v = v.strip().strip('\"').strip(\"'\")\n",
    "            if k and (k not in os.environ):\n",
    "                os.environ[k] = v\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class NFConfig:\n",
    "    # DB URLs\n",
    "    dataset_db_url: str\n",
    "    model_db_url: str\n",
    "\n",
    "    # artifacts\n",
    "    artifact_root: Path\n",
    "\n",
    "    # run behavior\n",
    "    run_mode: RunMode\n",
    "    log_level: str = \"INFO\"\n",
    "    strict_validation: bool = True\n",
    "\n",
    "    @staticmethod\n",
    "    def from_env(\n",
    "        project_root: Path,\n",
    "        dotenv_filename: str = \".env\",\n",
    "    ) -> \"NFConfig\":\n",
    "        \"\"\"\n",
    "        読み込み優先順位：\n",
    "        1) 既に設定されている環境変数\n",
    "        2) project_root/.env（存在すれば読み込む）\n",
    "        3) デフォルト値（ただしDB URLは必須にするのが安全）\n",
    "        \"\"\"\n",
    "        dotenv_path = project_root / dotenv_filename\n",
    "        _load_dotenv_if_present(dotenv_path)\n",
    "\n",
    "        dataset_db_url = os.environ.get(\"DATASET_DB_URL\", \"\").strip()\n",
    "        model_db_url = os.environ.get(\"MODEL_DB_URL\", \"\").strip()\n",
    "\n",
    "        # artifact root default\n",
    "        artifact_root_str = os.environ.get(\"NF_ARTIFACT_ROOT\", str(project_root / \"artifacts\")).strip()\n",
    "        artifact_root = Path(artifact_root_str)\n",
    "\n",
    "        # run mode default\n",
    "        run_mode_str = os.environ.get(\"NF_RUN_MODE\", RunMode.DRY_RUN.value).strip()\n",
    "        try:\n",
    "            run_mode = RunMode(run_mode_str)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"NF_RUN_MODE is invalid: {run_mode_str}. allowed={list(RunMode)}\") from e\n",
    "\n",
    "        log_level = os.environ.get(\"NF_LOG_LEVEL\", \"INFO\").strip()\n",
    "        strict_validation = _parse_bool(os.environ.get(\"NF_STRICT_VALIDATION\"), default=True)\n",
    "\n",
    "        cfg = NFConfig(\n",
    "            dataset_db_url=dataset_db_url,\n",
    "            model_db_url=model_db_url,\n",
    "            artifact_root=artifact_root,\n",
    "            run_mode=run_mode,\n",
    "            log_level=log_level,\n",
    "            strict_validation=strict_validation,\n",
    "        )\n",
    "\n",
    "        cfg._validate()\n",
    "        return cfg\n",
    "\n",
    "    def _validate(self) -> None:\n",
    "        # DB URLsは“原則必須”。DRY_RUNでもDB読み込みしたいので必須扱いが安全。\n",
    "        if self.strict_validation:\n",
    "            if not self.dataset_db_url:\n",
    "                raise ValueError(\"DATASET_DB_URL is required (empty).\")\n",
    "            if not self.model_db_url:\n",
    "                raise ValueError(\"MODEL_DB_URL is required (empty).\")\n",
    "\n",
    "        # artifact root は作れること（相対も許すが、Notebookでは絶対が安全）\n",
    "        try:\n",
    "            self.artifact_root.mkdir(parents=True, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Cannot create artifact_root: {self.artifact_root}\") from e\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"dataset_db_url\": self.dataset_db_url,\n",
    "            \"model_db_url\": self.model_db_url,\n",
    "            \"artifact_root\": str(self.artifact_root),\n",
    "            \"run_mode\": self.run_mode.value,\n",
    "            \"log_level\": self.log_level,\n",
    "            \"strict_validation\": self.strict_validation,\n",
    "        }\n",
    "'''\n",
    "config_path.write_text(config_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", config_path)\n",
    "print(config_path.read_text(encoding=\"utf-8\")[:800], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b2ea3",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Config-2. Notebookで動作確認（環境変数→読込）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d914b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_db_url': 'postgresql+psycopg://postgres@127.0.0.1:5432/dataset',\n",
       " 'model_db_url': 'postgresql+psycopg://postgres@127.0.0.1:5432/model',\n",
       " 'artifact_root': '/mnt/e/env/ts/tslib/artifacts',\n",
       " 'run_mode': 'DRY_RUN',\n",
       " 'log_level': 'INFO',\n",
       " 'strict_validation': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook内で一時的に設定（本来は .env に書く）\n",
    "os.environ[\"DATASET_DB_URL\"] = \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\"\n",
    "os.environ[\"MODEL_DB_URL\"]   = \"postgresql+psycopg://postgres@127.0.0.1:5432/model\"\n",
    "os.environ[\"NF_ARTIFACT_ROOT\"] = \"/mnt/e/env/ts/tslib/artifacts\"\n",
    "os.environ[\"NF_RUN_MODE\"] = \"DRY_RUN\"\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "\n",
    "cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "cfg.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf5b5c",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Config-4. pytest（tests/test_config.py を作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f24f2201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_config.py\n",
      "\\\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pytest\n",
      "\n",
      "from nf_app.config import NFConfig, RunMode\n",
      "\n",
      "\n",
      "def test_config_env_overrides_dotenv(tmp_path, monkeypatch):\n",
      "    # .env を作る\n",
      "    project_root = tmp_path\n",
      "    dotenv = project_root / \".env\"\n",
      "    dotenv.write_text(\n",
      "        \"\\n\".join([\n",
      "            \"DATASET_DB_URL=postgresql+psycopg://dotenv/dataset\",\n",
      "            \"MODEL_DB_URL=postgresql+psycopg://dotenv/model\",\n",
      "            f\"NF_ARTIFACT_ROOT={project_root / 'artifacts_from_dotenv'}\",\n",
      "            \"NF_RU ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_path = Path(\"/mnt/e/env/ts/tslib/tests/test_config.py\")\n",
    "test_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_code = r'''\\\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pytest\n",
    "\n",
    "from nf_app.config import NFConfig, RunMode\n",
    "\n",
    "\n",
    "def test_config_env_overrides_dotenv(tmp_path, monkeypatch):\n",
    "    # .env を作る\n",
    "    project_root = tmp_path\n",
    "    dotenv = project_root / \".env\"\n",
    "    dotenv.write_text(\n",
    "        \"\\n\".join([\n",
    "            \"DATASET_DB_URL=postgresql+psycopg://dotenv/dataset\",\n",
    "            \"MODEL_DB_URL=postgresql+psycopg://dotenv/model\",\n",
    "            f\"NF_ARTIFACT_ROOT={project_root / 'artifacts_from_dotenv'}\",\n",
    "            \"NF_RUN_MODE=TRAIN_PREDICT\",\n",
    "        ]) + \"\\n\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # env を上書き（env優先のはず）\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://env/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\", \"postgresql+psycopg://env/model\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=project_root)\n",
    "    assert cfg.dataset_db_url.endswith(\"/dataset\")\n",
    "    assert \"env\" in cfg.dataset_db_url\n",
    "    assert cfg.run_mode == RunMode.DRY_RUN\n",
    "\n",
    "\n",
    "def test_config_requires_db_urls_when_strict(tmp_path):\n",
    "    project_root = tmp_path\n",
    "    # env も .env も無い状態\n",
    "    with pytest.raises(ValueError):\n",
    "        NFConfig.from_env(project_root=project_root)\n",
    "\n",
    "\n",
    "def test_config_allows_empty_db_urls_when_not_strict(tmp_path, monkeypatch):\n",
    "    project_root = tmp_path\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"false\")\n",
    "    cfg = NFConfig.from_env(project_root=project_root)\n",
    "    # strict_validationがfalseなら空でも通る（ただし実運用では推奨しない）\n",
    "    assert cfg.strict_validation is False\n",
    "    assert cfg.dataset_db_url == \"\"\n",
    "    assert cfg.model_db_url == \"\"\n",
    "    assert cfg.artifact_root.exists()\n",
    "\n",
    "\n",
    "def test_invalid_run_mode_raises(tmp_path, monkeypatch):\n",
    "    project_root = tmp_path\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://x/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\", \"postgresql+psycopg://x/model\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"NO_SUCH_MODE\")\n",
    "\n",
    "    with pytest.raises(ValueError):\n",
    "        NFConfig.from_env(project_root=project_root)\n",
    "'''\n",
    "test_path.write_text(test_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", test_path)\n",
    "print(test_path.read_text(encoding=\"utf-8\")[:500], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c12b43",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Config-5. pytest 実行（Notebookから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa554807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m____________________ ERROR collecting tests/test_config.py _____________________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/mnt/e/env/ts/tslib/tests/test_config.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_config.py\u001b[0m:6: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mconfig\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m NFConfig, RunMode\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   ModuleNotFoundError: No module named 'nf_app'\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/test_config.py\n",
      "!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m\u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.14s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Notebookのカレントをプロジェクトルートに合わせるのが安全\n",
    "%cd /mnt/e/env/ts/tslib\n",
    "\n",
    "# まずはこのテストだけ\n",
    "!pytest -q tests/test_config.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d610d89",
   "metadata": {},
   "source": [
    "## pytestが `nf_app` を見つけられない問題の修正\n",
    "\n",
    "pytestはNotebookの `sys.path` 設定を引き継がないため、\n",
    "`tests/conftest.py` で `/mnt/e/env/ts/tslib/src` を import パスに追加する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4aad01",
   "metadata": {},
   "source": [
    "## /mnt/e/env/ts/tslib/tests/conftest.py を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fce3f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/conftest.py\n",
      "import sys\n",
      "from pathlib import Path\n",
      "\n",
      "# /mnt/e/env/ts/tslib/tests/conftest.py\n",
      "# pytest起動時に src を import path に追加して、`import nf_app` を可能にする\n",
      "\n",
      "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
      "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
      "\n",
      "src_str = str(SRC_ROOT)\n",
      "if src_str not in sys.path:\n",
      "    sys.path.insert(0, src_str)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "conftest_path = Path(\"/mnt/e/env/ts/tslib/tests/conftest.py\")\n",
    "conftest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "conftest_code = '''\\\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# /mnt/e/env/ts/tslib/tests/conftest.py\n",
    "# pytest起動時に src を import path に追加して、`import nf_app` を可能にする\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
    "\n",
    "src_str = str(SRC_ROOT)\n",
    "if src_str not in sys.path:\n",
    "    sys.path.insert(0, src_str)\n",
    "'''\n",
    "\n",
    "conftest_path.write_text(conftest_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", conftest_path)\n",
    "print(conftest_path.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e859a81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                     [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________ test_config_requires_db_urls_when_strict ___________________\u001b[0m\n",
      "\n",
      "tmp_path = PosixPath('/tmp/pytest-of-az/pytest-0/test_config_requires_db_urls_w0')\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_config_requires_db_urls_when_strict\u001b[39;49;00m(tmp_path):\u001b[90m\u001b[39;49;00m\n",
      "        project_root = tmp_path\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# env も .env も無い状態\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       Failed: DID NOT RAISE <class 'ValueError'>\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_config.py\u001b[0m:37: Failed\n",
      "\u001b[31m\u001b[1m_______________ test_config_allows_empty_db_urls_when_not_strict _______________\u001b[0m\n",
      "\n",
      "tmp_path = PosixPath('/tmp/pytest-of-az/pytest-0/test_config_allows_empty_db_ur0')\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x72818cd844d0>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_config_allows_empty_db_urls_when_not_strict\u001b[39;49;00m(tmp_path, monkeypatch):\u001b[90m\u001b[39;49;00m\n",
      "        project_root = tmp_path\u001b[90m\u001b[39;49;00m\n",
      "        monkeypatch.setenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mNF_STRICT_VALIDATION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        cfg = NFConfig.from_env(project_root=project_root)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# strict_validationがfalseなら空でも通る（ただし実運用では推奨しない）\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m cfg.strict_validation \u001b[95mis\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m cfg.dataset_db_url == \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'postgresql+p...:5432/dataset' == ''\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         + postgresql+psycopg://postgres@127.0.0.1:5432/dataset\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_config.py\u001b[0m:47: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_config.py::\u001b[1mtest_config_requires_db_urls_when_strict\u001b[0m - Failed: DID NOT RAISE <class 'ValueError'>\n",
      "\u001b[31mFAILED\u001b[0m tests/test_config.py::\u001b[1mtest_config_allows_empty_db_urls_when_not_strict\u001b[0m - AssertionError: assert 'postgresql+p...:5432/dataset' == ''\n",
      "\u001b[31m\u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_config.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c51b4f",
   "metadata": {},
   "source": [
    "## pytestの環境変数リークを防ぐ\n",
    "\n",
    "Notebookやシェルで設定された環境変数がpytestに残り、テストが想定する状態（空のenv）とズレる。\n",
    "`tests/conftest.py` に autouse fixture を追加して、各テスト開始時に設定系の環境変数を必ずクリアする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398c344",
   "metadata": {},
   "source": [
    "## ✅ codecell: conftest.py を上書き（sys.path追加＋envクリア）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7460201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/conftest.py\n",
      "import sys\n",
      "from pathlib import Path\n",
      "import pytest\n",
      "\n",
      "# pytest起動時に src を import path に追加して、`import nf_app` を可能にする\n",
      "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
      "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
      "\n",
      "src_str = str(SRC_ROOT)\n",
      "if src_str not in sys.path:\n",
      "    sys.path.insert(0, src_str)\n",
      "\n",
      "# テストは「環境変数が汚れていない」前提にしたいので毎回クリアする\n",
      "_CONFIG_ENV_KEYS = [\n",
      "    \"DATASET_DB_URL\",\n",
      "    \"MODEL_DB_URL\",\n",
      "    \"NF_ARTIFACT_ROOT\",\n",
      "    \"NF_RUN_MODE\",\n",
      "    \"NF_STRICT_VALIDATION\",\n",
      "    \"NF_LOG_LEVEL\",\n",
      "]\n",
      "\n",
      "@pytest.fixture(autouse=True)\n",
      "def _clean_config_env(monkeypatch):\n",
      "    for k in _CONFIG_ENV_KEYS:\n",
      "        monkeypatch.delenv(k, raising=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "conftest_path = Path(\"/mnt/e/env/ts/tslib/tests/conftest.py\")\n",
    "conftest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "conftest_code = '''\\\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pytest\n",
    "\n",
    "# pytest起動時に src を import path に追加して、`import nf_app` を可能にする\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
    "\n",
    "src_str = str(SRC_ROOT)\n",
    "if src_str not in sys.path:\n",
    "    sys.path.insert(0, src_str)\n",
    "\n",
    "# テストは「環境変数が汚れていない」前提にしたいので毎回クリアする\n",
    "_CONFIG_ENV_KEYS = [\n",
    "    \"DATASET_DB_URL\",\n",
    "    \"MODEL_DB_URL\",\n",
    "    \"NF_ARTIFACT_ROOT\",\n",
    "    \"NF_RUN_MODE\",\n",
    "    \"NF_STRICT_VALIDATION\",\n",
    "    \"NF_LOG_LEVEL\",\n",
    "]\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def _clean_config_env(monkeypatch):\n",
    "    for k in _CONFIG_ENV_KEYS:\n",
    "        monkeypatch.delenv(k, raising=False)\n",
    "'''\n",
    "conftest_path.write_text(conftest_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", conftest_path)\n",
    "print(conftest_path.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78361b28",
   "metadata": {},
   "source": [
    "## ✅ codecell: pytest 再実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1fb3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                     [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_config.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94444b79",
   "metadata": {},
   "source": [
    "# 0. Logging（run_id付き）\n",
    "\n",
    "目的：\n",
    "- すべてのログ行に `run_id` を含める（実験・実行単位の追跡を可能にする）\n",
    "- Notebookの再実行でもログが二重出力にならない（ハンドラ増殖防止）\n",
    "- pytestで「run_idが必ず入る」を担保する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dba2fc",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Logging-1. nf_app/logging.py を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "793c3cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/logging.py\n",
      "\\\n",
      "from __future__ import annotations\n",
      "\n",
      "import logging\n",
      "import uuid\n",
      "from dataclasses import dataclass\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "RUN_ID_MDC_KEY = \"run_id\"\n",
      "\n",
      "\n",
      "class RunIdFilter(logging.Filter):\n",
      "    \"\"\"\n",
      "    LogRecordに run_id を注入するフィルタ。\n",
      "    formatterで %(run_id)s を参照できるようにする。\n",
      "    \"\"\"\n",
      "    def __init__(self, run_id: str):\n",
      "        super().__init__()\n",
      "        self.run_id = run_id\n",
      "\n",
      "    def filter(self, record: logging.LogRecord) -> bool:\n",
      "        # 既に run_id が明示されている場合は尊重\n",
      "        if not hasattr(record, RUN_ID_MDC_KEY):\n",
      "            setattr(record, RUN_ID_MDC_KEY, self.run_id)\n",
      "        return True\n",
      "\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class LoggerSpec:\n",
      "    name: str = \"nf_app\"\n",
      "    level: int = logging.INFO\n",
      "    fmt: str = \"%(asctime)s | %(levelname)s | %(name)s | run_id=%(run_id)s | %(message)s\"\n",
      "    datefmt: str = \"%Y-%m-%d %H:%M:%S\"\n",
      "\n",
      "\n",
      "def new_run_id() -> str:\n",
      "    return str(uuid.uuid4())\n",
      "\n",
      "\n",
      "def get_logger(\n",
      "    *,\n",
      "  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "logging_path = Path(\"/mnt/e/env/ts/tslib/src/nf_app/logging.py\")\n",
    "logging_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging_code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "RUN_ID_MDC_KEY = \"run_id\"\n",
    "\n",
    "\n",
    "class RunIdFilter(logging.Filter):\n",
    "    \"\"\"\n",
    "    LogRecordに run_id を注入するフィルタ。\n",
    "    formatterで %(run_id)s を参照できるようにする。\n",
    "    \"\"\"\n",
    "    def __init__(self, run_id: str):\n",
    "        super().__init__()\n",
    "        self.run_id = run_id\n",
    "\n",
    "    def filter(self, record: logging.LogRecord) -> bool:\n",
    "        # 既に run_id が明示されている場合は尊重\n",
    "        if not hasattr(record, RUN_ID_MDC_KEY):\n",
    "            setattr(record, RUN_ID_MDC_KEY, self.run_id)\n",
    "        return True\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LoggerSpec:\n",
    "    name: str = \"nf_app\"\n",
    "    level: int = logging.INFO\n",
    "    fmt: str = \"%(asctime)s | %(levelname)s | %(name)s | run_id=%(run_id)s | %(message)s\"\n",
    "    datefmt: str = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "\n",
    "def new_run_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "def get_logger(\n",
    "    *,\n",
    "    run_id: Optional[str] = None,\n",
    "    spec: LoggerSpec = LoggerSpec(),\n",
    ") -> logging.Logger:\n",
    "    \"\"\"\n",
    "    run_id付きのロガーを取得する。\n",
    "    - Notebook再実行でハンドラが増殖しないように毎回クリア\n",
    "    - run_idはFilterで注入\n",
    "    \"\"\"\n",
    "    if run_id is None:\n",
    "        run_id = new_run_id()\n",
    "\n",
    "    logger = logging.getLogger(spec.name)\n",
    "    logger.setLevel(spec.level)\n",
    "\n",
    "    # ハンドラ増殖防止\n",
    "    if logger.handlers:\n",
    "        for h in list(logger.handlers):\n",
    "            logger.removeHandler(h)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setLevel(spec.level)\n",
    "    handler.setFormatter(logging.Formatter(fmt=spec.fmt, datefmt=spec.datefmt))\n",
    "\n",
    "    # 既存フィルタが残らないように毎回クリア→付け直し\n",
    "    if logger.filters:\n",
    "        for f in list(logger.filters):\n",
    "            logger.removeFilter(f)\n",
    "\n",
    "    logger.addFilter(RunIdFilter(run_id))\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "\n",
    "    return logger\n",
    "'''\n",
    "logging_path.write_text(logging_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", logging_path)\n",
    "print(logging_path.read_text(encoding=\"utf-8\")[:900], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56226dcc",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Logging-2. Notebookで動作確認（run_idが全行に入る）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdd6126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 08:13:09 | INFO | nf_app | run_id=b8a2833a-56f5-4974-8c8d-d94fa48e2fe2 | hello\n",
      "2026-01-15 08:13:09 | WARNING | nf_app | run_id=b8a2833a-56f5-4974-8c8d-d94fa48e2fe2 | warn\n",
      "2026-01-15 08:13:09 | ERROR | nf_app | run_id=b8a2833a-56f5-4974-8c8d-d94fa48e2fe2 | error\n",
      "2026-01-15 08:13:09 | INFO | nf_app | run_id=b8a2833a-56f5-4974-8c8d-d94fa48e2fe2 | logger recreated (should appear once)\n"
     ]
    }
   ],
   "source": [
    "from nf_app.logging import get_logger, new_run_id\n",
    "\n",
    "rid = new_run_id()\n",
    "logger = get_logger(run_id=rid)\n",
    "\n",
    "logger.info(\"hello\")\n",
    "logger.warning(\"warn\")\n",
    "logger.error(\"error\")\n",
    "\n",
    "# Notebook再実行や再取得でも二重出力しないことを確認\n",
    "logger2 = get_logger(run_id=rid)\n",
    "logger2.info(\"logger recreated (should appear once)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b660b9",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Logging-3. pytest（tests/test_logging.py を作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce5fd60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_logging.py\n",
      "\\\n",
      "import re\n",
      "from nf_app.logging import get_logger\n",
      "\n",
      "def test_logger_injects_run_id(caplog):\n",
      "    run_id = \"TEST-RUN-ID-123\"\n",
      "    logger = get_logger(run_id=run_id)\n",
      "\n",
      "    # caplogはroot側に集約するので、logger名に縛らずメッセージにrun_idが入るかを見る\n",
      "    # ただし本実装はFormatterに run_id を入れているので、handler出力を検査したい。\n",
      "    # pytestのcaplogは標準ではhandlerのフォーマット文字列まで捕まえないことがあるため、\n",
      "    # LogRecord属性として run_id が付与されていることを確認する。\n",
      "    with caplog.at_level(\"INFO\"):\n",
      "        logger.info(\"hello\")\n",
      "\n",
      "    # recordに run_id が付いていること\n",
      "    assert any(getattr(r, \"run_id\", None) == run_id for r in caplog.records)\n",
      "\n",
      "def test_logger_does_not_duplicate_handlers():\n",
      "    run_id = \"RID\"\n",
      "    logger = get_logger(run_id=run_id)\n",
      "    n1 = len(logger.handlers)\n",
      "\n",
      "    # 再取得してもハンドラ数が増えない\n",
      "    logger2 = get_logger(run_id=run_id)\n",
      "    n2 = len(logger2.handlers)\n",
      "\n",
      "    assert n1 == 1\n",
      "    assert n2 == 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_logging_path = Path(\"/mnt/e/env/ts/tslib/tests/test_logging.py\")\n",
    "test_logging_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_logging_code = r'''\\\n",
    "import re\n",
    "from nf_app.logging import get_logger\n",
    "\n",
    "def test_logger_injects_run_id(caplog):\n",
    "    run_id = \"TEST-RUN-ID-123\"\n",
    "    logger = get_logger(run_id=run_id)\n",
    "\n",
    "    # caplogはroot側に集約するので、logger名に縛らずメッセージにrun_idが入るかを見る\n",
    "    # ただし本実装はFormatterに run_id を入れているので、handler出力を検査したい。\n",
    "    # pytestのcaplogは標準ではhandlerのフォーマット文字列まで捕まえないことがあるため、\n",
    "    # LogRecord属性として run_id が付与されていることを確認する。\n",
    "    with caplog.at_level(\"INFO\"):\n",
    "        logger.info(\"hello\")\n",
    "\n",
    "    # recordに run_id が付いていること\n",
    "    assert any(getattr(r, \"run_id\", None) == run_id for r in caplog.records)\n",
    "\n",
    "def test_logger_does_not_duplicate_handlers():\n",
    "    run_id = \"RID\"\n",
    "    logger = get_logger(run_id=run_id)\n",
    "    n1 = len(logger.handlers)\n",
    "\n",
    "    # 再取得してもハンドラ数が増えない\n",
    "    logger2 = get_logger(run_id=run_id)\n",
    "    n2 = len(logger2.handlers)\n",
    "\n",
    "    assert n1 == 1\n",
    "    assert n2 == 1\n",
    "'''\n",
    "test_logging_path.write_text(test_logging_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", test_logging_path)\n",
    "print(test_logging_path.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8ca1d",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Logging-4. pytest 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbe08e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                       [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ test_logger_injects_run_id __________________________\u001b[0m\n",
      "\n",
      "caplog = <_pytest.logging.LogCaptureFixture object at 0x7b2098005790>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_logger_injects_run_id\u001b[39;49;00m(caplog):\u001b[90m\u001b[39;49;00m\n",
      "        run_id = \u001b[33m\"\u001b[39;49;00m\u001b[33mTEST-RUN-ID-123\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        logger = get_logger(run_id=run_id)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# caplogはroot側に集約するので、logger名に縛らずメッセージにrun_idが入るかを見る\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# ただし本実装はFormatterに run_id を入れているので、handler出力を検査したい。\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# pytestのcaplogは標準ではhandlerのフォーマット文字列まで捕まえないことがあるため、\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# LogRecord属性として run_id が付与されていることを確認する。\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m caplog.at_level(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mhello\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# recordに run_id が付いていること\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(\u001b[96mgetattr\u001b[39;49;00m(r, \u001b[33m\"\u001b[39;49;00m\u001b[33mrun_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m) == run_id \u001b[94mfor\u001b[39;49;00m r \u001b[95min\u001b[39;49;00m caplog.records)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = any(<generator object test_logger_injects_run_id.<locals>.<genexpr> at 0x7b2097f87f10>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_logging.py\u001b[0m:17: AssertionError\n",
      "----------------------------- Captured stderr call -----------------------------\n",
      "2026-01-15 08:13:40 | INFO | nf_app | run_id=TEST-RUN-ID-123 | hello\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_logging.py::\u001b[1mtest_logger_injects_run_id\u001b[0m - assert False\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.05s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_logging.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464966c",
   "metadata": {},
   "source": [
    "### pytestのログ捕捉を修正\n",
    "\n",
    "`nf_app` ロガーは propagate=False なので caplog(root捕捉)ではrecordsが取れない。\n",
    "テストではロガーに一時ハンドラを付け、LogRecordの run_id 注入を直接検証する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e1109",
   "metadata": {},
   "source": [
    "## ✅ codecell: tests/test_logging.py を修正（上書き）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c49e1c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_logging.py\n",
      "\\\n",
      "import logging\n",
      "from nf_app.logging import get_logger\n",
      "\n",
      "class ListHandler(logging.Handler):\n",
      "    \"\"\"LogRecordを収集するだけのテスト用ハンドラ\"\"\"\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.records = []\n",
      "\n",
      "    def emit(self, record: logging.LogRecord) -> None:\n",
      "        self.records.append(record)\n",
      "\n",
      "\n",
      "def test_logger_injects_run_id():\n",
      "    run_id = \"TEST-RUN-ID-123\"\n",
      "    logger = get_logger(run_id=run_id)\n",
      "\n",
      "    lh = ListHandler()\n",
      "    logger.addHandler(lh)\n",
      "\n",
      "    logger.info(\"hello\")\n",
      "\n",
      "    assert len(lh.records) >= 1\n",
      "    assert getattr(lh.records[0], \"run_id\", None) == run_id\n",
      "\n",
      "\n",
      "def test_logger_does_not_duplicate_handlers():\n",
      "    run_id = \"RID\"\n",
      "    logger = get_logger(run_id=run_id)\n",
      "    n1 = len(logger.handlers)\n",
      "\n",
      "    # 再取得してもハンドラ数が増えない（get_logger内で毎回クリアしている想定）\n",
      "    logger2 = get_logger(run_id=run_id)\n",
      "    n2 = len(logger2.handlers)\n",
      "\n",
      "    assert n1 == 1\n",
      "    assert n2 == 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_logging_path = Path(\"/mnt/e/env/ts/tslib/tests/test_logging.py\")\n",
    "\n",
    "test_logging_code = r'''\\\n",
    "import logging\n",
    "from nf_app.logging import get_logger\n",
    "\n",
    "class ListHandler(logging.Handler):\n",
    "    \"\"\"LogRecordを収集するだけのテスト用ハンドラ\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.records = []\n",
    "\n",
    "    def emit(self, record: logging.LogRecord) -> None:\n",
    "        self.records.append(record)\n",
    "\n",
    "\n",
    "def test_logger_injects_run_id():\n",
    "    run_id = \"TEST-RUN-ID-123\"\n",
    "    logger = get_logger(run_id=run_id)\n",
    "\n",
    "    lh = ListHandler()\n",
    "    logger.addHandler(lh)\n",
    "\n",
    "    logger.info(\"hello\")\n",
    "\n",
    "    assert len(lh.records) >= 1\n",
    "    assert getattr(lh.records[0], \"run_id\", None) == run_id\n",
    "\n",
    "\n",
    "def test_logger_does_not_duplicate_handlers():\n",
    "    run_id = \"RID\"\n",
    "    logger = get_logger(run_id=run_id)\n",
    "    n1 = len(logger.handlers)\n",
    "\n",
    "    # 再取得してもハンドラ数が増えない（get_logger内で毎回クリアしている想定）\n",
    "    logger2 = get_logger(run_id=run_id)\n",
    "    n2 = len(logger2.handlers)\n",
    "\n",
    "    assert n1 == 1\n",
    "    assert n2 == 1\n",
    "'''\n",
    "test_logging_path.write_text(test_logging_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", test_logging_path)\n",
    "print(test_logging_path.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eaf677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e303cc",
   "metadata": {},
   "source": [
    "## ✅ codecell: pytest 再実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bd90ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                       [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_logging.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7991e61",
   "metadata": {},
   "source": [
    "# 0. Errors（例外ポリシー：Data / DB / Model）\n",
    "\n",
    "目的：\n",
    "- 失敗を「原因カテゴリ」で分類する（Data / DB / Model / Config / Unknown）\n",
    "- 例外をDBに保存できる形（dict/JSON）へ正規化する\n",
    "- run_id を含む context（状況情報）を一緒に持てるようにする\n",
    "- pytestで分類・シリアライズが担保される\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac8f4d",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Errors-1. nf_app/errors.py を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df896554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/errors.py\n",
      "\\\n",
      "from __future__ import annotations\n",
      "\n",
      "import traceback as tb\n",
      "from dataclasses import dataclass, field\n",
      "from enum import Enum\n",
      "from typing import Any, Dict, Optional\n",
      "\n",
      "\n",
      "class ErrorType(str, Enum):\n",
      "    DATA = \"DATA\"        # データ品質、欠損、リーク、型不整合など\n",
      "    DB = \"DB\"            # DB接続/SQL/整合性制約など\n",
      "    MODEL = \"MODEL\"      # 学習/推論/保存/ロード、パラメータ不正など\n",
      "    CONFIG = \"CONFIG\"    # 設定値不正（env/.env等）\n",
      "    UNKNOWN = \"UNKNOWN\"  # 未分類\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class NFError(Exception):\n",
      "    \"\"\"\n",
      "    DBに保存できるように構造化した例外。\n",
      "    - error_type: 失敗カテゴリ\n",
      "    - error_code: 文字列コード（集計・アラートに使う）\n",
      "    - message: 人間向けメッセージ\n",
      "    - context: JSON化可能な補足情報（run_id, table名, unique_idなど）\n",
      "    - cause: 元例外（例外チェーン）\n",
      "    \"\"\"\n",
      "    error_type: ErrorType\n",
      "    error_code: str\n",
      "    message: str\n",
      "    context: Dict[str, Any] = field(default_factory=dict)\n",
      "    cause: Optional[BaseException] = None\n",
      "\n",
      "    def __post_init__(self):\n",
      "        super().__init__(self.message)\n",
      "\n",
      "    def to ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "errors_path = Path(\"/mnt/e/env/ts/tslib/src/nf_app/errors.py\")\n",
    "errors_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "errors_code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "import traceback as tb\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "\n",
    "class ErrorType(str, Enum):\n",
    "    DATA = \"DATA\"        # データ品質、欠損、リーク、型不整合など\n",
    "    DB = \"DB\"            # DB接続/SQL/整合性制約など\n",
    "    MODEL = \"MODEL\"      # 学習/推論/保存/ロード、パラメータ不正など\n",
    "    CONFIG = \"CONFIG\"    # 設定値不正（env/.env等）\n",
    "    UNKNOWN = \"UNKNOWN\"  # 未分類\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NFError(Exception):\n",
    "    \"\"\"\n",
    "    DBに保存できるように構造化した例外。\n",
    "    - error_type: 失敗カテゴリ\n",
    "    - error_code: 文字列コード（集計・アラートに使う）\n",
    "    - message: 人間向けメッセージ\n",
    "    - context: JSON化可能な補足情報（run_id, table名, unique_idなど）\n",
    "    - cause: 元例外（例外チェーン）\n",
    "    \"\"\"\n",
    "    error_type: ErrorType\n",
    "    error_code: str\n",
    "    message: str\n",
    "    context: Dict[str, Any] = field(default_factory=dict)\n",
    "    cause: Optional[BaseException] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__init__(self.message)\n",
    "\n",
    "    def to_record(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        runsテーブル等にそのまま入れられる形式。\n",
    "        （SQLAlchemy/psycopgならJSONBにも入れやすい）\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"error_type\": self.error_type.value,\n",
    "            \"error_code\": self.error_code,\n",
    "            \"error_message\": self.message,\n",
    "            \"error_context\": self.context,\n",
    "            \"cause_type\": type(self.cause).__name__ if self.cause else None,\n",
    "            \"cause_message\": str(self.cause) if self.cause else None,\n",
    "            \"traceback\": self.format_traceback(),\n",
    "        }\n",
    "\n",
    "    def format_traceback(self) -> str:\n",
    "        # 自分自身がraiseされた位置のstackも残したいが、\n",
    "        # causeがあればcauseのtraceback優先で採る運用が多い\n",
    "        if self.cause is None:\n",
    "            return \"\".join(tb.format_exception(type(self), self, self.__traceback__))\n",
    "        return \"\".join(tb.format_exception(type(self.cause), self.cause, self.cause.__traceback__))\n",
    "\n",
    "\n",
    "# --- 便利コンストラクタ（統一したコード体系にしやすい） ---\n",
    "def data_error(code: str, message: str, *, context: Optional[Dict[str, Any]] = None, cause: Optional[BaseException] = None) -> NFError:\n",
    "    return NFError(ErrorType.DATA, code, message, context=context or {}, cause=cause)\n",
    "\n",
    "def db_error(code: str, message: str, *, context: Optional[Dict[str, Any]] = None, cause: Optional[BaseException] = None) -> NFError:\n",
    "    return NFError(ErrorType.DB, code, message, context=context or {}, cause=cause)\n",
    "\n",
    "def model_error(code: str, message: str, *, context: Optional[Dict[str, Any]] = None, cause: Optional[BaseException] = None) -> NFError:\n",
    "    return NFError(ErrorType.MODEL, code, message, context=context or {}, cause=cause)\n",
    "\n",
    "def config_error(code: str, message: str, *, context: Optional[Dict[str, Any]] = None, cause: Optional[BaseException] = None) -> NFError:\n",
    "    return NFError(ErrorType.CONFIG, code, message, context=context or {}, cause=cause)\n",
    "\n",
    "\n",
    "def classify_exception(ex: BaseException) -> NFError:\n",
    "    \"\"\"\n",
    "    既存の例外をNFErrorへ分類ラップする。\n",
    "    例：SQLAlchemyError, psycopg.Error, ValueError(データ型)などを分類したい。\n",
    "    ※後工程で依存ライブラリが増えるので、ここは“拡張ポイント”。\n",
    "    \"\"\"\n",
    "    # すでにNFErrorならそのまま\n",
    "    if isinstance(ex, NFError):\n",
    "        return ex\n",
    "\n",
    "    # DB系：psycopg/SQLAlchemyに依存したいが、ここでは文字列ベースの安全分類に留める\n",
    "    name = type(ex).__name__.lower()\n",
    "    msg = str(ex)\n",
    "\n",
    "    if \"psycopg\" in name or \"sqlalchemy\" in name or \"database\" in name or \"connection\" in msg.lower():\n",
    "        return db_error(\"DB_UNKNOWN\", msg, cause=ex)\n",
    "\n",
    "    # Data系っぽい\n",
    "    if isinstance(ex, (KeyError, TypeError)) or \"nan\" in msg.lower() or \"null\" in msg.lower():\n",
    "        return data_error(\"DATA_UNKNOWN\", msg, cause=ex)\n",
    "\n",
    "    # Config系っぽい\n",
    "    if isinstance(ex, ValueError) and (\"NF_\" in msg or \"DATASET_DB_URL\" in msg or \"MODEL_DB_URL\" in msg):\n",
    "        return config_error(\"CONFIG_INVALID\", msg, cause=ex)\n",
    "\n",
    "    return NFError(ErrorType.UNKNOWN, \"UNKNOWN\", msg, cause=ex)\n",
    "'''\n",
    "errors_path.write_text(errors_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", errors_path)\n",
    "print(errors_path.read_text(encoding=\"utf-8\")[:900], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295d35f",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Errors-2. Notebookで動作確認（分類→to_record）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20bdc9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ErrorType.UNKNOWN UNKNOWN\n",
      "error_type => UNKNOWN\n",
      "error_code => UNKNOWN\n",
      "error_message => division by zero\n",
      "cause_type => ZeroDivisionError\n",
      "traceback => Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_58523/3620536958.py\", ...\n"
     ]
    }
   ],
   "source": [
    "from nf_app.errors import data_error, db_error, model_error, classify_exception\n",
    "\n",
    "try:\n",
    "    1 / 0\n",
    "except Exception as e:\n",
    "    nferr = classify_exception(e)\n",
    "    print(nferr.error_type, nferr.error_code)\n",
    "    rec = nferr.to_record()\n",
    "    # DBへそのまま入る形（dict）になっている\n",
    "    for k in [\"error_type\", \"error_code\", \"error_message\", \"cause_type\", \"traceback\"]:\n",
    "        print(k, \"=>\", (rec[k][:80] + \"...\") if isinstance(rec[k], str) and len(rec[k]) > 80 else rec[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5872bf6",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Errors-3. pytest（tests/test_errors.py 作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30117cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_errors.py\n",
      "\\\n",
      "from nf_app.errors import (\n",
      "    NFError, ErrorType,\n",
      "    data_error, db_error, model_error, config_error,\n",
      "    classify_exception\n",
      ")\n",
      "\n",
      "def test_nferror_to_record_has_required_fields():\n",
      "    err = data_error(\"DATA_X\", \"bad data\", context={\"run_id\": \"RID\"})\n",
      "    rec = err.to_record()\n",
      "    assert rec[\"error_type\"] == ErrorType.DATA.value\n",
      "    assert rec[\"error_code\"] == \"DATA_X\"\n",
      "    assert rec[\"error_message\"] == \"bad data\"\n",
      "    assert rec[\"error_context\"][\"run_id\"] == \"RID\"\n",
      "    assert \"traceback\" in rec\n",
      "\n",
      "def test_classify_exception_wraps_non_nferror():\n",
      "    try:\n",
      "        1 / 0\n",
      "    except Exception as e:\n",
      "        wrapped = classify_exception(e)\n",
      "\n",
      "    assert isinstance(wrapped, NFError)\n",
      "    # ゼロ除算はここではUNKNOWN分類でよい（後で拡張してもOK）\n",
      "    assert wrapped.error_type in (ErrorType.UNKNOWN,)\n",
      "\n",
      "def test_factories_set_error_type():\n",
      "    assert data_error(\"A\", \"m\").error_type == ErrorType.DATA\n",
      "    assert db_error(\"A\", \"m\").error_type == ErrorType.DB\n",
      "    assert model_error(\"A\", \"m\").error_type == ErrorType.MODEL\n",
      "    assert config_error(\"A\", \"m\").error_type == ErrorType.CONFIG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_errors_path = Path(\"/mnt/e/env/ts/tslib/tests/test_errors.py\")\n",
    "test_errors_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_errors_code = r'''\\\n",
    "from nf_app.errors import (\n",
    "    NFError, ErrorType,\n",
    "    data_error, db_error, model_error, config_error,\n",
    "    classify_exception\n",
    ")\n",
    "\n",
    "def test_nferror_to_record_has_required_fields():\n",
    "    err = data_error(\"DATA_X\", \"bad data\", context={\"run_id\": \"RID\"})\n",
    "    rec = err.to_record()\n",
    "    assert rec[\"error_type\"] == ErrorType.DATA.value\n",
    "    assert rec[\"error_code\"] == \"DATA_X\"\n",
    "    assert rec[\"error_message\"] == \"bad data\"\n",
    "    assert rec[\"error_context\"][\"run_id\"] == \"RID\"\n",
    "    assert \"traceback\" in rec\n",
    "\n",
    "def test_classify_exception_wraps_non_nferror():\n",
    "    try:\n",
    "        1 / 0\n",
    "    except Exception as e:\n",
    "        wrapped = classify_exception(e)\n",
    "\n",
    "    assert isinstance(wrapped, NFError)\n",
    "    # ゼロ除算はここではUNKNOWN分類でよい（後で拡張してもOK）\n",
    "    assert wrapped.error_type in (ErrorType.UNKNOWN,)\n",
    "\n",
    "def test_factories_set_error_type():\n",
    "    assert data_error(\"A\", \"m\").error_type == ErrorType.DATA\n",
    "    assert db_error(\"A\", \"m\").error_type == ErrorType.DB\n",
    "    assert model_error(\"A\", \"m\").error_type == ErrorType.MODEL\n",
    "    assert config_error(\"A\", \"m\").error_type == ErrorType.CONFIG\n",
    "'''\n",
    "test_errors_path.write_text(test_errors_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", test_errors_path)\n",
    "print(test_errors_path.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febee39a",
   "metadata": {},
   "source": [
    "## ✅ codecell: 0-Errors-4. pytest 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "653b3b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                      [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_errors.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca768fb8",
   "metadata": {},
   "source": [
    "# 1. DB Connect（SQLAlchemy + psycopg）\n",
    "\n",
    "目的：\n",
    "- dataset DB / model DB の両方に接続できることを確認する（Smoke）\n",
    "- Notebookでもpytestでも同じコードで動く\n",
    "- 再実行時に接続プールが残り続けないよう、明示的にdisposeできる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7381e5",
   "metadata": {},
   "source": [
    "# ✅ codecell: 1-DB-1. nf_app/db.py を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/db.py\n",
      "\\\n",
      "from __future__ import annotations\n",
      "\n",
      "from dataclasses import dataclass\n",
      "from enum import Enum\n",
      "from typing import Optional, Tuple\n",
      "\n",
      "from sqlalchemy import create_engine, text\n",
      "from sqlalchemy.engine import Engine\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "\n",
      "from nf_app.errors import db_error, NFError\n",
      "from nf_app.config import NFConfig\n",
      "\n",
      "\n",
      "class DbKind(str, Enum):\n",
      "    DATASET = \"dataset\"\n",
      "    MODEL = \"model\"\n",
      "\n",
      "\n",
      "def _make_engine(db_url: str, *, echo: bool = False) -> Engine:\n",
      "    \"\"\"\n",
      "    SQLAlchemy Engine を生成する。\n",
      "    - pool_pre_ping=True：切れた接続を再利用して落ちる事故を減らす\n",
      "    - connect_timeout：ハングを避ける（psycopg側へ渡す）\n",
      "    \"\"\"\n",
      "    try:\n",
      "        engine = create_engine(\n",
      "            db_url,\n",
      "            echo=echo,\n",
      "            pool_pre_ping=True,\n",
      "            future=True,\n",
      "            connect_args={\"connect_timeout\": 5},\n",
      "        )\n",
      "        return engine\n",
      "    except Exception as e:\n",
      "        raise db_error(\n",
      "            \"DB_ENGINE_ ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "db_path = Path(\"/mnt/e/env/ts/tslib/src/nf_app/db.py\")\n",
    "\n",
    "db_code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Optional, Tuple\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error\n",
    "from nf_app.config import NFConfig\n",
    "\n",
    "\n",
    "class DbKind(str, Enum):\n",
    "    DATASET = \"dataset\"\n",
    "    MODEL = \"model\"\n",
    "\n",
    "\n",
    "def _inject_password_if_missing(db_url: str, *, kind: DbKind) -> str:\n",
    "    \"\"\"\n",
    "    URLにパスワードが含まれていない場合、環境変数から注入する。\n",
    "    優先順位：\n",
    "      1) DATASET_DB_PASSWORD / MODEL_DB_PASSWORD\n",
    "      2) PGPASSWORD\n",
    "    ※ URLに既に password があれば何もしない\n",
    "    \"\"\"\n",
    "    parsed = urlparse(db_url)\n",
    "    if parsed.password:\n",
    "        return db_url\n",
    "\n",
    "    # postgresql以外はそのまま\n",
    "    if not parsed.scheme.startswith(\"postgresql\"):\n",
    "        return db_url\n",
    "\n",
    "    pwd_key = \"DATASET_DB_PASSWORD\" if kind == DbKind.DATASET else \"MODEL_DB_PASSWORD\"\n",
    "    pwd = os.environ.get(pwd_key) or os.environ.get(\"PGPASSWORD\")\n",
    "    if not pwd:\n",
    "        return db_url  # 注入できない（後で接続時に落ちる/skip対象）\n",
    "\n",
    "    # netloc: user@host:port 形式に password を入れる -> user:pass@host:port\n",
    "    username = parsed.username or \"\"\n",
    "    hostname = parsed.hostname or \"\"\n",
    "    port = f\":{parsed.port}\" if parsed.port else \"\"\n",
    "    if username:\n",
    "        netloc = f\"{username}:{pwd}@{hostname}{port}\"\n",
    "    else:\n",
    "        netloc = f\"{hostname}{port}\"\n",
    "\n",
    "    rebuilt = parsed._replace(netloc=netloc)\n",
    "    return urlunparse(rebuilt)\n",
    "\n",
    "\n",
    "def _make_engine(db_url: str, *, echo: bool = False) -> Engine:\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            db_url,\n",
    "            echo=echo,\n",
    "            pool_pre_ping=True,\n",
    "            future=True,\n",
    "            connect_args={\"connect_timeout\": 5},\n",
    "        )\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        raise db_error(\n",
    "            \"DB_ENGINE_CREATE_FAILED\",\n",
    "            f\"Failed to create engine for url={db_url}\",\n",
    "            context={\"db_url\": db_url},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "\n",
    "def _ping(engine: Engine) -> None:\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"SELECT 1\"))\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_PING_FAILED\",\n",
    "            \"DB ping failed (SELECT 1).\",\n",
    "            context={\"dialect\": str(engine.dialect.name)},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "\n",
    "def _version(engine: Engine) -> str:\n",
    "    with engine.connect() as conn:\n",
    "        r = conn.execute(text(\"SELECT version()\")).scalar_one()\n",
    "    return str(r)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DbManager:\n",
    "    cfg: NFConfig\n",
    "    echo_sql: bool = False\n",
    "\n",
    "    _dataset_engine: Optional[Engine] = None\n",
    "    _model_engine: Optional[Engine] = None\n",
    "\n",
    "    def dataset_engine(self) -> Engine:\n",
    "        if self._dataset_engine is None:\n",
    "            if not self.cfg.dataset_db_url:\n",
    "                raise db_error(\"DB_URL_EMPTY\", \"DATASET_DB_URL is empty.\", context={\"kind\": DbKind.DATASET.value})\n",
    "            url = _inject_password_if_missing(self.cfg.dataset_db_url, kind=DbKind.DATASET)\n",
    "            self._dataset_engine = _make_engine(url, echo=self.echo_sql)\n",
    "        return self._dataset_engine\n",
    "\n",
    "    def model_engine(self) -> Engine:\n",
    "        if self._model_engine is None:\n",
    "            if not self.cfg.model_db_url:\n",
    "                raise db_error(\"DB_URL_EMPTY\", \"MODEL_DB_URL is empty.\", context={\"kind\": DbKind.MODEL.value})\n",
    "            url = _inject_password_if_missing(self.cfg.model_db_url, kind=DbKind.MODEL)\n",
    "            self._model_engine = _make_engine(url, echo=self.echo_sql)\n",
    "        return self._model_engine\n",
    "\n",
    "    def test_connections(self) -> Tuple[str, str]:\n",
    "        ds = self.dataset_engine()\n",
    "        md = self.model_engine()\n",
    "\n",
    "        _ping(ds)\n",
    "        _ping(md)\n",
    "\n",
    "        return _version(ds), _version(md)\n",
    "\n",
    "    def dispose(self) -> None:\n",
    "        if self._dataset_engine is not None:\n",
    "            self._dataset_engine.dispose()\n",
    "            self._dataset_engine = None\n",
    "        if self._model_engine is not None:\n",
    "            self._model_engine.dispose()\n",
    "            self._model_engine = None\n",
    "'''\n",
    "db_path.write_text(db_code, encoding=\"utf-8\")\n",
    "print(\"Updated:\", db_path)\n",
    "print(db_path.read_text(encoding=\"utf-8\")[:700], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb4eb0",
   "metadata": {},
   "source": [
    "## ✅ codecell：/mnt/e/env/ts/tslib/tests/test_db_smoke.py を上書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "260af73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: /mnt/e/env/ts/tslib/tests/test_db_smoke.py\n",
      "\\\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pytest\n",
      "\n",
      "from nf_app.config import NFConfig\n",
      "from nf_app.db import DbManager\n",
      "from nf_app.errors import NFError\n",
      "\n",
      "\n",
      "def _has_db_creds() -> bool:\n",
      "    # URLは必須。パスワードは URLに含まれるか、環境変数で供給されればOK。\n",
      "    if not os.environ.get(\"DATASET_DB_URL\") or not os.environ.get(\"MODEL_DB_URL\"):\n",
      "        return False\n",
      "    # パスワード必須環境の可能性が高いので、どれかが無い場合はskipに寄せる\n",
      "    return bool(\n",
      "        os.environ.get(\"DATASET_DB_PASSWORD\")\n",
      "        or os.environ.get(\"MODEL_DB_PASSWORD\")\n",
      "        or os.environ.get(\"PGPASSWORD\")\n",
      "        or (\"://\" in os.environ[\"DATASET_DB_URL\"] and \"@\" in os.environ[\"DATASET_DB_URL\"] and \":\" in os.environ[\"DATASET_DB_ ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_path = Path(\"/mnt/e/env/ts/tslib/tests/test_db_smoke.py\")\n",
    "\n",
    "test_code = r'''\\\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pytest\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "from nf_app.errors import NFError\n",
    "\n",
    "\n",
    "def _has_db_creds() -> bool:\n",
    "    # URLは必須。パスワードは URLに含まれるか、環境変数で供給されればOK。\n",
    "    if not os.environ.get(\"DATASET_DB_URL\") or not os.environ.get(\"MODEL_DB_URL\"):\n",
    "        return False\n",
    "    # パスワード必須環境の可能性が高いので、どれかが無い場合はskipに寄せる\n",
    "    return bool(\n",
    "        os.environ.get(\"DATASET_DB_PASSWORD\")\n",
    "        or os.environ.get(\"MODEL_DB_PASSWORD\")\n",
    "        or os.environ.get(\"PGPASSWORD\")\n",
    "        or (\"://\" in os.environ[\"DATASET_DB_URL\"] and \"@\" in os.environ[\"DATASET_DB_URL\"] and \":\" in os.environ[\"DATASET_DB_URL\"].split(\"@\")[0])\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_db_smoke_connects_dataset_and_model(monkeypatch):\n",
    "    # conftest が env をクリアするので、ここでセットする。\n",
    "    # NOTE: パスワードはURL直書きせず、DATASET_DB_PASSWORD/MODEL_DB_PASSWORD か PGPASSWORD を使う。\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    # ここが無いなら環境依存で落ちるのでskip\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set DATASET_DB_PASSWORD/MODEL_DB_PASSWORD or PGPASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    try:\n",
    "        ds_ver, md_ver = dbm.test_connections()\n",
    "        assert \"PostgreSQL\" in ds_ver\n",
    "        assert \"PostgreSQL\" in md_ver\n",
    "    except NFError as e:\n",
    "        # 認証/起動状態など環境要因はskip扱いにする（統合テストのため）\n",
    "        pytest.skip(f\"DB not reachable/auth failed: {e.to_record().get('cause_message')}\")\n",
    "    finally:\n",
    "        dbm.dispose()\n",
    "'''\n",
    "test_path.write_text(test_code, encoding=\"utf-8\")\n",
    "print(\"Updated:\", test_path)\n",
    "print(test_path.read_text(encoding=\"utf-8\")[:650], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59699526",
   "metadata": {},
   "source": [
    "## ✅ codecell：Notebookで環境変数セット例（推奨）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a25c5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# URLにパスワードを書かない運用\n",
    "os.environ[\"DATASET_DB_URL\"] = \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\"\n",
    "os.environ[\"MODEL_DB_URL\"]   = \"postgresql+psycopg://postgres@127.0.0.1:5432/model\"\n",
    "\n",
    "# どちらかで供給（推奨はDB別）\n",
    "os.environ[\"DATASET_DB_PASSWORD\"] = \"z\"\n",
    "os.environ[\"MODEL_DB_PASSWORD\"]   = \"z\"\n",
    "\n",
    "os.environ[\"PGPASSWORD\"] = \"z\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307f06b",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-DB-2. Notebookで疎通確認（cfg→DbManager→test）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f2e0457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset version: PostgreSQL 16.11 (Ubuntu 16.11-0ubuntu0.24.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit\n",
      "model   version: PostgreSQL 16.11 (Ubuntu 16.11-0ubuntu0.24.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook用の一時設定（.envに書いてもOK）\n",
    "os.environ[\"DATASET_DB_URL\"] = \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\"\n",
    "os.environ[\"MODEL_DB_URL\"]   = \"postgresql+psycopg://postgres@127.0.0.1:5432/model\"\n",
    "os.environ[\"NF_RUN_MODE\"] = \"DRY_RUN\"\n",
    "os.environ[\"NF_ARTIFACT_ROOT\"] = \"/mnt/e/env/ts/tslib/artifacts\"\n",
    "os.environ[\"NF_STRICT_VALIDATION\"] = \"true\"\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "\n",
    "cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "ds_ver, md_ver = dbm.test_connections()\n",
    "print(\"dataset version:\", ds_ver)\n",
    "print(\"model   version:\", md_ver)\n",
    "\n",
    "# Notebook再実行耐性：最後に必ず閉じる（次工程で“共有”するなら閉じない設計も可）\n",
    "dbm.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964560c",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-DB-3. pytest（tests/test_db_smoke.py を作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33d31df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_db_smoke.py\n",
      "\\\n",
      "from pathlib import Path\n",
      "\n",
      "from nf_app.config import NFConfig\n",
      "from nf_app.db import DbManager\n",
      "\n",
      "\n",
      "def test_db_smoke_connects_dataset_and_model(monkeypatch):\n",
      "    # conftestがenvをクリアするので、ここで明示的にセット\n",
      "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
      "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
      "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
      "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
      "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
      "\n",
      "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
      "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
      "\n",
      "    ds_ver, md_ver = dbm.test_connections()\n",
      "\n",
      "    assert \"PostgreSQL\" in ds_ver\n",
      "    assert \"PostgreSQL\" in md_ver\n",
      "\n",
      "    dbm.dispose()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_path = Path(\"/mnt/e/env/ts/tslib/tests/test_db_smoke.py\")\n",
    "test_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_code = r'''\\\n",
    "from pathlib import Path\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "\n",
    "\n",
    "def test_db_smoke_connects_dataset_and_model(monkeypatch):\n",
    "    # conftestがenvをクリアするので、ここで明示的にセット\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    ds_ver, md_ver = dbm.test_connections()\n",
    "\n",
    "    assert \"PostgreSQL\" in ds_ver\n",
    "    assert \"PostgreSQL\" in md_ver\n",
    "\n",
    "    dbm.dispose()\n",
    "'''\n",
    "test_path.write_text(test_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", test_path)\n",
    "print(test_path.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197c0d3",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-DB-4. pytest 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afe4d8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.23s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_db_smoke.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8cf7f",
   "metadata": {},
   "source": [
    "# 1. DB Migrate（DDL実行：初回セットアップ）\n",
    "\n",
    "目的：\n",
    "- model DB に `neuralforecast` スキーマと管理テーブル群を作成する\n",
    "- 何度実行しても壊れない（CREATE IF NOT EXISTS / 追記可能）\n",
    "- pytestで「必須テーブルが存在する」ことを担保する\n",
    "\n",
    "作成対象（model DB: schema=neuralforecast）\n",
    "- schema_migrations（適用履歴）\n",
    "- feature_prefix_rules（hist_/futr_/stat_等のルール）\n",
    "- join_rules（dataset結合SQLのルール）\n",
    "- experiments（実験単位の台帳）\n",
    "- run_plans（実行計画：enabled/priority等）\n",
    "- runs（実行結果：status/error/metrics）\n",
    "- model_versions（保存モデルと外生変数リスト）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32929db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/migrations.py\n",
      "\\\n",
      "from __future__ import annotations\n",
      "\n",
      "from dataclasses import dataclass\n",
      "from typing import Iterable, List, Optional, Sequence\n",
      "\n",
      "from sqlalchemy import text\n",
      "from sqlalchemy.engine import Engine\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "\n",
      "from nf_app.errors import db_error\n",
      "\n",
      "\n",
      "SCHEMA_NAME = \"neuralforecast\"\n",
      "MIGRATION_VERSION = 1\n",
      "\n",
      "\n",
      "def _exec(engine: Engine, statements: Sequence[str]) -> None:\n",
      "    \"\"\"\n",
      "    DDL実行（トランザクション）。\n",
      "    CREATE IF NOT EXISTS を前提に、繰り返し実行しても安全にする。\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with engine.begin() as conn:\n",
      "            for stmt in statements:\n",
      "                conn.execute(text(stmt))\n",
      "    except SQLAlchemyError as e:\n",
      "        raise db_error(\n",
      "            \"DB_MIGRATION_FAILED\",\n",
      "            \"Migration failed.\",\n",
      "            context={\"schema\": SCHEMA_NAME, \"version\": MIGRATION_VERSION, \"statement\": statements[-1] if statements else None},\n",
      "            cause=e,\n",
      "        )\n",
      "\n",
      "\n",
      "def migrate_model_ ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "mig_path = Path(\"/mnt/e/env/ts/tslib/src/nf_app/migrations.py\")\n",
    "mig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mig_code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Optional, Sequence\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error\n",
    "\n",
    "\n",
    "SCHEMA_NAME = \"neuralforecast\"\n",
    "MIGRATION_VERSION = 1\n",
    "\n",
    "\n",
    "def _exec(engine: Engine, statements: Sequence[str]) -> None:\n",
    "    \"\"\"\n",
    "    DDL実行（トランザクション）。\n",
    "    CREATE IF NOT EXISTS を前提に、繰り返し実行しても安全にする。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            for stmt in statements:\n",
    "                conn.execute(text(stmt))\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_MIGRATION_FAILED\",\n",
    "            \"Migration failed.\",\n",
    "            context={\"schema\": SCHEMA_NAME, \"version\": MIGRATION_VERSION, \"statement\": statements[-1] if statements else None},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "\n",
    "def migrate_model_db(engine: Engine) -> None:\n",
    "    \"\"\"\n",
    "    model DBに neuralforecast スキーマと必須テーブルを作成する。\n",
    "    \"\"\"\n",
    "    stmts: List[str] = []\n",
    "\n",
    "    # UUID生成用（gen_random_uuid）\n",
    "    stmts.append(\"CREATE EXTENSION IF NOT EXISTS pgcrypto;\")\n",
    "\n",
    "    # schema\n",
    "    stmts.append(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME};\")\n",
    "\n",
    "    # migration ledger\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.schema_migrations (\n",
    "        version INTEGER PRIMARY KEY,\n",
    "        applied_ts TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # prefix rules: hist_/futr_/stat_ など\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.feature_prefix_rules (\n",
    "        rule_id BIGSERIAL PRIMARY KEY,\n",
    "        prefix TEXT NOT NULL UNIQUE,\n",
    "        role TEXT NOT NULL, -- HIST / FUTR / STAT / IGNORE / META\n",
    "        enabled BOOLEAN NOT NULL DEFAULT TRUE,\n",
    "        description TEXT,\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        updated_ts TIMESTAMPTZ\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # join rules: dataset側の結合ビューをSQLで管理（最初はSQL文字列で十分に強い）\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.join_rules (\n",
    "        rule_id BIGSERIAL PRIMARY KEY,\n",
    "        rule_name TEXT NOT NULL UNIQUE,\n",
    "        enabled BOOLEAN NOT NULL DEFAULT TRUE,\n",
    "        priority INTEGER NOT NULL DEFAULT 100,\n",
    "        sql_text TEXT NOT NULL,\n",
    "        description TEXT,\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        updated_ts TIMESTAMPTZ\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # experiments: 実験の台帳（大枠）\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.experiments (\n",
    "        experiment_id BIGSERIAL PRIMARY KEY,\n",
    "        name TEXT NOT NULL UNIQUE,\n",
    "        description TEXT,\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # run plans: 実行計画（enabled/priorityで実行対象を制御）\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.run_plans (\n",
    "        plan_id BIGSERIAL PRIMARY KEY,\n",
    "        enabled BOOLEAN NOT NULL DEFAULT TRUE,\n",
    "        priority INTEGER NOT NULL DEFAULT 100,\n",
    "        plan_name TEXT NOT NULL UNIQUE,\n",
    "\n",
    "        experiment_id BIGINT REFERENCES {SCHEMA_NAME}.experiments(experiment_id),\n",
    "\n",
    "        model_name TEXT NOT NULL,           -- 例: AutoNBEATS, AutoTFT ...\n",
    "        run_mode TEXT NOT NULL,             -- DRY_RUN / TRAIN_SAVE_PREDICT / TRAIN_PREDICT / PREDICT_ONLY\n",
    "        params_json JSONB NOT NULL DEFAULT '{{}}'::jsonb,        -- モデルパラメータ\n",
    "        data_json JSONB NOT NULL DEFAULT '{{}}'::jsonb,          -- データ結合/フィルタ設定（join_rule参照など）\n",
    "        artifact_subdir TEXT,               -- 成果物の相対パス（artifact_root配下）\n",
    "\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        updated_ts TIMESTAMPTZ\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # runs: 実行結果（失敗も資産化）\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.runs (\n",
    "        run_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "        plan_id BIGINT REFERENCES {SCHEMA_NAME}.run_plans(plan_id),\n",
    "        experiment_id BIGINT REFERENCES {SCHEMA_NAME}.experiments(experiment_id),\n",
    "\n",
    "        status TEXT NOT NULL DEFAULT 'PENDING',   -- PENDING/RUNNING/SUCCEEDED/FAILED/SKIPPED\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        started_ts TIMESTAMPTZ,\n",
    "        ended_ts TIMESTAMPTZ,\n",
    "\n",
    "        data_signature JSONB,     -- データ署名（期間/件数/列/ハッシュ等）\n",
    "        metrics_json JSONB,       -- 評価指標（集計）\n",
    "        resources_json JSONB,     -- GPU/CPU/メモリ/時間など\n",
    "\n",
    "        error_type TEXT,\n",
    "        error_code TEXT,\n",
    "        error_message TEXT,\n",
    "        error_context JSONB,\n",
    "        traceback TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_runs_plan_id ON {SCHEMA_NAME}.runs(plan_id);\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_runs_experiment_id ON {SCHEMA_NAME}.runs(experiment_id);\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_runs_status ON {SCHEMA_NAME}.runs(status);\")\n",
    "\n",
    "    # model_versions: 保存モデルの台帳（外生変数リストも保持）\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.model_versions (\n",
    "        model_version_id BIGSERIAL PRIMARY KEY,\n",
    "        model_name TEXT NOT NULL,\n",
    "        run_id UUID REFERENCES {SCHEMA_NAME}.runs(run_id),\n",
    "\n",
    "        artifact_path TEXT NOT NULL,\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "\n",
    "        exog_hist_cols TEXT[] DEFAULT ARRAY[]::TEXT[],\n",
    "        exog_futr_cols TEXT[] DEFAULT ARRAY[]::TEXT[],\n",
    "        exog_stat_cols TEXT[] DEFAULT ARRAY[]::TEXT[],\n",
    "\n",
    "        notes TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_model_versions_model_name ON {SCHEMA_NAME}.model_versions(model_name);\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_model_versions_run_id ON {SCHEMA_NAME}.model_versions(run_id);\")\n",
    "\n",
    "    # 実行\n",
    "    _exec(engine, stmts)\n",
    "\n",
    "    # version upsert（最小の履歴）\n",
    "    _exec(engine, [f\"\"\"\n",
    "    INSERT INTO {SCHEMA_NAME}.schema_migrations(version)\n",
    "    VALUES ({MIGRATION_VERSION})\n",
    "    ON CONFLICT (version) DO NOTHING;\n",
    "    \"\"\"])\n",
    "\n",
    "    # 初期データ投入：prefix rules（idempotent）\n",
    "    _exec(engine, [\n",
    "        f\"\"\"\n",
    "        INSERT INTO {SCHEMA_NAME}.feature_prefix_rules(prefix, role, enabled, description)\n",
    "        VALUES\n",
    "            ('hist_', 'HIST', TRUE, 'Historical exogenous features'),\n",
    "            ('futr_', 'FUTR', TRUE, 'Future exogenous features'),\n",
    "            ('stat_', 'STAT', TRUE, 'Static exogenous features')\n",
    "        ON CONFLICT (prefix) DO NOTHING;\n",
    "        \"\"\"\n",
    "    ])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MigrationRunner:\n",
    "    \"\"\"\n",
    "    今後マイグレーションが増えても拡張しやすいようにrunner化。\n",
    "    \"\"\"\n",
    "    model_engine: Engine\n",
    "\n",
    "    def apply_all(self) -> None:\n",
    "        migrate_model_db(self.model_engine)\n",
    "'''\n",
    "mig_path.write_text(mig_code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", mig_path)\n",
    "print(mig_path.read_text(encoding=\"utf-8\")[:900], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72055145",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-Migrate-2. Notebookでmigrate実行（model DB）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "557149c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migration applied for schema: neuralforecast\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 例：URLはパスワードなし、パスワードは環境変数で供給（推奨）\n",
    "os.environ[\"DATASET_DB_URL\"] = \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\"\n",
    "os.environ[\"MODEL_DB_URL\"]   = \"postgresql+psycopg://postgres@127.0.0.1:5432/model\"\n",
    "# os.environ[\"MODEL_DB_PASSWORD\"] = \"...\"  # 必要なら\n",
    "\n",
    "os.environ[\"NF_ARTIFACT_ROOT\"] = \"/mnt/e/env/ts/tslib/artifacts\"\n",
    "os.environ[\"NF_RUN_MODE\"] = \"DRY_RUN\"\n",
    "os.environ[\"NF_STRICT_VALIDATION\"] = \"true\"\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "from nf_app.migrations import MigrationRunner, SCHEMA_NAME\n",
    "\n",
    "cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "runner = MigrationRunner(model_engine=dbm.model_engine())\n",
    "runner.apply_all()\n",
    "\n",
    "print(\"Migration applied for schema:\", SCHEMA_NAME)\n",
    "dbm.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9afce",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-Migrate-3. pytest（tests/test_migrations.py を作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d76cfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_migrations.py\n",
      "\\\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pytest\n",
      "from sqlalchemy import text\n",
      "\n",
      "from nf_app.config import NFConfig\n",
      "from nf_app.db import DbManager\n",
      "from nf_app.migrations import MigrationRunner, SCHEMA_NAME\n",
      "\n",
      "\n",
      "def _has_db_creds() -> bool:\n",
      "    if not os.environ.get(\"MODEL_DB_URL\"):\n",
      "        return False\n",
      "    return bool(\n",
      "        os.environ.get(\"MODEL_DB_PASSWORD\")\n",
      "        or os.environ.get(\"PGPASSWORD\")\n",
      "        or (\"://\" in os.environ[\"MODEL_DB_URL\"] and \"@\" in os.environ[\"MODEL_DB_URL\"] and \":\" in os.environ[\"MODEL_DB_URL\"].split(\"@\")[0])\n",
      "    )\n",
      "\n",
      "\n",
      "@pytest.mark.integration\n",
      "def test_migrations_create_required_tables(monkeypatch):\n",
      "    # conftestがenvをクリアするのでセット\n",
      "    monkeypatch.setenv(\"DATASET_DB_URL\" ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tpath = Path(\"/mnt/e/env/ts/tslib/tests/test_migrations.py\")\n",
    "tpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tcode = r'''\\\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pytest\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "from nf_app.migrations import MigrationRunner, SCHEMA_NAME\n",
    "\n",
    "\n",
    "def _has_db_creds() -> bool:\n",
    "    if not os.environ.get(\"MODEL_DB_URL\"):\n",
    "        return False\n",
    "    return bool(\n",
    "        os.environ.get(\"MODEL_DB_PASSWORD\")\n",
    "        or os.environ.get(\"PGPASSWORD\")\n",
    "        or (\"://\" in os.environ[\"MODEL_DB_URL\"] and \"@\" in os.environ[\"MODEL_DB_URL\"] and \":\" in os.environ[\"MODEL_DB_URL\"].split(\"@\")[0])\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_migrations_create_required_tables(monkeypatch):\n",
    "    # conftestがenvをクリアするのでセット\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    # パスワード系が無ければskip（環境依存の統合テスト）\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set MODEL_DB_PASSWORD or PGPASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    try:\n",
    "        runner = MigrationRunner(model_engine=dbm.model_engine())\n",
    "        runner.apply_all()\n",
    "\n",
    "        required = [\n",
    "            f\"{SCHEMA_NAME}.schema_migrations\",\n",
    "            f\"{SCHEMA_NAME}.feature_prefix_rules\",\n",
    "            f\"{SCHEMA_NAME}.join_rules\",\n",
    "            f\"{SCHEMA_NAME}.experiments\",\n",
    "            f\"{SCHEMA_NAME}.run_plans\",\n",
    "            f\"{SCHEMA_NAME}.runs\",\n",
    "            f\"{SCHEMA_NAME}.model_versions\",\n",
    "        ]\n",
    "\n",
    "        # to_regclass('schema.table') がNULLでなければ存在\n",
    "        with dbm.model_engine().connect() as conn:\n",
    "            for t in required:\n",
    "                exists = conn.execute(text(\"SELECT to_regclass(:t) IS NOT NULL\"), {\"t\": t}).scalar_one()\n",
    "                assert exists, f\"missing table: {t}\"\n",
    "\n",
    "            # 初期prefixが入っていること\n",
    "            prefixes = conn.execute(text(f\"SELECT prefix FROM {SCHEMA_NAME}.feature_prefix_rules ORDER BY prefix\")).scalars().all()\n",
    "            assert \"hist_\" in prefixes\n",
    "            assert \"futr_\" in prefixes\n",
    "            assert \"stat_\" in prefixes\n",
    "\n",
    "    finally:\n",
    "        dbm.dispose()\n",
    "'''\n",
    "tpath.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", tpath)\n",
    "print(tpath.read_text(encoding=\"utf-8\")[:700], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba54719",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-Migrate-4. pytest 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "946337ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.23s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_migrations.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7487b8",
   "metadata": {},
   "source": [
    "## pytest mark登録\n",
    "\n",
    "integration マークを pytest.ini に登録して警告を消す。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8130e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/pytest.ini\n",
      "[pytest]\n",
      "markers =\n",
      "    integration: integration tests requiring external services (e.g., Postgres)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "pytest_ini = Path(\"/mnt/e/env/ts/tslib/pytest.ini\")\n",
    "pytest_ini.write_text(\n",
    "    \"\\n\".join([\n",
    "        \"[pytest]\",\n",
    "        \"markers =\",\n",
    "        \"    integration: integration tests requiring external services (e.g., Postgres)\",\n",
    "        \"\",\n",
    "    ]),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(\"Wrote:\", pytest_ini)\n",
    "print(pytest_ini.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf057a0",
   "metadata": {},
   "source": [
    "## 1. DB Migrate - join_rules（ID 013）\n",
    "\n",
    "目的：\n",
    "- model DB の `neuralforecast.join_rules` に初期ルールを投入（何度実行しても安全）\n",
    "- pytestで join_rules に INSERT できることを検証する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57648b",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-Migrate-joinrules-1 migrations.py を更新（フル上書き）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "114704cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: /mnt/e/env/ts/tslib/src/nf_app/migrations.py\n",
      "\\\n",
      "from __future__ import annotations\n",
      "\n",
      "from dataclasses import dataclass\n",
      "from typing import List, Sequence\n",
      "\n",
      "from sqlalchemy import text\n",
      "from sqlalchemy.engine import Engine\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "\n",
      "from nf_app.errors import db_error\n",
      "\n",
      "\n",
      "SCHEMA_NAME = \"neuralforecast\"\n",
      "MIGRATION_VERSION = 2\n",
      "\n",
      "\n",
      "def _exec(engine: Engine, statements: Sequence[str]) -> None:\n",
      "    \"\"\"\n",
      "    DDL/seed実行（トランザクション）\n",
      "    - できるだけCREATE IF NOT EXISTS / ON CONFLICT で冪等にする\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with engine.begin() as conn:\n",
      "            for stmt in statements:\n",
      "                conn.execute(text(stmt))\n",
      "    except SQLAlchemyError as e:\n",
      "        raise db_error(\n",
      "            \"DB_MIGRATION_FAILED\",\n",
      "            \"Migration failed.\",\n",
      "            context={\n",
      "                \"schema\": SCHEMA_NAME,\n",
      "                \"version\": MIGRATION_VERSION,\n",
      "                \"statement_tail\": (statements[-1][:400] if statements else None), ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "mig_path = Path(\"/mnt/e/env/ts/tslib/src/nf_app/migrations.py\")\n",
    "mig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mig_code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Sequence\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error\n",
    "\n",
    "\n",
    "SCHEMA_NAME = \"neuralforecast\"\n",
    "MIGRATION_VERSION = 2\n",
    "\n",
    "\n",
    "def _exec(engine: Engine, statements: Sequence[str]) -> None:\n",
    "    \"\"\"\n",
    "    DDL/seed実行（トランザクション）\n",
    "    - できるだけCREATE IF NOT EXISTS / ON CONFLICT で冪等にする\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            for stmt in statements:\n",
    "                conn.execute(text(stmt))\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_MIGRATION_FAILED\",\n",
    "            \"Migration failed.\",\n",
    "            context={\n",
    "                \"schema\": SCHEMA_NAME,\n",
    "                \"version\": MIGRATION_VERSION,\n",
    "                \"statement_tail\": (statements[-1][:400] if statements else None),\n",
    "            },\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "\n",
    "def migrate_model_db(engine: Engine) -> None:\n",
    "    \"\"\"\n",
    "    model DBに neuralforecast スキーマと必須テーブルを作成し、初期データを投入する。\n",
    "    \"\"\"\n",
    "    stmts: List[str] = []\n",
    "\n",
    "    # UUID生成用\n",
    "    stmts.append(\"CREATE EXTENSION IF NOT EXISTS pgcrypto;\")\n",
    "\n",
    "    # schema\n",
    "    stmts.append(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME};\")\n",
    "\n",
    "    # migration ledger\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.schema_migrations (\n",
    "        version INTEGER PRIMARY KEY,\n",
    "        applied_ts TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # prefix rules\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.feature_prefix_rules (\n",
    "        rule_id BIGSERIAL PRIMARY KEY,\n",
    "        prefix TEXT NOT NULL UNIQUE,\n",
    "        role TEXT NOT NULL, -- HIST / FUTR / STAT / IGNORE / META\n",
    "        enabled BOOLEAN NOT NULL DEFAULT TRUE,\n",
    "        description TEXT,\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        updated_ts TIMESTAMPTZ\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # join rules\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.join_rules (\n",
    "        rule_id BIGSERIAL PRIMARY KEY,\n",
    "        rule_name TEXT NOT NULL UNIQUE,\n",
    "        enabled BOOLEAN NOT NULL DEFAULT TRUE,\n",
    "        priority INTEGER NOT NULL DEFAULT 100,\n",
    "        sql_text TEXT NOT NULL,\n",
    "        description TEXT,\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        updated_ts TIMESTAMPTZ\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # experiments\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.experiments (\n",
    "        experiment_id BIGSERIAL PRIMARY KEY,\n",
    "        name TEXT NOT NULL UNIQUE,\n",
    "        description TEXT,\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # run plans\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.run_plans (\n",
    "        plan_id BIGSERIAL PRIMARY KEY,\n",
    "        enabled BOOLEAN NOT NULL DEFAULT TRUE,\n",
    "        priority INTEGER NOT NULL DEFAULT 100,\n",
    "        plan_name TEXT NOT NULL UNIQUE,\n",
    "\n",
    "        experiment_id BIGINT REFERENCES {SCHEMA_NAME}.experiments(experiment_id),\n",
    "\n",
    "        model_name TEXT NOT NULL,\n",
    "        run_mode TEXT NOT NULL,\n",
    "        params_json JSONB NOT NULL DEFAULT '{{}}'::jsonb,\n",
    "        data_json JSONB NOT NULL DEFAULT '{{}}'::jsonb,\n",
    "        artifact_subdir TEXT,\n",
    "\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        updated_ts TIMESTAMPTZ\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # runs\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.runs (\n",
    "        run_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "        plan_id BIGINT REFERENCES {SCHEMA_NAME}.run_plans(plan_id),\n",
    "        experiment_id BIGINT REFERENCES {SCHEMA_NAME}.experiments(experiment_id),\n",
    "\n",
    "        status TEXT NOT NULL DEFAULT 'PENDING',\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        started_ts TIMESTAMPTZ,\n",
    "        ended_ts TIMESTAMPTZ,\n",
    "\n",
    "        data_signature JSONB,\n",
    "        metrics_json JSONB,\n",
    "        resources_json JSONB,\n",
    "\n",
    "        error_type TEXT,\n",
    "        error_code TEXT,\n",
    "        error_message TEXT,\n",
    "        error_context JSONB,\n",
    "        traceback TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_runs_plan_id ON {SCHEMA_NAME}.runs(plan_id);\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_runs_experiment_id ON {SCHEMA_NAME}.runs(experiment_id);\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_runs_status ON {SCHEMA_NAME}.runs(status);\")\n",
    "\n",
    "    # model_versions\n",
    "    stmts.append(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.model_versions (\n",
    "        model_version_id BIGSERIAL PRIMARY KEY,\n",
    "        model_name TEXT NOT NULL,\n",
    "        run_id UUID REFERENCES {SCHEMA_NAME}.runs(run_id),\n",
    "\n",
    "        artifact_path TEXT NOT NULL,\n",
    "        created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "\n",
    "        exog_hist_cols TEXT[] DEFAULT ARRAY[]::TEXT[],\n",
    "        exog_futr_cols TEXT[] DEFAULT ARRAY[]::TEXT[],\n",
    "        exog_stat_cols TEXT[] DEFAULT ARRAY[]::TEXT[],\n",
    "\n",
    "        notes TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_model_versions_model_name ON {SCHEMA_NAME}.model_versions(model_name);\")\n",
    "    stmts.append(f\"CREATE INDEX IF NOT EXISTS ix_model_versions_run_id ON {SCHEMA_NAME}.model_versions(run_id);\")\n",
    "\n",
    "    # 1) DDL適用\n",
    "    _exec(engine, stmts)\n",
    "\n",
    "    # 2) schema_migrations（冪等）\n",
    "    _exec(engine, [f\"\"\"\n",
    "    INSERT INTO {SCHEMA_NAME}.schema_migrations(version)\n",
    "    VALUES ({MIGRATION_VERSION})\n",
    "    ON CONFLICT (version) DO NOTHING;\n",
    "    \"\"\"])\n",
    "\n",
    "    # 3) 初期prefix rules（冪等）\n",
    "    _exec(engine, [f\"\"\"\n",
    "    INSERT INTO {SCHEMA_NAME}.feature_prefix_rules(prefix, role, enabled, description)\n",
    "    VALUES\n",
    "        ('hist_', 'HIST', TRUE, 'Historical exogenous features'),\n",
    "        ('futr_', 'FUTR', TRUE, 'Future exogenous features'),\n",
    "        ('stat_', 'STAT', TRUE, 'Static exogenous features')\n",
    "    ON CONFLICT (prefix) DO NOTHING;\n",
    "    \"\"\"])\n",
    "\n",
    "    # 4) 初期join rule（冪等）\n",
    "    # NOTE: この sql_text は dataset DB で実行される想定のSQLテンプレ。\n",
    "    #       後工程で「列の自動選択（prefixで外生変数分離）」を実装して、SELECT句を動的生成する。\n",
    "    _exec(engine, [f\"\"\"\n",
    "    INSERT INTO {SCHEMA_NAME}.join_rules(rule_name, enabled, priority, sql_text, description)\n",
    "    VALUES (\n",
    "        'loto_join_v1',\n",
    "        TRUE,\n",
    "        100,\n",
    "        $SQL$\n",
    "        SELECT\n",
    "            h.ds,\n",
    "            h.unique_id,\n",
    "            h.y,\n",
    "            -- prizes: hist_*\n",
    "            p.hist_b1, p.hist_b2,\n",
    "            p.hist_stc, p.hist_stm,\n",
    "            -- futr: futr_*\n",
    "            f.futr_year, f.futr_month, f.futr_day\n",
    "        FROM public.loto_hist h\n",
    "        LEFT JOIN public.loto_prizes p\n",
    "          ON h.ds = p.ds AND h.unique_id = p.unique_id\n",
    "        LEFT JOIN public.loto_futr f\n",
    "          ON h.ds = f.ds\n",
    "        $SQL$,\n",
    "        'Default join rule for loto_* tables (dataset DB SQL template).'\n",
    "    )\n",
    "    ON CONFLICT (rule_name) DO NOTHING;\n",
    "    \"\"\"])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MigrationRunner:\n",
    "    model_engine: Engine\n",
    "\n",
    "    def apply_all(self) -> None:\n",
    "        migrate_model_db(self.model_engine)\n",
    "'''\n",
    "mig_path.write_text(mig_code, encoding=\"utf-8\")\n",
    "print(\"Updated:\", mig_path)\n",
    "print(mig_path.read_text(encoding=\"utf-8\")[:900], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246d5f6",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-Migrate-joinrules-2 test_migrations.py を更新（INSERT検証を追加）\n",
    "\n",
    "既存の tests/test_migrations.py に join_rulesへINSERTできるテストを追加します（フル上書き）。\n",
    "パス：/mnt/e/env/ts/tslib/tests/test_migrations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd79a78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: /mnt/e/env/ts/tslib/tests/test_migrations.py\n",
      "\\\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pytest\n",
      "from sqlalchemy import text\n",
      "\n",
      "from nf_app.config import NFConfig\n",
      "from nf_app.db import DbManager\n",
      "from nf_app.migrations import MigrationRunner, SCHEMA_NAME\n",
      "\n",
      "\n",
      "def _has_db_creds() -> bool:\n",
      "    if not os.environ.get(\"MODEL_DB_URL\"):\n",
      "        return False\n",
      "    return bool(\n",
      "        os.environ.get(\"MODEL_DB_PASSWORD\")\n",
      "        or os.environ.get(\"PGPASSWORD\")\n",
      "        or (\"://\" in os.environ[\"MODEL_DB_URL\"] and \"@\" in os.environ[\"MODEL_DB_URL\"] and \":\" in os.environ[\"MODEL_DB_URL\"].split(\"@\")[0])\n",
      "    )\n",
      "\n",
      "\n",
      "@pytest.mark.integration\n",
      "def test_migrations_create_required_tables(monkeypatch):\n",
      "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
      "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@ ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tpath = Path(\"/mnt/e/env/ts/tslib/tests/test_migrations.py\")\n",
    "tpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tcode = r'''\\\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pytest\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "from nf_app.migrations import MigrationRunner, SCHEMA_NAME\n",
    "\n",
    "\n",
    "def _has_db_creds() -> bool:\n",
    "    if not os.environ.get(\"MODEL_DB_URL\"):\n",
    "        return False\n",
    "    return bool(\n",
    "        os.environ.get(\"MODEL_DB_PASSWORD\")\n",
    "        or os.environ.get(\"PGPASSWORD\")\n",
    "        or (\"://\" in os.environ[\"MODEL_DB_URL\"] and \"@\" in os.environ[\"MODEL_DB_URL\"] and \":\" in os.environ[\"MODEL_DB_URL\"].split(\"@\")[0])\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_migrations_create_required_tables(monkeypatch):\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set MODEL_DB_PASSWORD or PGPASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    try:\n",
    "        runner = MigrationRunner(model_engine=dbm.model_engine())\n",
    "        runner.apply_all()\n",
    "\n",
    "        required = [\n",
    "            f\"{SCHEMA_NAME}.schema_migrations\",\n",
    "            f\"{SCHEMA_NAME}.feature_prefix_rules\",\n",
    "            f\"{SCHEMA_NAME}.join_rules\",\n",
    "            f\"{SCHEMA_NAME}.experiments\",\n",
    "            f\"{SCHEMA_NAME}.run_plans\",\n",
    "            f\"{SCHEMA_NAME}.runs\",\n",
    "            f\"{SCHEMA_NAME}.model_versions\",\n",
    "        ]\n",
    "\n",
    "        with dbm.model_engine().connect() as conn:\n",
    "            for t in required:\n",
    "                exists = conn.execute(text(\"SELECT to_regclass(:t) IS NOT NULL\"), {\"t\": t}).scalar_one()\n",
    "                assert exists, f\"missing table: {t}\"\n",
    "\n",
    "            prefixes = conn.execute(\n",
    "                text(f\"SELECT prefix FROM {SCHEMA_NAME}.feature_prefix_rules ORDER BY prefix\")\n",
    "            ).scalars().all()\n",
    "            assert \"hist_\" in prefixes\n",
    "            assert \"futr_\" in prefixes\n",
    "            assert \"stat_\" in prefixes\n",
    "\n",
    "            # seed join ruleが入っていること（013にも関係）\n",
    "            rule_names = conn.execute(\n",
    "                text(f\"SELECT rule_name FROM {SCHEMA_NAME}.join_rules ORDER BY rule_name\")\n",
    "            ).scalars().all()\n",
    "            assert \"loto_join_v1\" in rule_names\n",
    "\n",
    "    finally:\n",
    "        dbm.dispose()\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_join_rules_allows_insert(monkeypatch):\n",
    "    \"\"\"\n",
    "    ID 013 Done: join_rulesにINSERTできること\n",
    "    \"\"\"\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set MODEL_DB_PASSWORD or PGPASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    try:\n",
    "        runner = MigrationRunner(model_engine=dbm.model_engine())\n",
    "        runner.apply_all()\n",
    "\n",
    "        test_name = \"pytest_join_rule\"\n",
    "        sql_text = \"SELECT 1 AS dummy;\"\n",
    "\n",
    "        with dbm.model_engine().begin() as conn:\n",
    "            conn.execute(text(f\"\"\"\n",
    "                INSERT INTO {SCHEMA_NAME}.join_rules(rule_name, enabled, priority, sql_text, description)\n",
    "                VALUES (:name, TRUE, 999, :sql, 'pytest insert check')\n",
    "                ON CONFLICT (rule_name) DO UPDATE\n",
    "                SET sql_text = EXCLUDED.sql_text,\n",
    "                    priority = EXCLUDED.priority,\n",
    "                    updated_ts = NOW();\n",
    "            \"\"\"), {\"name\": test_name, \"sql\": sql_text})\n",
    "\n",
    "        with dbm.model_engine().connect() as conn:\n",
    "            got = conn.execute(\n",
    "                text(f\"SELECT sql_text, priority FROM {SCHEMA_NAME}.join_rules WHERE rule_name=:name\"),\n",
    "                {\"name\": test_name}\n",
    "            ).one()\n",
    "            assert got[0].strip() == sql_text\n",
    "            assert int(got[1]) == 999\n",
    "\n",
    "    finally:\n",
    "        dbm.dispose()\n",
    "'''\n",
    "tpath.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Updated:\", tpath)\n",
    "print(tpath.read_text(encoding=\"utf-8\")[:800], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94305705",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-Migrate-joinrules-3 pytest 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43187b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                       [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.21s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_migrations.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a171d",
   "metadata": {},
   "source": [
    "## ID 014: enabled planだけ抽出できる\n",
    "\n",
    "目的：\n",
    "- model DB（neuralforecast.run_plans）に plan を upsert で登録できる\n",
    "- enabled=true の plan のみを priority昇順で抽出できる\n",
    "- pytestで担保（DB統合テスト：資格情報なければskip）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd639d7",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-DB-014-1 run_plans.py を作成\n",
    "\n",
    "実装先：/mnt/e/env/ts/tslib/src/nf_app/run_plans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a171f0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/run_plans.py\n",
      "\\\n",
      "from __future__ import annotations\n",
      "\n",
      "import json\n",
      "from dataclasses import dataclass\n",
      "from typing import Any, Dict, List, Optional\n",
      "\n",
      "from sqlalchemy import text\n",
      "from sqlalchemy.engine import Engine\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "\n",
      "from nf_app.errors import db_error\n",
      "\n",
      "SCHEMA_NAME = \"neuralforecast\"\n",
      "\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class RunPlan:\n",
      "    plan_id: int\n",
      "    enabled: bool\n",
      "    priority: int\n",
      "    plan_name: str\n",
      "    experiment_id: Optional[int]\n",
      "    model_name: str\n",
      "    run_mode: str\n",
      "    params_json: Dict[str, Any]\n",
      "    data_json: Dict[str, Any]\n",
      "    artifact_subdir: Optional[str]\n",
      "\n",
      "\n",
      "def ensure_experiment(engine: Engine, *, name: str, description: Optional[str] = None) -> int:\n",
      "    \"\"\"\n",
      "    experimentsに name をupsertし、experiment_idを返す\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with engine.begin() as conn:\n",
      "       ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/run_plans.py\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error\n",
    "\n",
    "SCHEMA_NAME = \"neuralforecast\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RunPlan:\n",
    "    plan_id: int\n",
    "    enabled: bool\n",
    "    priority: int\n",
    "    plan_name: str\n",
    "    experiment_id: Optional[int]\n",
    "    model_name: str\n",
    "    run_mode: str\n",
    "    params_json: Dict[str, Any]\n",
    "    data_json: Dict[str, Any]\n",
    "    artifact_subdir: Optional[str]\n",
    "\n",
    "\n",
    "def ensure_experiment(engine: Engine, *, name: str, description: Optional[str] = None) -> int:\n",
    "    \"\"\"\n",
    "    experimentsに name をupsertし、experiment_idを返す\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(\n",
    "                text(f\"\"\"\n",
    "                INSERT INTO {SCHEMA_NAME}.experiments(name, description)\n",
    "                VALUES (:name, :desc)\n",
    "                ON CONFLICT (name) DO UPDATE\n",
    "                SET description = COALESCE(EXCLUDED.description, {SCHEMA_NAME}.experiments.description);\n",
    "                \"\"\"),\n",
    "                {\"name\": name, \"desc\": description},\n",
    "            )\n",
    "        with engine.connect() as conn:\n",
    "            eid = conn.execute(\n",
    "                text(f\"SELECT experiment_id FROM {SCHEMA_NAME}.experiments WHERE name=:name\"),\n",
    "                {\"name\": name},\n",
    "            ).scalar_one()\n",
    "        return int(eid)\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\"DB_EXPERIMENT_UPSERT_FAILED\", \"Failed to upsert experiment.\", context={\"name\": name}, cause=e)\n",
    "\n",
    "\n",
    "def upsert_run_plan(\n",
    "    engine: Engine,\n",
    "    *,\n",
    "    plan_name: str,\n",
    "    enabled: bool = True,\n",
    "    priority: int = 100,\n",
    "    experiment_id: Optional[int] = None,\n",
    "    model_name: str,\n",
    "    run_mode: str,\n",
    "    params_json: Optional[Dict[str, Any]] = None,\n",
    "    data_json: Optional[Dict[str, Any]] = None,\n",
    "    artifact_subdir: Optional[str] = None,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    run_plansへupsertし、plan_idを返す（冪等）\n",
    "    - JSONBは文字列として渡して ::jsonb キャスト（psycopg依存を減らす）\n",
    "    \"\"\"\n",
    "    params_s = json.dumps(params_json or {}, ensure_ascii=False)\n",
    "    data_s = json.dumps(data_json or {}, ensure_ascii=False)\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(\n",
    "                text(f\"\"\"\n",
    "                INSERT INTO {SCHEMA_NAME}.run_plans(\n",
    "                    enabled, priority, plan_name, experiment_id,\n",
    "                    model_name, run_mode, params_json, data_json, artifact_subdir\n",
    "                )\n",
    "                VALUES (\n",
    "                    :enabled, :priority, :plan_name, :experiment_id,\n",
    "                    :model_name, :run_mode, :params_json::jsonb, :data_json::jsonb, :artifact_subdir\n",
    "                )\n",
    "                ON CONFLICT (plan_name) DO UPDATE\n",
    "                SET enabled = EXCLUDED.enabled,\n",
    "                    priority = EXCLUDED.priority,\n",
    "                    experiment_id = EXCLUDED.experiment_id,\n",
    "                    model_name = EXCLUDED.model_name,\n",
    "                    run_mode = EXCLUDED.run_mode,\n",
    "                    params_json = EXCLUDED.params_json,\n",
    "                    data_json = EXCLUDED.data_json,\n",
    "                    artifact_subdir = EXCLUDED.artifact_subdir,\n",
    "                    updated_ts = NOW();\n",
    "                \"\"\"),\n",
    "                {\n",
    "                    \"enabled\": enabled,\n",
    "                    \"priority\": priority,\n",
    "                    \"plan_name\": plan_name,\n",
    "                    \"experiment_id\": experiment_id,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"run_mode\": run_mode,\n",
    "                    \"params_json\": params_s,\n",
    "                    \"data_json\": data_s,\n",
    "                    \"artifact_subdir\": artifact_subdir,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            pid = conn.execute(\n",
    "                text(f\"SELECT plan_id FROM {SCHEMA_NAME}.run_plans WHERE plan_name=:plan_name\"),\n",
    "                {\"plan_name\": plan_name},\n",
    "            ).scalar_one()\n",
    "        return int(pid)\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\"DB_RUN_PLAN_UPSERT_FAILED\", \"Failed to upsert run_plan.\", context={\"plan_name\": plan_name}, cause=e)\n",
    "\n",
    "\n",
    "def list_enabled_run_plans(engine: Engine, *, limit: Optional[int] = None) -> List[RunPlan]:\n",
    "    \"\"\"\n",
    "    enabled=true の plan だけを priority昇順（同順位はplan_id昇順）で返す\n",
    "    \"\"\"\n",
    "    try:\n",
    "        q = f\"\"\"\n",
    "        SELECT\n",
    "            plan_id, enabled, priority, plan_name, experiment_id,\n",
    "            model_name, run_mode, params_json, data_json, artifact_subdir\n",
    "        FROM {SCHEMA_NAME}.run_plans\n",
    "        WHERE enabled = TRUE\n",
    "        ORDER BY priority ASC, plan_id ASC\n",
    "        \"\"\"\n",
    "        if limit is not None:\n",
    "            q += \" LIMIT :limit\"\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            rows = conn.execute(text(q), {\"limit\": limit} if limit is not None else {}).mappings().all()\n",
    "\n",
    "        out: List[RunPlan] = []\n",
    "        for r in rows:\n",
    "            out.append(\n",
    "                RunPlan(\n",
    "                    plan_id=int(r[\"plan_id\"]),\n",
    "                    enabled=bool(r[\"enabled\"]),\n",
    "                    priority=int(r[\"priority\"]),\n",
    "                    plan_name=str(r[\"plan_name\"]),\n",
    "                    experiment_id=int(r[\"experiment_id\"]) if r[\"experiment_id\"] is not None else None,\n",
    "                    model_name=str(r[\"model_name\"]),\n",
    "                    run_mode=str(r[\"run_mode\"]),\n",
    "                    params_json=dict(r[\"params_json\"] or {}),\n",
    "                    data_json=dict(r[\"data_json\"] or {}),\n",
    "                    artifact_subdir=str(r[\"artifact_subdir\"]) if r[\"artifact_subdir\"] is not None else None,\n",
    "                )\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\"DB_RUN_PLAN_LIST_FAILED\", \"Failed to list enabled run_plans.\", cause=e)\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p)\n",
    "print(p.read_text(encoding=\"utf-8\")[:800], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ffd3d",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-DB-014-2 test_migrations.py を更新（enabled plan抽出テストを追加）\n",
    "\n",
    "pytest先：/mnt/e/env/ts/tslib/tests/test_migrations.py\n",
    "（既存2テストは保持しつつ、3つ目のテストを追加する形でフル上書きします）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4370e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: /mnt/e/env/ts/tslib/tests/test_migrations.py\n",
      "\\\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pytest\n",
      "from sqlalchemy import text\n",
      "\n",
      "from nf_app.config import NFConfig\n",
      "from nf_app.db import DbManager\n",
      "from nf_app.migrations import MigrationRunner, SCHEMA_NAME\n",
      "from nf_app.run_plans import ensure_experiment, upsert_run_plan, list_enabled_run_plans\n",
      "\n",
      "\n",
      "def _has_db_creds() -> bool:\n",
      "    if not os.environ.get(\"MODEL_DB_URL\"):\n",
      "        return False\n",
      "    return bool(\n",
      "        os.environ.get(\"MODEL_DB_PASSWORD\")\n",
      "        or os.environ.get(\"PGPASSWORD\")\n",
      "        or (\"://\" in os.environ[\"MODEL_DB_URL\"] and \"@\" in os.environ[\"MODEL_DB_URL\"] and \":\" in os.environ[\"MODEL_DB_URL\"].split(\"@\")[0])\n",
      "    )\n",
      "\n",
      "\n",
      "@pytest.mark.integration\n",
      "def test_migrations_create_required_tables(monkeypatch):\n",
      "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
      "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:54 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tpath = Path(\"/mnt/e/env/ts/tslib/tests/test_migrations.py\")\n",
    "tpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tcode = r'''\\\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pytest\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "from nf_app.migrations import MigrationRunner, SCHEMA_NAME\n",
    "from nf_app.run_plans import ensure_experiment, upsert_run_plan, list_enabled_run_plans\n",
    "\n",
    "\n",
    "def _has_db_creds() -> bool:\n",
    "    if not os.environ.get(\"MODEL_DB_URL\"):\n",
    "        return False\n",
    "    return bool(\n",
    "        os.environ.get(\"MODEL_DB_PASSWORD\")\n",
    "        or os.environ.get(\"PGPASSWORD\")\n",
    "        or (\"://\" in os.environ[\"MODEL_DB_URL\"] and \"@\" in os.environ[\"MODEL_DB_URL\"] and \":\" in os.environ[\"MODEL_DB_URL\"].split(\"@\")[0])\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_migrations_create_required_tables(monkeypatch):\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set MODEL_DB_PASSWORD or PGPASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    try:\n",
    "        runner = MigrationRunner(model_engine=dbm.model_engine())\n",
    "        runner.apply_all()\n",
    "\n",
    "        required = [\n",
    "            f\"{SCHEMA_NAME}.schema_migrations\",\n",
    "            f\"{SCHEMA_NAME}.feature_prefix_rules\",\n",
    "            f\"{SCHEMA_NAME}.join_rules\",\n",
    "            f\"{SCHEMA_NAME}.experiments\",\n",
    "            f\"{SCHEMA_NAME}.run_plans\",\n",
    "            f\"{SCHEMA_NAME}.runs\",\n",
    "            f\"{SCHEMA_NAME}.model_versions\",\n",
    "        ]\n",
    "\n",
    "        with dbm.model_engine().connect() as conn:\n",
    "            for t in required:\n",
    "                exists = conn.execute(text(\"SELECT to_regclass(:t) IS NOT NULL\"), {\"t\": t}).scalar_one()\n",
    "                assert exists, f\"missing table: {t}\"\n",
    "\n",
    "            prefixes = conn.execute(\n",
    "                text(f\"SELECT prefix FROM {SCHEMA_NAME}.feature_prefix_rules ORDER BY prefix\")\n",
    "            ).scalars().all()\n",
    "            assert \"hist_\" in prefixes\n",
    "            assert \"futr_\" in prefixes\n",
    "            assert \"stat_\" in prefixes\n",
    "\n",
    "            rule_names = conn.execute(\n",
    "                text(f\"SELECT rule_name FROM {SCHEMA_NAME}.join_rules ORDER BY rule_name\")\n",
    "            ).scalars().all()\n",
    "            assert \"loto_join_v1\" in rule_names\n",
    "\n",
    "    finally:\n",
    "        dbm.dispose()\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_join_rules_allows_insert(monkeypatch):\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set MODEL_DB_PASSWORD or PGPASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    try:\n",
    "        runner = MigrationRunner(model_engine=dbm.model_engine())\n",
    "        runner.apply_all()\n",
    "\n",
    "        test_name = \"pytest_join_rule\"\n",
    "        sql_text = \"SELECT 1 AS dummy;\"\n",
    "\n",
    "        with dbm.model_engine().begin() as conn:\n",
    "            conn.execute(text(f\"\"\"\n",
    "                INSERT INTO {SCHEMA_NAME}.join_rules(rule_name, enabled, priority, sql_text, description)\n",
    "                VALUES (:name, TRUE, 999, :sql, 'pytest insert check')\n",
    "                ON CONFLICT (rule_name) DO UPDATE\n",
    "                SET sql_text = EXCLUDED.sql_text,\n",
    "                    priority = EXCLUDED.priority,\n",
    "                    updated_ts = NOW();\n",
    "            \"\"\"), {\"name\": test_name, \"sql\": sql_text})\n",
    "\n",
    "        with dbm.model_engine().connect() as conn:\n",
    "            got = conn.execute(\n",
    "                text(f\"SELECT sql_text, priority FROM {SCHEMA_NAME}.join_rules WHERE rule_name=:name\"),\n",
    "                {\"name\": test_name}\n",
    "            ).one()\n",
    "            assert got[0].strip() == sql_text\n",
    "            assert int(got[1]) == 999\n",
    "\n",
    "    finally:\n",
    "        dbm.dispose()\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_run_plans_list_enabled_only(monkeypatch):\n",
    "    \"\"\"\n",
    "    ID 014 Done: enabled planだけ抽出できる（priority昇順）\n",
    "    \"\"\"\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set MODEL_DB_PASSWORD or PGPASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    try:\n",
    "        runner = MigrationRunner(model_engine=dbm.model_engine())\n",
    "        runner.apply_all()\n",
    "\n",
    "        eng = dbm.model_engine()\n",
    "        eid = ensure_experiment(eng, name=\"pytest_exp_014\", description=\"for enabled plan filtering\")\n",
    "\n",
    "        # enabled: True (priority 20)\n",
    "        upsert_run_plan(\n",
    "            eng,\n",
    "            plan_name=\"pytest_plan_enabled_20\",\n",
    "            enabled=True,\n",
    "            priority=20,\n",
    "            experiment_id=eid,\n",
    "            model_name=\"AutoNBEATS\",\n",
    "            run_mode=\"DRY_RUN\",\n",
    "            params_json={\"a\": 1},\n",
    "            data_json={\"join_rule\": \"loto_join_v1\"},\n",
    "            artifact_subdir=\"pytest/014/enabled20\",\n",
    "        )\n",
    "        # enabled: True (priority 10) -> 先に来るはず\n",
    "        upsert_run_plan(\n",
    "            eng,\n",
    "            plan_name=\"pytest_plan_enabled_10\",\n",
    "            enabled=True,\n",
    "            priority=10,\n",
    "            experiment_id=eid,\n",
    "            model_name=\"AutoTFT\",\n",
    "            run_mode=\"DRY_RUN\",\n",
    "            params_json={\"b\": 2},\n",
    "            data_json={\"join_rule\": \"loto_join_v1\"},\n",
    "            artifact_subdir=\"pytest/014/enabled10\",\n",
    "        )\n",
    "        # enabled: False -> 出てこない\n",
    "        upsert_run_plan(\n",
    "            eng,\n",
    "            plan_name=\"pytest_plan_disabled\",\n",
    "            enabled=False,\n",
    "            priority=1,\n",
    "            experiment_id=eid,\n",
    "            model_name=\"AutoNBEATS\",\n",
    "            run_mode=\"DRY_RUN\",\n",
    "            params_json={},\n",
    "            data_json={},\n",
    "            artifact_subdir=\"pytest/014/disabled\",\n",
    "        )\n",
    "\n",
    "        plans = list_enabled_run_plans(eng)\n",
    "\n",
    "        names = [p.plan_name for p in plans]\n",
    "        assert \"pytest_plan_disabled\" not in names\n",
    "        assert \"pytest_plan_enabled_10\" in names\n",
    "        assert \"pytest_plan_enabled_20\" in names\n",
    "\n",
    "        # priority順になっていること（enabled_10 が enabled_20 より前）\n",
    "        i10 = names.index(\"pytest_plan_enabled_10\")\n",
    "        i20 = names.index(\"pytest_plan_enabled_20\")\n",
    "        assert i10 < i20\n",
    "\n",
    "    finally:\n",
    "        dbm.dispose()\n",
    "'''\n",
    "tpath.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Updated:\", tpath)\n",
    "print(tpath.read_text(encoding=\"utf-8\")[:900], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba0b16",
   "metadata": {},
   "source": [
    "## ✅ codecell: 1-DB-014-3 pytest 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41eeea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                      [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________ test_run_plans_list_enabled_only _______________________\u001b[0m\n",
      "\n",
      "self = <sqlalchemy.engine.base.Connection object at 0x7222ed5db010>\n",
      "dialect = <sqlalchemy.dialects.postgresql.psycopg.PGDialect_psycopg object at 0x7222ed5e0410>\n",
      "context = <sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg object at 0x7222ed5da210>\n",
      "statement = <sqlalchemy.dialects.postgresql.psycopg.PGCompiler_psycopg object at 0x7222ed5d9110>\n",
      "parameters = [{'artifact_subdir': 'pytest/014/enabled20', 'enabled': True, 'experiment_id': 1, 'model_name': 'AutoNBEATS', ...}]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_exec_single_context\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        dialect: Dialect,\u001b[90m\u001b[39;49;00m\n",
      "        context: ExecutionContext,\u001b[90m\u001b[39;49;00m\n",
      "        statement: Union[\u001b[96mstr\u001b[39;49;00m, Compiled],\u001b[90m\u001b[39;49;00m\n",
      "        parameters: Optional[_AnyMultiExecuteParams],\u001b[90m\u001b[39;49;00m\n",
      "    ) -> CursorResult[Any]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"continue the _execute_context() method for a single DBAPI\u001b[39;49;00m\n",
      "    \u001b[33m    cursor.execute() or cursor.executemany() call.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m dialect.bind_typing \u001b[95mis\u001b[39;49;00m BindTyping.SETINPUTSIZES:\u001b[90m\u001b[39;49;00m\n",
      "            generic_setinputsizes = context._prepare_set_input_sizes()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m generic_setinputsizes:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    dialect.do_set_input_sizes(\u001b[90m\u001b[39;49;00m\n",
      "                        context.cursor, generic_setinputsizes, context\u001b[90m\u001b[39;49;00m\n",
      "                    )\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mexcept\u001b[39;49;00m \u001b[96mBaseException\u001b[39;49;00m \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[96mself\u001b[39;49;00m._handle_dbapi_exception(\u001b[90m\u001b[39;49;00m\n",
      "                        e, \u001b[96mstr\u001b[39;49;00m(statement), parameters, \u001b[94mNone\u001b[39;49;00m, context\u001b[90m\u001b[39;49;00m\n",
      "                    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        cursor, str_statement, parameters = (\u001b[90m\u001b[39;49;00m\n",
      "            context.cursor,\u001b[90m\u001b[39;49;00m\n",
      "            context.statement,\u001b[90m\u001b[39;49;00m\n",
      "            context.parameters,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        effective_parameters: Optional[_AnyExecuteParams]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m context.executemany:\u001b[90m\u001b[39;49;00m\n",
      "            effective_parameters = parameters[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            effective_parameters = parameters\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._has_events \u001b[95mor\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.engine._has_events:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m fn \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.dispatch.before_cursor_execute:\u001b[90m\u001b[39;49;00m\n",
      "                str_statement, effective_parameters = fn(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    cursor,\u001b[90m\u001b[39;49;00m\n",
      "                    str_statement,\u001b[90m\u001b[39;49;00m\n",
      "                    effective_parameters,\u001b[90m\u001b[39;49;00m\n",
      "                    context,\u001b[90m\u001b[39;49;00m\n",
      "                    context.executemany,\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._echo:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m._log_info(str_statement)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            stats = context._get_cache_stats()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.engine.hide_parameters:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m._log_info(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33m[\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m] \u001b[39;49;00m\u001b[33m%r\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    stats,\u001b[90m\u001b[39;49;00m\n",
      "                    sql_util._repr_params(\u001b[90m\u001b[39;49;00m\n",
      "                        effective_parameters,\u001b[90m\u001b[39;49;00m\n",
      "                        batches=\u001b[94m10\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        ismulti=context.executemany,\u001b[90m\u001b[39;49;00m\n",
      "                    ),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m._log_info(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33m[\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m] [SQL parameters hidden due to hide_parameters=True]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    stats,\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        evt_handled: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m context.execute_style \u001b[95mis\u001b[39;49;00m ExecuteStyle.EXECUTEMANY:\u001b[90m\u001b[39;49;00m\n",
      "                effective_parameters = cast(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33m_CoreMultiExecuteParams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, effective_parameters\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.dialect._has_events:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mfor\u001b[39;49;00m fn \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.dialect.dispatch.do_executemany:\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94mif\u001b[39;49;00m fn(\u001b[90m\u001b[39;49;00m\n",
      "                            cursor,\u001b[90m\u001b[39;49;00m\n",
      "                            str_statement,\u001b[90m\u001b[39;49;00m\n",
      "                            effective_parameters,\u001b[90m\u001b[39;49;00m\n",
      "                            context,\u001b[90m\u001b[39;49;00m\n",
      "                        ):\u001b[90m\u001b[39;49;00m\n",
      "                            evt_handled = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m evt_handled:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[96mself\u001b[39;49;00m.dialect.do_executemany(\u001b[90m\u001b[39;49;00m\n",
      "                        cursor,\u001b[90m\u001b[39;49;00m\n",
      "                        str_statement,\u001b[90m\u001b[39;49;00m\n",
      "                        effective_parameters,\u001b[90m\u001b[39;49;00m\n",
      "                        context,\u001b[90m\u001b[39;49;00m\n",
      "                    )\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m effective_parameters \u001b[95mand\u001b[39;49;00m context.no_parameters:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.dialect._has_events:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mfor\u001b[39;49;00m fn \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.dialect.dispatch.do_execute_no_params:\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94mif\u001b[39;49;00m fn(cursor, str_statement, context):\u001b[90m\u001b[39;49;00m\n",
      "                            evt_handled = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m evt_handled:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[96mself\u001b[39;49;00m.dialect.do_execute_no_params(\u001b[90m\u001b[39;49;00m\n",
      "                        cursor, str_statement, context\u001b[90m\u001b[39;49;00m\n",
      "                    )\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                effective_parameters = cast(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33m_CoreSingleExecuteParams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, effective_parameters\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.dialect._has_events:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mfor\u001b[39;49;00m fn \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.dialect.dispatch.do_execute:\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94mif\u001b[39;49;00m fn(\u001b[90m\u001b[39;49;00m\n",
      "                            cursor,\u001b[90m\u001b[39;49;00m\n",
      "                            str_statement,\u001b[90m\u001b[39;49;00m\n",
      "                            effective_parameters,\u001b[90m\u001b[39;49;00m\n",
      "                            context,\u001b[90m\u001b[39;49;00m\n",
      "                        ):\u001b[90m\u001b[39;49;00m\n",
      "                            evt_handled = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m evt_handled:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[96mself\u001b[39;49;00m.dialect.do_execute(\u001b[90m\u001b[39;49;00m\n",
      "                        cursor, str_statement, effective_parameters, context\u001b[90m\u001b[39;49;00m\n",
      "                    )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/base.py\u001b[0m:1967: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/default.py\u001b[0m:952: in do_execute\n",
      "    \u001b[0mcursor.execute(statement, parameters)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <psycopg.Cursor [closed] [BAD] at 0x7222ed987530>\n",
      "query = '\\n                INSERT INTO neuralforecast.run_plans(\\n                    enabled, priority, plan_name, experiment...                artifact_subdir = EXCLUDED.artifact_subdir,\\n                    updated_ts = NOW();\\n                '\n",
      "params = {'artifact_subdir': 'pytest/014/enabled20', 'enabled': True, 'experiment_id': 1, 'model_name': 'AutoNBEATS', ...}\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mexecute\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        query: Query,\u001b[90m\u001b[39;49;00m\n",
      "        params: Params | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        *,\u001b[90m\u001b[39;49;00m\n",
      "        prepare: \u001b[96mbool\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        binary: \u001b[96mbool\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Self:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Execute a query or command to the database.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._conn.lock:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m._conn.wait(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[96mself\u001b[39;49;00m._execute_gen(query, params, prepare=prepare, binary=binary)\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m e._NO_TRACEBACK \u001b[94mas\u001b[39;49;00m ex:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m ex.with_traceback(\u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           psycopg.errors.SyntaxError: syntax error at or near \":\"\u001b[0m\n",
      "\u001b[1m\u001b[31mE           LINE 8:                     $5, $6, :params_json::jsonb, :data_json:...\u001b[0m\n",
      "\u001b[1m\u001b[31mE                                               ^\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/psycopg/cursor.py\u001b[0m:117: SyntaxError\n",
      "\n",
      "\u001b[33mThe above exception was the direct cause of the following exception:\u001b[0m\n",
      "\n",
      "engine = Engine(postgresql+psycopg://postgres@127.0.0.1:5432/model)\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mupsert_run_plan\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        engine: Engine,\u001b[90m\u001b[39;49;00m\n",
      "        *,\u001b[90m\u001b[39;49;00m\n",
      "        plan_name: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        enabled: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        priority: \u001b[96mint\u001b[39;49;00m = \u001b[94m100\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        experiment_id: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        model_name: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        run_mode: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        params_json: Optional[Dict[\u001b[96mstr\u001b[39;49;00m, Any]] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        data_json: Optional[Dict[\u001b[96mstr\u001b[39;49;00m, Any]] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        artifact_subdir: Optional[\u001b[96mstr\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> \u001b[96mint\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    run_plansへupsertし、plan_idを返す（冪等）\u001b[39;49;00m\n",
      "    \u001b[33m    - JSONBは文字列として渡して ::jsonb キャスト（psycopg依存を減らす）\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        params_s = json.dumps(params_json \u001b[95mor\u001b[39;49;00m {}, ensure_ascii=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        data_s = json.dumps(data_json \u001b[95mor\u001b[39;49;00m {}, ensure_ascii=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m engine.begin() \u001b[94mas\u001b[39;49;00m conn:\u001b[90m\u001b[39;49;00m\n",
      ">               conn.execute(\u001b[90m\u001b[39;49;00m\n",
      "                    text(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                INSERT INTO \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mSCHEMA_NAME\u001b[33m}\u001b[39;49;00m\u001b[33m.run_plans(\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    enabled, priority, plan_name, experiment_id,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    model_name, run_mode, params_json, data_json, artifact_subdir\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                )\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                VALUES (\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    :enabled, :priority, :plan_name, :experiment_id,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    :model_name, :run_mode, :params_json::jsonb, :data_json::jsonb, :artifact_subdir\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                )\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                ON CONFLICT (plan_name) DO UPDATE\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                SET enabled = EXCLUDED.enabled,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    priority = EXCLUDED.priority,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    experiment_id = EXCLUDED.experiment_id,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    model_name = EXCLUDED.model_name,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    run_mode = EXCLUDED.run_mode,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    params_json = EXCLUDED.params_json,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    data_json = EXCLUDED.data_json,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    artifact_subdir = EXCLUDED.artifact_subdir,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    updated_ts = NOW();\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                    {\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33menabled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: enabled,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mpriority\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: priority,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mplan_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: plan_name,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: experiment_id,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_name,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mrun_mode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: run_mode,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams_json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: params_s,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mdata_json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: data_s,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33martifact_subdir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: artifact_subdir,\u001b[90m\u001b[39;49;00m\n",
      "                    },\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31msrc/nf_app/run_plans.py\u001b[0m:78: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/base.py\u001b[0m:1419: in execute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m meth(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/sql/elements.py\u001b[0m:527: in _execute_on_connection\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m connection._execute_clauseelement(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/base.py\u001b[0m:1641: in _execute_clauseelement\n",
      "    \u001b[0mret = \u001b[96mself\u001b[39;49;00m._execute_context(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/base.py\u001b[0m:1846: in _execute_context\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._exec_single_context(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/base.py\u001b[0m:1986: in _exec_single_context\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._handle_dbapi_exception(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/base.py\u001b[0m:2363: in _handle_dbapi_exception\n",
      "    \u001b[0m\u001b[94mraise\u001b[39;49;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[94m2\u001b[39;49;00m]) \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96me\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/base.py\u001b[0m:1967: in _exec_single_context\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.dialect.do_execute(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/sqlalchemy/engine/default.py\u001b[0m:952: in do_execute\n",
      "    \u001b[0mcursor.execute(statement, parameters)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <psycopg.Cursor [closed] [BAD] at 0x7222ed987530>\n",
      "query = '\\n                INSERT INTO neuralforecast.run_plans(\\n                    enabled, priority, plan_name, experiment...                artifact_subdir = EXCLUDED.artifact_subdir,\\n                    updated_ts = NOW();\\n                '\n",
      "params = {'artifact_subdir': 'pytest/014/enabled20', 'enabled': True, 'experiment_id': 1, 'model_name': 'AutoNBEATS', ...}\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mexecute\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        query: Query,\u001b[90m\u001b[39;49;00m\n",
      "        params: Params | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        *,\u001b[90m\u001b[39;49;00m\n",
      "        prepare: \u001b[96mbool\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        binary: \u001b[96mbool\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Self:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Execute a query or command to the database.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._conn.lock:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m._conn.wait(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[96mself\u001b[39;49;00m._execute_gen(query, params, prepare=prepare, binary=binary)\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m e._NO_TRACEBACK \u001b[94mas\u001b[39;49;00m ex:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m ex.with_traceback(\u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           sqlalchemy.exc.ProgrammingError: (psycopg.errors.SyntaxError) syntax error at or near \":\"\u001b[0m\n",
      "\u001b[1m\u001b[31mE           LINE 8:                     $5, $6, :params_json::jsonb, :data_json:...\u001b[0m\n",
      "\u001b[1m\u001b[31mE                                               ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE           [SQL: \u001b[0m\n",
      "\u001b[1m\u001b[31mE                           INSERT INTO neuralforecast.run_plans(\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               enabled, priority, plan_name, experiment_id,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               model_name, run_mode, params_json, data_json, artifact_subdir\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           )\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           VALUES (\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               %(enabled)s, %(priority)s, %(plan_name)s, %(experiment_id)s,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               %(model_name)s, %(run_mode)s, :params_json::jsonb, :data_json::jsonb, %(artifact_subdir)s\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           )\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           ON CONFLICT (plan_name) DO UPDATE\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           SET enabled = EXCLUDED.enabled,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               priority = EXCLUDED.priority,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               experiment_id = EXCLUDED.experiment_id,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               model_name = EXCLUDED.model_name,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               run_mode = EXCLUDED.run_mode,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               params_json = EXCLUDED.params_json,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               data_json = EXCLUDED.data_json,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               artifact_subdir = EXCLUDED.artifact_subdir,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                               updated_ts = NOW();\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           ]\u001b[0m\n",
      "\u001b[1m\u001b[31mE           [parameters: {'enabled': True, 'priority': 20, 'plan_name': 'pytest_plan_enabled_20', 'experiment_id': 1, 'model_name': 'AutoNBEATS', 'run_mode': 'DRY_RUN', 'artifact_subdir': 'pytest/014/enabled20'}]\u001b[0m\n",
      "\u001b[1m\u001b[31mE           (Background on this error at: https://sqlalche.me/e/20/f405)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/psycopg/cursor.py\u001b[0m:117: ProgrammingError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7222ee9ae310>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.integration\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_run_plans_list_enabled_only\u001b[39;49;00m(monkeypatch):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    ID 014 Done: enabled planだけ抽出できる（priority昇順）\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        monkeypatch.setenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mDATASET_DB_URL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpostgresql+psycopg://postgres@127.0.0.1:5432/dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        monkeypatch.setenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mMODEL_DB_URL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,   \u001b[33m\"\u001b[39;49;00m\u001b[33mpostgresql+psycopg://postgres@127.0.0.1:5432/model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        monkeypatch.setenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mNF_STRICT_VALIDATION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        monkeypatch.setenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mNF_ARTIFACT_ROOT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/tslib_artifacts_for_test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        monkeypatch.setenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mNF_RUN_MODE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mDRY_RUN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m _has_db_creds():\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mDB credentials not provided. Set MODEL_DB_PASSWORD or PGPASSWORD.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        cfg = NFConfig.from_env(project_root=Path(\u001b[33m\"\u001b[39;49;00m\u001b[33m/mnt/e/env/ts/tslib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "        dbm = DbManager(cfg=cfg, echo_sql=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            runner = MigrationRunner(model_engine=dbm.model_engine())\u001b[90m\u001b[39;49;00m\n",
      "            runner.apply_all()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            eng = dbm.model_engine()\u001b[90m\u001b[39;49;00m\n",
      "            eid = ensure_experiment(eng, name=\u001b[33m\"\u001b[39;49;00m\u001b[33mpytest_exp_014\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, description=\u001b[33m\"\u001b[39;49;00m\u001b[33mfor enabled plan filtering\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# enabled: True (priority 20)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">           upsert_run_plan(\u001b[90m\u001b[39;49;00m\n",
      "                eng,\u001b[90m\u001b[39;49;00m\n",
      "                plan_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mpytest_plan_enabled_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                enabled=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                priority=\u001b[94m20\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                experiment_id=eid,\u001b[90m\u001b[39;49;00m\n",
      "                model_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mAutoNBEATS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                run_mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mDRY_RUN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                params_json={\u001b[33m\"\u001b[39;49;00m\u001b[33ma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m1\u001b[39;49;00m},\u001b[90m\u001b[39;49;00m\n",
      "                data_json={\u001b[33m\"\u001b[39;49;00m\u001b[33mjoin_rule\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mloto_join_v1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\u001b[90m\u001b[39;49;00m\n",
      "                artifact_subdir=\u001b[33m\"\u001b[39;49;00m\u001b[33mpytest/014/enabled20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_migrations.py\u001b[0m:140: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "engine = Engine(postgresql+psycopg://postgres@127.0.0.1:5432/model)\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mupsert_run_plan\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        engine: Engine,\u001b[90m\u001b[39;49;00m\n",
      "        *,\u001b[90m\u001b[39;49;00m\n",
      "        plan_name: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        enabled: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        priority: \u001b[96mint\u001b[39;49;00m = \u001b[94m100\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        experiment_id: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        model_name: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        run_mode: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        params_json: Optional[Dict[\u001b[96mstr\u001b[39;49;00m, Any]] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        data_json: Optional[Dict[\u001b[96mstr\u001b[39;49;00m, Any]] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        artifact_subdir: Optional[\u001b[96mstr\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> \u001b[96mint\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    run_plansへupsertし、plan_idを返す（冪等）\u001b[39;49;00m\n",
      "    \u001b[33m    - JSONBは文字列として渡して ::jsonb キャスト（psycopg依存を減らす）\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        params_s = json.dumps(params_json \u001b[95mor\u001b[39;49;00m {}, ensure_ascii=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        data_s = json.dumps(data_json \u001b[95mor\u001b[39;49;00m {}, ensure_ascii=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m engine.begin() \u001b[94mas\u001b[39;49;00m conn:\u001b[90m\u001b[39;49;00m\n",
      "                conn.execute(\u001b[90m\u001b[39;49;00m\n",
      "                    text(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                INSERT INTO \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mSCHEMA_NAME\u001b[33m}\u001b[39;49;00m\u001b[33m.run_plans(\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    enabled, priority, plan_name, experiment_id,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    model_name, run_mode, params_json, data_json, artifact_subdir\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                )\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                VALUES (\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    :enabled, :priority, :plan_name, :experiment_id,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    :model_name, :run_mode, :params_json::jsonb, :data_json::jsonb, :artifact_subdir\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                )\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                ON CONFLICT (plan_name) DO UPDATE\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                SET enabled = EXCLUDED.enabled,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    priority = EXCLUDED.priority,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    experiment_id = EXCLUDED.experiment_id,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    model_name = EXCLUDED.model_name,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    run_mode = EXCLUDED.run_mode,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    params_json = EXCLUDED.params_json,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    data_json = EXCLUDED.data_json,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    artifact_subdir = EXCLUDED.artifact_subdir,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                    updated_ts = NOW();\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m                \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                    {\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33menabled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: enabled,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mpriority\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: priority,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mplan_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: plan_name,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: experiment_id,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_name,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mrun_mode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: run_mode,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams_json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: params_s,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33mdata_json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: data_s,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[33m\"\u001b[39;49;00m\u001b[33martifact_subdir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: artifact_subdir,\u001b[90m\u001b[39;49;00m\n",
      "                    },\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m engine.connect() \u001b[94mas\u001b[39;49;00m conn:\u001b[90m\u001b[39;49;00m\n",
      "                pid = conn.execute(\u001b[90m\u001b[39;49;00m\n",
      "                    text(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mSELECT plan_id FROM \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mSCHEMA_NAME\u001b[33m}\u001b[39;49;00m\u001b[33m.run_plans WHERE plan_name=:plan_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                    {\u001b[33m\"\u001b[39;49;00m\u001b[33mplan_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: plan_name},\u001b[90m\u001b[39;49;00m\n",
      "                ).scalar_one()\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mint\u001b[39;49;00m(pid)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m SQLAlchemyError \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m db_error(\u001b[33m\"\u001b[39;49;00m\u001b[33mDB_RUN_PLAN_UPSERT_FAILED\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mFailed to upsert run_plan.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, context={\u001b[33m\"\u001b[39;49;00m\u001b[33mplan_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: plan_name}, cause=e)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           nf_app.errors.NFError: Failed to upsert run_plan.\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31msrc/nf_app/run_plans.py\u001b[0m:120: NFError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_migrations.py::\u001b[1mtest_run_plans_list_enabled_only\u001b[0m - nf_app.errors.NFError: Failed to upsert run_plan.\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.35s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_migrations.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48786386",
   "metadata": {},
   "source": [
    "## run_plans upsert のSQL修正\n",
    "\n",
    "SQLAlchemy text() で `:param::jsonb` を書くと bind解析されず `:` がSQLに残ってSyntaxErrorになる。\n",
    "`CAST(:param AS JSONB)` に変更して解消する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1930d",
   "metadata": {},
   "source": [
    "## ✅ codecell（/mnt/e/env/ts/tslib/src/nf_app/run_plans.py をフル上書き）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc79aedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/run_plans.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/run_plans.py\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error\n",
    "\n",
    "SCHEMA_NAME = \"neuralforecast\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RunPlan:\n",
    "    plan_id: int\n",
    "    enabled: bool\n",
    "    priority: int\n",
    "    plan_name: str\n",
    "    experiment_id: Optional[int]\n",
    "    model_name: str\n",
    "    run_mode: str\n",
    "    params_json: Dict[str, Any]\n",
    "    data_json: Dict[str, Any]\n",
    "    artifact_subdir: Optional[str]\n",
    "\n",
    "\n",
    "def ensure_experiment(engine: Engine, *, name: str, description: Optional[str] = None) -> int:\n",
    "    \"\"\"experimentsに name をupsertし、experiment_idを返す\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(\n",
    "                text(f\"\"\"\n",
    "                INSERT INTO {SCHEMA_NAME}.experiments(name, description)\n",
    "                VALUES (:name, :desc)\n",
    "                ON CONFLICT (name) DO UPDATE\n",
    "                SET description = COALESCE(EXCLUDED.description, {SCHEMA_NAME}.experiments.description);\n",
    "                \"\"\"),\n",
    "                {\"name\": name, \"desc\": description},\n",
    "            )\n",
    "        with engine.connect() as conn:\n",
    "            eid = conn.execute(\n",
    "                text(f\"SELECT experiment_id FROM {SCHEMA_NAME}.experiments WHERE name=:name\"),\n",
    "                {\"name\": name},\n",
    "            ).scalar_one()\n",
    "        return int(eid)\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_EXPERIMENT_UPSERT_FAILED\",\n",
    "            \"Failed to upsert experiment.\",\n",
    "            context={\"name\": name},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "\n",
    "def upsert_run_plan(\n",
    "    engine: Engine,\n",
    "    *,\n",
    "    plan_name: str,\n",
    "    enabled: bool = True,\n",
    "    priority: int = 100,\n",
    "    experiment_id: Optional[int] = None,\n",
    "    model_name: str,\n",
    "    run_mode: str,\n",
    "    params_json: Optional[Dict[str, Any]] = None,\n",
    "    data_json: Optional[Dict[str, Any]] = None,\n",
    "    artifact_subdir: Optional[str] = None,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    run_plansへupsertし、plan_idを返す（冪等）\n",
    "    NOTE: SQLAlchemy text() では ':param::jsonb' がbindとして解釈されないことがあるため\n",
    "          CAST(:param AS JSONB) を使う。\n",
    "    \"\"\"\n",
    "    params_s = json.dumps(params_json or {}, ensure_ascii=False)\n",
    "    data_s = json.dumps(data_json or {}, ensure_ascii=False)\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(\n",
    "                text(f\"\"\"\n",
    "                INSERT INTO {SCHEMA_NAME}.run_plans(\n",
    "                    enabled, priority, plan_name, experiment_id,\n",
    "                    model_name, run_mode, params_json, data_json, artifact_subdir\n",
    "                )\n",
    "                VALUES (\n",
    "                    :enabled, :priority, :plan_name, :experiment_id,\n",
    "                    :model_name, :run_mode,\n",
    "                    CAST(:params_json AS JSONB),\n",
    "                    CAST(:data_json AS JSONB),\n",
    "                    :artifact_subdir\n",
    "                )\n",
    "                ON CONFLICT (plan_name) DO UPDATE\n",
    "                SET enabled = EXCLUDED.enabled,\n",
    "                    priority = EXCLUDED.priority,\n",
    "                    experiment_id = EXCLUDED.experiment_id,\n",
    "                    model_name = EXCLUDED.model_name,\n",
    "                    run_mode = EXCLUDED.run_mode,\n",
    "                    params_json = EXCLUDED.params_json,\n",
    "                    data_json = EXCLUDED.data_json,\n",
    "                    artifact_subdir = EXCLUDED.artifact_subdir,\n",
    "                    updated_ts = NOW();\n",
    "                \"\"\"),\n",
    "                {\n",
    "                    \"enabled\": enabled,\n",
    "                    \"priority\": priority,\n",
    "                    \"plan_name\": plan_name,\n",
    "                    \"experiment_id\": experiment_id,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"run_mode\": run_mode,\n",
    "                    \"params_json\": params_s,\n",
    "                    \"data_json\": data_s,\n",
    "                    \"artifact_subdir\": artifact_subdir,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            pid = conn.execute(\n",
    "                text(f\"SELECT plan_id FROM {SCHEMA_NAME}.run_plans WHERE plan_name=:plan_name\"),\n",
    "                {\"plan_name\": plan_name},\n",
    "            ).scalar_one()\n",
    "        return int(pid)\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_RUN_PLAN_UPSERT_FAILED\",\n",
    "            \"Failed to upsert run_plan.\",\n",
    "            context={\"plan_name\": plan_name},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "\n",
    "def list_enabled_run_plans(engine: Engine, *, limit: Optional[int] = None) -> List[RunPlan]:\n",
    "    \"\"\"enabled=true の plan だけを priority昇順（同順位はplan_id昇順）で返す\"\"\"\n",
    "    try:\n",
    "        q = f\"\"\"\n",
    "        SELECT\n",
    "            plan_id, enabled, priority, plan_name, experiment_id,\n",
    "            model_name, run_mode, params_json, data_json, artifact_subdir\n",
    "        FROM {SCHEMA_NAME}.run_plans\n",
    "        WHERE enabled = TRUE\n",
    "        ORDER BY priority ASC, plan_id ASC\n",
    "        \"\"\"\n",
    "        if limit is not None:\n",
    "            q += \" LIMIT :limit\"\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            rows = conn.execute(text(q), {\"limit\": limit} if limit is not None else {}).mappings().all()\n",
    "\n",
    "        out: List[RunPlan] = []\n",
    "        for r in rows:\n",
    "            out.append(\n",
    "                RunPlan(\n",
    "                    plan_id=int(r[\"plan_id\"]),\n",
    "                    enabled=bool(r[\"enabled\"]),\n",
    "                    priority=int(r[\"priority\"]),\n",
    "                    plan_name=str(r[\"plan_name\"]),\n",
    "                    experiment_id=int(r[\"experiment_id\"]) if r[\"experiment_id\"] is not None else None,\n",
    "                    model_name=str(r[\"model_name\"]),\n",
    "                    run_mode=str(r[\"run_mode\"]),\n",
    "                    params_json=dict(r[\"params_json\"] or {}),\n",
    "                    data_json=dict(r[\"data_json\"] or {}),\n",
    "                    artifact_subdir=str(r[\"artifact_subdir\"]) if r[\"artifact_subdir\"] is not None else None,\n",
    "                )\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\"DB_RUN_PLAN_LIST_FAILED\", \"Failed to list enabled run_plans.\", cause=e)\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2c7ed",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest再実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a80ef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.25s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_migrations.py::test_run_plans_list_enabled_only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c0390",
   "metadata": {},
   "source": [
    "## ID 015 dataset: forecast保存テーブル（preds/backtest）\n",
    "\n",
    "dataset DB側に `neuralforecast.forecast_preds` と `neuralforecast.forecast_backtests` を作成し、\n",
    "予測（yhat）をINSERTできることをpytestで担保する。\n",
    "\n",
    "方針:\n",
    "- 既存のmodel DB migrationは変更せず、migrations.pyに dataset用関数を追記\n",
    "- JSONBは CAST(:x AS JSONB) で安全にバインドする（:x::jsonb でコケる事故回避）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eccb38c",
   "metadata": {},
   "source": [
    "## codecell（migrations.py に dataset migration 関数を追記）\n",
    "\n",
    "フルパス: /mnt/e/env/ts/tslib/src/nf_app/migrations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e59c6757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended dataset migrations to: /mnt/e/env/ts/tslib/src/nf_app/migrations.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"/mnt/e/env/ts/tslib/src/nf_app/migrations.py\")\n",
    "src = path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "sentinel = \"def apply_dataset_migrations(\"\n",
    "if sentinel in src:\n",
    "    print(\"apply_dataset_migrations already exists. Skip append.\")\n",
    "else:\n",
    "    append = r'''\n",
    "\n",
    "# =========================\n",
    "# Dataset DB migrations (ID 015)\n",
    "# =========================\n",
    "DATASET_MIGRATION_VERSION = 1\n",
    "\n",
    "def apply_dataset_migrations(dataset_engine):\n",
    "    \"\"\"\n",
    "    dataset DB側の予測保存テーブル（preds/backtest）を作成する（冪等）\n",
    "    - cross-db FKは張れないので run_id/plan_id は参照キーとして保持するだけ\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from sqlalchemy import text\n",
    "    from sqlalchemy.exc import SQLAlchemyError\n",
    "    from nf_app.errors import db_error\n",
    "\n",
    "    try:\n",
    "        with dataset_engine.begin() as conn:\n",
    "            # schema + schema_migrations\n",
    "            conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME};\"))\n",
    "            conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.schema_migrations(\n",
    "                id INTEGER PRIMARY KEY DEFAULT 1 CHECK (id = 1),\n",
    "                version INTEGER NOT NULL,\n",
    "                updated_ts TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    "            );\n",
    "            \"\"\"))\n",
    "            # version upsert\n",
    "            conn.execute(\n",
    "                text(f\"\"\"\n",
    "                INSERT INTO {SCHEMA_NAME}.schema_migrations(id, version)\n",
    "                VALUES (1, :v)\n",
    "                ON CONFLICT (id) DO UPDATE\n",
    "                SET version = GREATEST({SCHEMA_NAME}.schema_migrations.version, EXCLUDED.version),\n",
    "                    updated_ts = NOW();\n",
    "                \"\"\"),\n",
    "                {\"v\": DATASET_MIGRATION_VERSION},\n",
    "            )\n",
    "\n",
    "            # -------- forecast_preds (future prediction)\n",
    "            conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.forecast_preds(\n",
    "                pred_id BIGSERIAL PRIMARY KEY,\n",
    "                run_id UUID NOT NULL,\n",
    "                plan_id BIGINT NULL,\n",
    "                model_name TEXT NOT NULL,\n",
    "                unique_id TEXT NOT NULL,\n",
    "                ds DATE NOT NULL,\n",
    "                horizon INTEGER NULL,\n",
    "                cutoff DATE NULL,\n",
    "                yhat DOUBLE PRECISION NOT NULL,\n",
    "                yhat_lower DOUBLE PRECISION NULL,\n",
    "                yhat_upper DOUBLE PRECISION NULL,\n",
    "                extra_json JSONB NOT NULL DEFAULT '{{}}'::jsonb,\n",
    "                created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "                UNIQUE (run_id, model_name, unique_id, ds)\n",
    "            );\n",
    "            \"\"\"))\n",
    "\n",
    "            conn.execute(text(f\"CREATE INDEX IF NOT EXISTS ix_forecast_preds_run_id ON {SCHEMA_NAME}.forecast_preds(run_id);\"))\n",
    "            conn.execute(text(f\"CREATE INDEX IF NOT EXISTS ix_forecast_preds_uid_ds ON {SCHEMA_NAME}.forecast_preds(unique_id, ds);\"))\n",
    "            conn.execute(text(f\"CREATE INDEX IF NOT EXISTS ix_forecast_preds_plan_id ON {SCHEMA_NAME}.forecast_preds(plan_id);\"))\n",
    "\n",
    "            # -------- forecast_backtests (historical backtest prediction)\n",
    "            conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.forecast_backtests(\n",
    "                bt_id BIGSERIAL PRIMARY KEY,\n",
    "                run_id UUID NOT NULL,\n",
    "                plan_id BIGINT NULL,\n",
    "                model_name TEXT NOT NULL,\n",
    "                unique_id TEXT NOT NULL,\n",
    "                cutoff DATE NOT NULL,\n",
    "                ds DATE NOT NULL,\n",
    "                horizon INTEGER NULL,\n",
    "                y DOUBLE PRECISION NULL,\n",
    "                yhat DOUBLE PRECISION NOT NULL,\n",
    "                extra_json JSONB NOT NULL DEFAULT '{{}}'::jsonb,\n",
    "                created_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "                UNIQUE (run_id, model_name, unique_id, cutoff, ds)\n",
    "            );\n",
    "            \"\"\"))\n",
    "\n",
    "            conn.execute(text(f\"CREATE INDEX IF NOT EXISTS ix_forecast_bt_run_id ON {SCHEMA_NAME}.forecast_backtests(run_id);\"))\n",
    "            conn.execute(text(f\"CREATE INDEX IF NOT EXISTS ix_forecast_bt_uid_ds ON {SCHEMA_NAME}.forecast_backtests(unique_id, ds);\"))\n",
    "            conn.execute(text(f\"CREATE INDEX IF NOT EXISTS ix_forecast_bt_cutoff ON {SCHEMA_NAME}.forecast_backtests(cutoff);\"))\n",
    "            conn.execute(text(f\"CREATE INDEX IF NOT EXISTS ix_forecast_bt_plan_id ON {SCHEMA_NAME}.forecast_backtests(plan_id);\"))\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_DATASET_MIGRATION_FAILED\",\n",
    "            \"Failed to apply dataset migrations (forecast tables).\",\n",
    "            cause=e,\n",
    "        )\n",
    "'''\n",
    "    path.write_text(src + append, encoding=\"utf-8\")\n",
    "    print(\"Appended dataset migrations to:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57700323",
   "metadata": {},
   "source": [
    "## codecell（test_migrations.py に ID015 テストを追記）\n",
    "\n",
    "フルパス: /mnt/e/env/ts/tslib/tests/test_migrations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c72860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended ID015 test to: /mnt/e/env/ts/tslib/tests/test_migrations.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tpath = Path(\"/mnt/e/env/ts/tslib/tests/test_migrations.py\")\n",
    "tsrc = tpath.read_text(encoding=\"utf-8\")\n",
    "\n",
    "test_name = \"test_dataset_forecast_tables_allow_insert\"\n",
    "if test_name in tsrc:\n",
    "    print(\"ID015 test already exists. Skip append.\")\n",
    "else:\n",
    "    add = r'''\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_dataset_forecast_tables_allow_insert(monkeypatch):\n",
    "    \"\"\"\n",
    "    ID 015 Done: dataset側の forecast_preds / forecast_backtests にINSERTできる\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import uuid\n",
    "    from datetime import date\n",
    "    from pathlib import Path\n",
    "    from sqlalchemy import text\n",
    "\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set MODEL_DB_PASSWORD or PGPASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "\n",
    "    # model側 migrations は既存の runner で適用（既存テストと同じ流れ）\n",
    "    runner = MigrationRunner(model_engine=dbm.model_engine())\n",
    "    runner.apply_all()\n",
    "\n",
    "    # dataset側 migrations（ID015）\n",
    "    from nf_app.migrations import apply_dataset_migrations\n",
    "    apply_dataset_migrations(dbm.dataset_engine())\n",
    "\n",
    "    ds_eng = dbm.dataset_engine()\n",
    "\n",
    "    run_id = str(uuid.uuid4())\n",
    "    model_name = \"AutoNBEATS\"\n",
    "    unique_id = \"pytest_uid_015\"\n",
    "\n",
    "    # --- forecast_preds insert\n",
    "    with ds_eng.begin() as conn:\n",
    "        conn.execute(\n",
    "            text(f\"\"\"\n",
    "            INSERT INTO {SCHEMA_NAME}.forecast_preds(\n",
    "                run_id, plan_id, model_name, unique_id, ds, horizon, cutoff, yhat, yhat_lower, yhat_upper, extra_json\n",
    "            ) VALUES (\n",
    "                :run_id, :plan_id, :model_name, :unique_id, :ds, :h, :cutoff, :yhat, :lo, :hi, CAST(:extra AS JSONB)\n",
    "            )\n",
    "            \"\"\"),\n",
    "            {\n",
    "                \"run_id\": run_id,\n",
    "                \"plan_id\": 999,\n",
    "                \"model_name\": model_name,\n",
    "                \"unique_id\": unique_id,\n",
    "                \"ds\": date(2020, 1, 2),\n",
    "                \"h\": 1,\n",
    "                \"cutoff\": date(2020, 1, 1),\n",
    "                \"yhat\": 123.0,\n",
    "                \"lo\": 100.0,\n",
    "                \"hi\": 150.0,\n",
    "                \"extra\": json.dumps({\"q\": [0.1, 0.9]}, ensure_ascii=False),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    with ds_eng.connect() as conn:\n",
    "        n = conn.execute(\n",
    "            text(f\"SELECT COUNT(*) FROM {SCHEMA_NAME}.forecast_preds WHERE run_id=:run_id\"),\n",
    "            {\"run_id\": run_id},\n",
    "        ).scalar_one()\n",
    "    assert int(n) == 1\n",
    "\n",
    "    # --- forecast_backtests insert\n",
    "    with ds_eng.begin() as conn:\n",
    "        conn.execute(\n",
    "            text(f\"\"\"\n",
    "            INSERT INTO {SCHEMA_NAME}.forecast_backtests(\n",
    "                run_id, plan_id, model_name, unique_id, cutoff, ds, horizon, y, yhat, extra_json\n",
    "            ) VALUES (\n",
    "                :run_id, :plan_id, :model_name, :unique_id, :cutoff, :ds, :h, :y, :yhat, CAST(:extra AS JSONB)\n",
    "            )\n",
    "            \"\"\"),\n",
    "            {\n",
    "                \"run_id\": run_id,\n",
    "                \"plan_id\": 999,\n",
    "                \"model_name\": model_name,\n",
    "                \"unique_id\": unique_id,\n",
    "                \"cutoff\": date(2020, 1, 1),\n",
    "                \"ds\": date(2020, 1, 2),\n",
    "                \"h\": 1,\n",
    "                \"y\": 120.0,\n",
    "                \"yhat\": 123.0,\n",
    "                \"extra\": json.dumps({\"fold\": 0}, ensure_ascii=False),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    with ds_eng.connect() as conn:\n",
    "        m = conn.execute(\n",
    "            text(f\"SELECT COUNT(*) FROM {SCHEMA_NAME}.forecast_backtests WHERE run_id=:run_id\"),\n",
    "            {\"run_id\": run_id},\n",
    "        ).scalar_one()\n",
    "    assert int(m) == 1\n",
    "'''\n",
    "    tpath.write_text(tsrc + add, encoding=\"utf-8\")\n",
    "    print(\"Appended ID015 test to:\", tpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ec31e",
   "metadata": {},
   "source": [
    "## codecell（pytest：ID015だけ実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1f92f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.35s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_migrations.py::test_dataset_forecast_tables_allow_insert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397a195",
   "metadata": {},
   "source": [
    "## ID 020 information_schema内省（列一覧・型・NULL）\n",
    "\n",
    "目的:\n",
    "- dataset DBの任意テーブルについて、information_schemaから列メタ情報を取得する\n",
    "- 列が増減しても壊れない（＝ハードコードで列セットを固定しない）\n",
    "\n",
    "Done条件:\n",
    "- テストで一時テーブルを作る → 内省 → ALTERで列追加 → 内省 → 追加列が検出できる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2757ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/introspect.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/introspect.py\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ColumnInfo:\n",
    "    table_schema: str\n",
    "    table_name: str\n",
    "    column_name: str\n",
    "    ordinal_position: int\n",
    "    data_type: str\n",
    "    udt_name: str\n",
    "    is_nullable: bool\n",
    "    column_default: Optional[str]\n",
    "    character_maximum_length: Optional[int]\n",
    "    numeric_precision: Optional[int]\n",
    "    numeric_scale: Optional[int]\n",
    "    datetime_precision: Optional[int]\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"table_schema\": self.table_schema,\n",
    "            \"table_name\": self.table_name,\n",
    "            \"column_name\": self.column_name,\n",
    "            \"ordinal_position\": self.ordinal_position,\n",
    "            \"data_type\": self.data_type,\n",
    "            \"udt_name\": self.udt_name,\n",
    "            \"is_nullable\": self.is_nullable,\n",
    "            \"column_default\": self.column_default,\n",
    "            \"character_maximum_length\": self.character_maximum_length,\n",
    "            \"numeric_precision\": self.numeric_precision,\n",
    "            \"numeric_scale\": self.numeric_scale,\n",
    "            \"datetime_precision\": self.datetime_precision,\n",
    "        }\n",
    "\n",
    "\n",
    "def list_columns(engine: Engine, *, schema: str, table: str) -> List[ColumnInfo]:\n",
    "    \"\"\"\n",
    "    information_schema.columns を使って列メタ情報を返す（列の増減に強い）\n",
    "    - PostgreSQLに依存しすぎない範囲で取れる情報を取る\n",
    "    - ORDER BY ordinal_position で安定順序\n",
    "    \"\"\"\n",
    "    q = text(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            table_schema,\n",
    "            table_name,\n",
    "            column_name,\n",
    "            ordinal_position,\n",
    "            data_type,\n",
    "            udt_name,\n",
    "            is_nullable,\n",
    "            column_default,\n",
    "            character_maximum_length,\n",
    "            numeric_precision,\n",
    "            numeric_scale,\n",
    "            datetime_precision\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = :schema\n",
    "          AND table_name = :table\n",
    "        ORDER BY ordinal_position;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            rows = conn.execute(q, {\"schema\": schema, \"table\": table}).mappings().all()\n",
    "\n",
    "        out: List[ColumnInfo] = []\n",
    "        for r in rows:\n",
    "            out.append(\n",
    "                ColumnInfo(\n",
    "                    table_schema=str(r[\"table_schema\"]),\n",
    "                    table_name=str(r[\"table_name\"]),\n",
    "                    column_name=str(r[\"column_name\"]),\n",
    "                    ordinal_position=int(r[\"ordinal_position\"]),\n",
    "                    data_type=str(r[\"data_type\"]),\n",
    "                    udt_name=str(r[\"udt_name\"]),\n",
    "                    is_nullable=(str(r[\"is_nullable\"]).upper() == \"YES\"),\n",
    "                    column_default=str(r[\"column_default\"]) if r[\"column_default\"] is not None else None,\n",
    "                    character_maximum_length=int(r[\"character_maximum_length\"]) if r[\"character_maximum_length\"] is not None else None,\n",
    "                    numeric_precision=int(r[\"numeric_precision\"]) if r[\"numeric_precision\"] is not None else None,\n",
    "                    numeric_scale=int(r[\"numeric_scale\"]) if r[\"numeric_scale\"] is not None else None,\n",
    "                    datetime_precision=int(r[\"datetime_precision\"]) if r[\"datetime_precision\"] is not None else None,\n",
    "                )\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_INTROSPECT_COLUMNS_FAILED\",\n",
    "            \"Failed to introspect columns via information_schema.\",\n",
    "            context={\"schema\": schema, \"table\": table},\n",
    "            cause=e,\n",
    "        )\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698120d",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest）/mnt/e/env/ts/tslib/tests/test_introspect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82490313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_introspect.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tp = Path(\"/mnt/e/env/ts/tslib/tests/test_introspect.py\")\n",
    "tp.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tcode = r'''\\\n",
    "import os\n",
    "import uuid\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "import pytest\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "from nf_app.introspect import list_columns\n",
    "\n",
    "SCHEMA = \"neuralforecast\"\n",
    "\n",
    "\n",
    "def _has_db_creds() -> bool:\n",
    "    # 既存テストと同じ運用：PGPASSWORD or MODEL_DB_PASSWORD があれば integration を回す\n",
    "    return bool(os.getenv(\"PGPASSWORD\") or os.getenv(\"MODEL_DB_PASSWORD\") or os.getenv(\"DATASET_DB_PASSWORD\"))\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_introspect_columns_survives_add_remove(monkeypatch):\n",
    "    \"\"\"\n",
    "    ID 020 Done: 列増減で壊れない（増減を“実際に”起こして検証）\n",
    "    \"\"\"\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set PGPASSWORD / MODEL_DB_PASSWORD / DATASET_DB_PASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "    eng = dbm.dataset_engine()\n",
    "\n",
    "    tbl = f\"pytest_introspect_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "    # create\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA};\"))\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE {SCHEMA}.{tbl}(\n",
    "                id BIGSERIAL PRIMARY KEY,\n",
    "                ds DATE NOT NULL,\n",
    "                y DOUBLE PRECISION NULL\n",
    "            );\n",
    "        \"\"\"))\n",
    "\n",
    "    cols1 = list_columns(eng, schema=SCHEMA, table=tbl)\n",
    "    names1 = [c.column_name for c in cols1]\n",
    "    assert \"id\" in names1 and \"ds\" in names1 and \"y\" in names1\n",
    "    assert len(cols1) >= 3\n",
    "\n",
    "    # add column (増分)\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(f\"ALTER TABLE {SCHEMA}.{tbl} ADD COLUMN hist_x INTEGER NULL;\"))\n",
    "\n",
    "    cols2 = list_columns(eng, schema=SCHEMA, table=tbl)\n",
    "    names2 = [c.column_name for c in cols2]\n",
    "    assert \"hist_x\" in names2\n",
    "    assert len(cols2) == len(cols1) + 1\n",
    "\n",
    "    # drop column (減分)\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(f\"ALTER TABLE {SCHEMA}.{tbl} DROP COLUMN hist_x;\"))\n",
    "\n",
    "    cols3 = list_columns(eng, schema=SCHEMA, table=tbl)\n",
    "    names3 = [c.column_name for c in cols3]\n",
    "    assert \"hist_x\" not in names3\n",
    "    assert len(cols3) == len(cols1)\n",
    "\n",
    "    # cleanup\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE {SCHEMA}.{tbl};\"))\n",
    "'''\n",
    "tp.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12773fd5",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2249bb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.25s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_introspect.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4793fb",
   "metadata": {},
   "source": [
    "## ID 021 join_rulesに基づくJOIN SQL生成（衝突回避）\n",
    "\n",
    "目的:\n",
    "- join_rules（JSON仕様）から dataset DB で実行可能な JOIN SQL を生成する\n",
    "- 列名衝突（同名カラム）を必ず回避し、生成SQLが毎回同一（安定）であること\n",
    "\n",
    "設計ポイント:\n",
    "- join_rulesテーブルに spec_json (JSONB) があればそれを最優先で使う\n",
    "- 無い場合でも、join_rulesの列を“推測”してspecに変換（拡張に強い）\n",
    "- dataset側のinformation_schema内省結果（ordinal_position順）でSELECT順序を固定（安定化）\n",
    "- 衝突回避ポリシー:\n",
    "  - JOIN側カラムは prefix があれば prefix+col\n",
    "  - prefixが無ければ join_alias__col\n",
    "  - ds/unique_id/y は “核” として固定（JOIN側同名は原則スキップ）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da334a",
   "metadata": {},
   "source": [
    "## codecell（実装）/mnt/e/env/ts/tslib/src/nf_app/query_builder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96a9bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/query_builder.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/query_builder.py\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error\n",
    "from nf_app.introspect import list_columns\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "_IDENT_RE = re.compile(r\"^[a-z_][a-z0-9_]*$\")\n",
    "\n",
    "\n",
    "def _q_ident(name: str) -> str:\n",
    "    \"\"\"\n",
    "    SQL identifier quote (Postgres). ほぼ小文字+_ならクオート不要、そうでなければ \"...\"。\n",
    "    \"\"\"\n",
    "    if _IDENT_RE.match(name):\n",
    "        return name\n",
    "    return '\"' + name.replace('\"', '\"\"') + '\"'\n",
    "\n",
    "\n",
    "def _q_fqn(schema: str, table: str) -> str:\n",
    "    return f\"{_q_ident(schema)}.{_q_ident(table)}\"\n",
    "\n",
    "\n",
    "def _stable_join_type(s: str) -> str:\n",
    "    s2 = (s or \"left\").strip().lower()\n",
    "    if s2 in (\"left\", \"left join\"):\n",
    "        return \"LEFT JOIN\"\n",
    "    if s2 in (\"inner\", \"inner join\"):\n",
    "        return \"INNER JOIN\"\n",
    "    if s2 in (\"right\", \"right join\"):\n",
    "        return \"RIGHT JOIN\"\n",
    "    if s2 in (\"full\", \"full join\", \"full outer\", \"full outer join\"):\n",
    "        return \"FULL OUTER JOIN\"\n",
    "    # デフォルトは安全側\n",
    "    return \"LEFT JOIN\"\n",
    "\n",
    "\n",
    "def _json_load_maybe(x: Any) -> Any:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (dict, list)):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if not x:\n",
    "            return None\n",
    "        try:\n",
    "            return json.loads(x)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Spec dataclasses\n",
    "# -------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class JoinOn:\n",
    "    left: str   # base column\n",
    "    right: str  # join column\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class JoinSpec:\n",
    "    schema: str\n",
    "    table: str\n",
    "    alias: str\n",
    "    join_type: str\n",
    "    on: Tuple[JoinOn, ...]\n",
    "    prefix: str = \"\"                   # collision回避 prefix (e.g., hist_, futr_, stat_)\n",
    "    include: Tuple[str, ...] = (\"*\",)   # [\"*\"] or explicit column list\n",
    "    exclude: Tuple[str, ...] = ()       # columns to exclude\n",
    "    rename: Dict[str, str] = None       # optional rename map (col -> out_name)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.rename is None:\n",
    "            object.__setattr__(self, \"rename\", {})\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BaseSpec:\n",
    "    schema: str\n",
    "    table: str\n",
    "    alias: str\n",
    "    ds_col: str = \"ds\"\n",
    "    uid_col: str = \"unique_id\"\n",
    "    y_col: str = \"y\"\n",
    "    include: Tuple[str, ...] = ()       # base extra columns; empty means only (ds, uid, y)\n",
    "    exclude: Tuple[str, ...] = ()       # base columns to exclude (rare)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class QuerySpec:\n",
    "    rule_name: str\n",
    "    base: BaseSpec\n",
    "    joins: Tuple[JoinSpec, ...]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Load join_rules from model DB (flexible)\n",
    "# -------------------------\n",
    "\n",
    "def load_join_rule_spec(model_engine: Engine, *, schema: str, rule_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    join_rules から spec を読む。\n",
    "    1) spec_json / rule_json / join_spec_json みたいな JSONB列があればそれを優先\n",
    "    2) 無ければ行を丸ごと読み、推測して最低限のspecに変換（拡張に強い）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with model_engine.connect() as conn:\n",
    "            # 列一覧を内省して JSON列の有無を見る\n",
    "            cols = conn.execute(\n",
    "                text(\"\"\"\n",
    "                SELECT column_name\n",
    "                FROM information_schema.columns\n",
    "                WHERE table_schema=:schema AND table_name='join_rules'\n",
    "                ORDER BY ordinal_position\n",
    "                \"\"\"),\n",
    "                {\"schema\": schema},\n",
    "            ).scalars().all()\n",
    "\n",
    "            if not cols:\n",
    "                raise db_error(\"DB_JOIN_RULES_NOT_FOUND\", \"join_rules table not found.\", context={\"schema\": schema})\n",
    "\n",
    "            json_candidates = [c for c in cols if c in (\"spec_json\", \"rule_json\", \"join_spec_json\", \"spec\", \"rule\")]\n",
    "            if json_candidates:\n",
    "                jcol = json_candidates[0]\n",
    "                row = conn.execute(\n",
    "                    text(f\"SELECT { _q_ident(jcol) } AS j FROM { _q_ident(schema) }.join_rules WHERE rule_name=:rn\"),\n",
    "                    {\"rn\": rule_name},\n",
    "                ).mappings().first()\n",
    "                if not row or row[\"j\"] is None:\n",
    "                    raise db_error(\"DB_JOIN_RULE_NOT_FOUND\", \"join_rule not found.\", context={\"rule_name\": rule_name})\n",
    "                spec = _json_load_maybe(row[\"j\"])\n",
    "                if not isinstance(spec, dict):\n",
    "                    raise db_error(\"DB_JOIN_RULE_BAD_JSON\", \"join_rule JSON invalid.\", context={\"rule_name\": rule_name})\n",
    "                return spec\n",
    "\n",
    "            # fallback: read whole row, guess\n",
    "            row = conn.execute(\n",
    "                text(f\"SELECT * FROM { _q_ident(schema) }.join_rules WHERE rule_name=:rn\"),\n",
    "                {\"rn\": rule_name},\n",
    "            ).mappings().first()\n",
    "            if not row:\n",
    "                raise db_error(\"DB_JOIN_RULE_NOT_FOUND\", \"join_rule not found.\", context={\"rule_name\": rule_name})\n",
    "\n",
    "            # 推測：base_table/base_schema と joins_json っぽいもの\n",
    "            guessed: Dict[str, Any] = {\"rule_name\": rule_name}\n",
    "            for k in (\"base_schema\", \"base_table\", \"ds_col\", \"uid_col\", \"y_col\"):\n",
    "                if k in row and row[k] is not None:\n",
    "                    guessed[k] = row[k]\n",
    "            for k in (\"joins_json\", \"joins\", \"join_json\"):\n",
    "                if k in row and row[k] is not None:\n",
    "                    j = _json_load_maybe(row[k])\n",
    "                    if isinstance(j, list):\n",
    "                        guessed[\"joins\"] = j\n",
    "                        break\n",
    "            return guessed\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_JOIN_RULE_LOAD_FAILED\",\n",
    "            \"Failed to load join_rules spec.\",\n",
    "            context={\"schema\": schema, \"rule_name\": rule_name},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "\n",
    "def normalize_query_spec(spec: Dict[str, Any]) -> QuerySpec:\n",
    "    \"\"\"\n",
    "    dict spec -> QuerySpec\n",
    "    必須:\n",
    "      - base_schema/base_table\n",
    "      - joins: list of {schema, table, alias, on:[{left,right}], join_type?, prefix?}\n",
    "    \"\"\"\n",
    "    rule_name = str(spec.get(\"rule_name\") or spec.get(\"name\") or \"unnamed_rule\")\n",
    "\n",
    "    base_schema = str(spec.get(\"base_schema\") or spec.get(\"schema\") or \"public\")\n",
    "    base_table = str(spec.get(\"base_table\") or spec.get(\"table\") or \"\")\n",
    "    if not base_table:\n",
    "        raise ValueError(\"join spec missing base_table/base_schema\")\n",
    "\n",
    "    base_alias = str(spec.get(\"base_alias\") or \"b\")\n",
    "    ds_col = str(spec.get(\"ds_col\") or \"ds\")\n",
    "    uid_col = str(spec.get(\"uid_col\") or \"unique_id\")\n",
    "    y_col = str(spec.get(\"y_col\") or \"y\")\n",
    "\n",
    "    base_include = tuple(spec.get(\"base_include\") or spec.get(\"base_columns\") or [])\n",
    "    base_exclude = tuple(spec.get(\"base_exclude\") or [])\n",
    "\n",
    "    joins_in = spec.get(\"joins\") or []\n",
    "    if not isinstance(joins_in, list):\n",
    "        raise ValueError(\"join spec 'joins' must be list\")\n",
    "\n",
    "    joins: List[JoinSpec] = []\n",
    "    for i, j in enumerate(joins_in):\n",
    "        if not isinstance(j, dict):\n",
    "            continue\n",
    "        js = str(j.get(\"schema\") or base_schema)\n",
    "        jt = str(j.get(\"table\") or \"\")\n",
    "        if not jt:\n",
    "            raise ValueError(f\"join[{i}] missing table\")\n",
    "        alias = str(j.get(\"alias\") or f\"j{i+1}\")\n",
    "        join_type = _stable_join_type(str(j.get(\"join_type\") or j.get(\"type\") or \"left\"))\n",
    "        prefix = str(j.get(\"prefix\") or \"\")\n",
    "\n",
    "        on_in = j.get(\"on\") or []\n",
    "        if not isinstance(on_in, list) or not on_in:\n",
    "            # デフォルトキー（ds, unique_id）\n",
    "            on = (JoinOn(left=uid_col, right=uid_col), JoinOn(left=ds_col, right=ds_col))\n",
    "        else:\n",
    "            ons: List[JoinOn] = []\n",
    "            for o in on_in:\n",
    "                if isinstance(o, dict) and \"left\" in o and \"right\" in o:\n",
    "                    ons.append(JoinOn(left=str(o[\"left\"]), right=str(o[\"right\"])))\n",
    "            if not ons:\n",
    "                ons = [JoinOn(left=uid_col, right=uid_col), JoinOn(left=ds_col, right=ds_col)]\n",
    "            on = tuple(ons)\n",
    "\n",
    "        include = tuple(j.get(\"include\") or (\"*\",))\n",
    "        exclude = tuple(j.get(\"exclude\") or ())\n",
    "        rename = j.get(\"rename\") or {}\n",
    "        if not isinstance(rename, dict):\n",
    "            rename = {}\n",
    "\n",
    "        joins.append(\n",
    "            JoinSpec(\n",
    "                schema=js,\n",
    "                table=jt,\n",
    "                alias=alias,\n",
    "                join_type=join_type,\n",
    "                on=on,\n",
    "                prefix=prefix,\n",
    "                include=include,\n",
    "                exclude=exclude,\n",
    "                rename=rename,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return QuerySpec(\n",
    "        rule_name=rule_name,\n",
    "        base=BaseSpec(\n",
    "            schema=base_schema,\n",
    "            table=base_table,\n",
    "            alias=base_alias,\n",
    "            ds_col=ds_col,\n",
    "            uid_col=uid_col,\n",
    "            y_col=y_col,\n",
    "            include=base_include,\n",
    "            exclude=base_exclude,\n",
    "        ),\n",
    "        joins=tuple(joins),\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SQL builder\n",
    "# -------------------------\n",
    "\n",
    "def _select_columns_for_table(\n",
    "    dataset_engine: Engine,\n",
    "    *,\n",
    "    schema: str,\n",
    "    table: str,\n",
    "    include: Sequence[str],\n",
    "    exclude: Sequence[str],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    include/excludeに従って列名を返す。include=[\"*\"]なら全列。\n",
    "    順序は ordinal_position で固定（安定生成の核）。\n",
    "    \"\"\"\n",
    "    cols = list_columns(dataset_engine, schema=schema, table=table)\n",
    "    all_names = [c.column_name for c in cols]\n",
    "\n",
    "    inc = list(include) if include else []\n",
    "    exc = set(exclude or [])\n",
    "\n",
    "    if not inc:\n",
    "        return [n for n in all_names if n not in exc]\n",
    "\n",
    "    if len(inc) == 1 and inc[0] == \"*\":\n",
    "        return [n for n in all_names if n not in exc]\n",
    "\n",
    "    # 明示列\n",
    "    return [n for n in inc if n in all_names and n not in exc]\n",
    "\n",
    "\n",
    "def build_join_sql(\n",
    "    dataset_engine: Engine,\n",
    "    *,\n",
    "    query_spec: QuerySpec,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    dataset DBで実行するJOIN SQLを生成する（安定・衝突回避）\n",
    "    \"\"\"\n",
    "    b = query_spec.base\n",
    "\n",
    "    # base columns\n",
    "    base_all = _select_columns_for_table(\n",
    "        dataset_engine, schema=b.schema, table=b.table,\n",
    "        include=([\"*\"] if not b.include else list(b.include)),\n",
    "        exclude=b.exclude,\n",
    "    )\n",
    "\n",
    "    # 核カラム（必須）\n",
    "    core = [b.uid_col, b.ds_col, b.y_col]\n",
    "    # base select順序：unique_id, ds, y, その後に残り（安定）\n",
    "    base_ordered: List[str] = []\n",
    "    for c in core:\n",
    "        if c in base_all and c not in base_ordered:\n",
    "            base_ordered.append(c)\n",
    "    for c in base_all:\n",
    "        if c not in base_ordered:\n",
    "            base_ordered.append(c)\n",
    "\n",
    "    # 生成SELECTと衝突管理\n",
    "    used_out_names: set[str] = set()\n",
    "    select_exprs: List[str] = []\n",
    "\n",
    "    def add_expr(src_alias: str, col: str, out_name: str) -> None:\n",
    "        used_out_names.add(out_name)\n",
    "        select_exprs.append(f\"{src_alias}.{_q_ident(col)} AS {_q_ident(out_name)}\")\n",
    "\n",
    "    # base: coreは固定名に正規化（unique_id/ds/y）\n",
    "    # ここは「ユーザー要件：ds/unique_id/yに設定」を満たす\n",
    "    if b.uid_col not in base_ordered or b.ds_col not in base_ordered or b.y_col not in base_ordered:\n",
    "        raise ValueError(\"base table must contain ds/unique_id/y columns (or mapped via ds_col/uid_col/y_col)\")\n",
    "\n",
    "    add_expr(b.alias, b.uid_col, \"unique_id\")\n",
    "    add_expr(b.alias, b.ds_col, \"ds\")\n",
    "    add_expr(b.alias, b.y_col, \"y\")\n",
    "\n",
    "    # base extra columns（核以外）\n",
    "    for c in base_ordered:\n",
    "        if c in (b.uid_col, b.ds_col, b.y_col):\n",
    "            continue\n",
    "        out = c\n",
    "        if out in used_out_names:\n",
    "            # base側の衝突は join 側で避けるのが基本だが、念のため安定サフィックス\n",
    "            out = f\"b__{out}\"\n",
    "        add_expr(b.alias, c, out)\n",
    "\n",
    "    # joins\n",
    "    join_clauses: List[str] = []\n",
    "    for j in query_spec.joins:\n",
    "        # join ON\n",
    "        ons = []\n",
    "        for o in j.on:\n",
    "            ons.append(f\"{b.alias}.{_q_ident(o.left)} = {j.alias}.{_q_ident(o.right)}\")\n",
    "        on_sql = \" AND \".join(ons)\n",
    "\n",
    "        join_clauses.append(\n",
    "            f\"{j.join_type} {_q_fqn(j.schema, j.table)} {j.alias} ON {on_sql}\"\n",
    "        )\n",
    "\n",
    "        # join select columns\n",
    "        join_cols = _select_columns_for_table(\n",
    "            dataset_engine, schema=j.schema, table=j.table,\n",
    "            include=j.include, exclude=j.exclude,\n",
    "        )\n",
    "\n",
    "        # デフォルトで “キー” と “核(y)” はJOIN側からは出さない（混乱・衝突の元）\n",
    "        skip = {o.right for o in j.on}\n",
    "        skip.update({b.ds_col, b.uid_col, b.y_col, \"ds\", \"unique_id\", \"y\"})\n",
    "        for col in join_cols:\n",
    "            if col in skip:\n",
    "                continue\n",
    "\n",
    "            # rename 優先\n",
    "            if col in (j.rename or {}):\n",
    "                out = str(j.rename[col])\n",
    "            else:\n",
    "                # prefixがあれば prefix+col、無ければ alias__col\n",
    "                out = f\"{j.prefix}{col}\" if j.prefix else f\"{j.alias}__{col}\"\n",
    "\n",
    "            # さらに衝突したら安定サフィックス\n",
    "            if out in used_out_names:\n",
    "                out2 = out\n",
    "                k = 2\n",
    "                while out2 in used_out_names:\n",
    "                    out2 = f\"{out}__{k}\"\n",
    "                    k += 1\n",
    "                out = out2\n",
    "\n",
    "            add_expr(j.alias, col, out)\n",
    "\n",
    "    sql = \"\\n\".join(\n",
    "        [\n",
    "            \"SELECT\",\n",
    "            \"  \" + \",\\n  \".join(select_exprs),\n",
    "            f\"FROM {_q_fqn(b.schema, b.table)} {b.alias}\",\n",
    "            *(join_clauses),\n",
    "        ]\n",
    "    )\n",
    "    return sql\n",
    "\n",
    "\n",
    "def build_join_sql_from_rule(\n",
    "    model_engine: Engine,\n",
    "    dataset_engine: Engine,\n",
    "    *,\n",
    "    model_schema: str,\n",
    "    rule_name: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    model DBの join_rules を読み、dataset DB向け JOIN SQL を返す\n",
    "    \"\"\"\n",
    "    raw = load_join_rule_spec(model_engine, schema=model_schema, rule_name=rule_name)\n",
    "    qs = normalize_query_spec(raw)\n",
    "    return build_join_sql(dataset_engine, query_spec=qs)\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff545ccd",
   "metadata": {},
   "source": [
    "## codecell（pytest）/mnt/e/env/ts/tslib/tests/test_query_builder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e860890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_query_builder.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tp = Path(\"/mnt/e/env/ts/tslib/tests/test_query_builder.py\")\n",
    "tp.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tcode = r'''\\\n",
    "import os\n",
    "import uuid\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "import pytest\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nf_app.config import NFConfig\n",
    "from nf_app.db import DbManager\n",
    "from nf_app.query_builder import build_join_sql, normalize_query_spec\n",
    "\n",
    "SCHEMA = \"neuralforecast\"\n",
    "\n",
    "\n",
    "def _has_db_creds() -> bool:\n",
    "    return bool(os.getenv(\"PGPASSWORD\") or os.getenv(\"MODEL_DB_PASSWORD\") or os.getenv(\"DATASET_DB_PASSWORD\"))\n",
    "\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_query_builder_stable_and_collision_safe(monkeypatch):\n",
    "    \"\"\"\n",
    "    ID 021 Done:\n",
    "    - JOIN SQLが毎回同一（安定生成）\n",
    "    - 列名衝突が回避される（prefix or alias__col）\n",
    "    \"\"\"\n",
    "    monkeypatch.setenv(\"DATASET_DB_URL\", \"postgresql+psycopg://postgres@127.0.0.1:5432/dataset\")\n",
    "    monkeypatch.setenv(\"MODEL_DB_URL\",   \"postgresql+psycopg://postgres@127.0.0.1:5432/model\")\n",
    "    monkeypatch.setenv(\"NF_STRICT_VALIDATION\", \"true\")\n",
    "    monkeypatch.setenv(\"NF_ARTIFACT_ROOT\", \"/tmp/tslib_artifacts_for_test\")\n",
    "    monkeypatch.setenv(\"NF_RUN_MODE\", \"DRY_RUN\")\n",
    "\n",
    "    if not _has_db_creds():\n",
    "        pytest.skip(\"DB credentials not provided. Set PGPASSWORD / MODEL_DB_PASSWORD / DATASET_DB_PASSWORD.\")\n",
    "\n",
    "    cfg = NFConfig.from_env(project_root=Path(\"/mnt/e/env/ts/tslib\"))\n",
    "    dbm = DbManager(cfg=cfg, echo_sql=False)\n",
    "    eng = dbm.dataset_engine()\n",
    "\n",
    "    base = f\"pytest_base_{uuid.uuid4().hex[:8]}\"\n",
    "    hist = f\"pytest_hist_{uuid.uuid4().hex[:8]}\"\n",
    "    stat = f\"pytest_stat_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA};\"))\n",
    "\n",
    "        # base（核: unique_id/ds/y + 追加: shared）\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE {SCHEMA}.{base}(\n",
    "                unique_id TEXT NOT NULL,\n",
    "                ds DATE NOT NULL,\n",
    "                y DOUBLE PRECISION NULL,\n",
    "                shared INTEGER NULL\n",
    "            );\n",
    "        \"\"\"))\n",
    "\n",
    "        # hist（衝突: shared を持つ）\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE {SCHEMA}.{hist}(\n",
    "                unique_id TEXT NOT NULL,\n",
    "                ds DATE NOT NULL,\n",
    "                shared INTEGER NULL,\n",
    "                h1 INTEGER NULL\n",
    "            );\n",
    "        \"\"\"))\n",
    "\n",
    "        # stat（衝突: shared を持つ）\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE {SCHEMA}.{stat}(\n",
    "                unique_id TEXT NOT NULL,\n",
    "                ds DATE NOT NULL,\n",
    "                shared INTEGER NULL,\n",
    "                s1 INTEGER NULL\n",
    "            );\n",
    "        \"\"\"))\n",
    "\n",
    "        # data\n",
    "        conn.execute(text(f\"INSERT INTO {SCHEMA}.{base}(unique_id, ds, y, shared) VALUES ('u1', '2020-01-01', 10.0, 7);\"))\n",
    "        conn.execute(text(f\"INSERT INTO {SCHEMA}.{hist}(unique_id, ds, shared, h1) VALUES ('u1', '2020-01-01', 70, 1);\"))\n",
    "        conn.execute(text(f\"INSERT INTO {SCHEMA}.{stat}(unique_id, ds, shared, s1) VALUES ('u1', '2020-01-01', 700, 2);\"))\n",
    "\n",
    "    spec = {\n",
    "        \"rule_name\": \"pytest_join_rule\",\n",
    "        \"base_schema\": SCHEMA,\n",
    "        \"base_table\": base,\n",
    "        \"base_alias\": \"b\",\n",
    "        # base include empty -> core+extrasを自動（実装では \"*\" 相当）\n",
    "        \"joins\": [\n",
    "            {\n",
    "                \"schema\": SCHEMA,\n",
    "                \"table\": hist,\n",
    "                \"alias\": \"h\",\n",
    "                \"join_type\": \"left\",\n",
    "                \"on\": [{\"left\": \"unique_id\", \"right\": \"unique_id\"}, {\"left\": \"ds\", \"right\": \"ds\"}],\n",
    "                \"prefix\": \"hist_\",\n",
    "                \"include\": [\"*\"],\n",
    "                \"exclude\": [],\n",
    "            },\n",
    "            {\n",
    "                \"schema\": SCHEMA,\n",
    "                \"table\": stat,\n",
    "                \"alias\": \"s\",\n",
    "                \"join_type\": \"left\",\n",
    "                \"on\": [{\"left\": \"unique_id\", \"right\": \"unique_id\"}, {\"left\": \"ds\", \"right\": \"ds\"}],\n",
    "                \"prefix\": \"stat_\",\n",
    "                \"include\": [\"*\"],\n",
    "                \"exclude\": [],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    qs = normalize_query_spec(spec)\n",
    "\n",
    "    sql1 = build_join_sql(eng, query_spec=qs)\n",
    "    sql2 = build_join_sql(eng, query_spec=qs)\n",
    "    assert sql1 == sql2  # 安定生成\n",
    "\n",
    "    # 核は固定で含まれる\n",
    "    assert \"b.unique_id AS unique_id\" in sql1\n",
    "    assert \"b.ds AS ds\" in sql1\n",
    "    assert \"b.y AS y\" in sql1\n",
    "\n",
    "    # 衝突(shared)は prefixで回避される（JOIN側は hist_shared / stat_shared）\n",
    "    assert \"h.shared AS hist_shared\" in sql1\n",
    "    assert \"s.shared AS stat_shared\" in sql1\n",
    "\n",
    "    # 実行して列が期待どおり取れる（Smoke）\n",
    "    with eng.connect() as conn:\n",
    "        row = conn.execute(text(sql1)).mappings().first()\n",
    "        assert row is not None\n",
    "        assert row[\"unique_id\"] == \"u1\"\n",
    "        assert row[\"ds\"].isoformat() == \"2020-01-01\"\n",
    "        assert float(row[\"y\"]) == 10.0\n",
    "        assert int(row[\"hist_shared\"]) == 70\n",
    "        assert int(row[\"stat_shared\"]) == 700\n",
    "        assert int(row[\"hist_h1\"]) == 1\n",
    "        assert int(row[\"stat_s1\"]) == 2\n",
    "\n",
    "    # cleanup\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE {SCHEMA}.{stat};\"))\n",
    "        conn.execute(text(f\"DROP TABLE {SCHEMA}.{hist};\"))\n",
    "        conn.execute(text(f\"DROP TABLE {SCHEMA}.{base};\"))\n",
    "'''\n",
    "tp.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c072891",
   "metadata": {},
   "source": [
    "## codecell（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "980be9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.22s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_query_builder.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc7b18",
   "metadata": {},
   "source": [
    "## ID 022 dataset結合結果の取得（pandas read_sql）\n",
    "\n",
    "目的:\n",
    "- 生成済みJOIN SQL（query_builderの出力）を pandas.read_sql で DataFrame 化して返す\n",
    "- 列の増減に強い（列を固定しない）\n",
    "- 最低限 required_cols = [\"unique_id\",\"ds\",\"y\"] を満たすことを検証\n",
    "\n",
    "設計:\n",
    "- dataset_loader は「SQL→DataFrame」だけ担当（JOIN生成は query_builder 側）\n",
    "- ds は datetime に正規化（Date/str混在を吸収）\n",
    "- 必須列が無ければ DataError（NFError）で落とす\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86a32e",
   "metadata": {},
   "source": [
    "## codecell（実装）/mnt/e/env/ts/tslib/src/nf_app/dataset_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/dataset_loader.py\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, Iterable, Mapping, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error, data_error\n",
    "\n",
    "\n",
    "def load_dataframe(\n",
    "    engine: Engine,\n",
    "    *,\n",
    "    sql: str,\n",
    "    params: Optional[Mapping[str, Any]] = None,\n",
    "    required_cols: Sequence[str] = (\"unique_id\", \"ds\", \"y\"),\n",
    "    parse_dates: Sequence[str] = (\"ds\",),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    SQLをpandas.read_sqlでDataFrame化して返す（列増減に強い）\n",
    "\n",
    "    - required_cols が存在しない場合は data_error を投げる\n",
    "    - ds は datetime に正規化（date/str混在を吸収）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            df = pd.read_sql_query(\n",
    "                text(sql),\n",
    "                con=conn,\n",
    "                params=dict(params or {}),\n",
    "                parse_dates=list(parse_dates) if parse_dates else None,\n",
    "            )\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_READ_SQL_FAILED\",\n",
    "            \"Failed to read SQL into DataFrame.\",\n",
    "            context={\"sql_head\": sql[:200]},\n",
    "            cause=e,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise data_error(\n",
    "            \"DATA_READ_SQL_FAILED\",\n",
    "            \"Failed to read SQL into DataFrame.\",\n",
    "            context={\"sql_head\": sql[:200]},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "    # 必須列チェック（列増減は許容、必須だけ要求）\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise data_error(\n",
    "            \"DATA_MISSING_REQUIRED_COLUMNS\",\n",
    "            f\"Missing required columns: {missing}\",\n",
    "            context={\"missing\": missing, \"columns\": list(df.columns)},\n",
    "        )\n",
    "\n",
    "    # ds 正規化（parse_datesで拾えない型もあるため最後に保険）\n",
    "    if \"ds\" in df.columns:\n",
    "        df[\"ds\"] = pd.to_datetime(df[\"ds\"], errors=\"coerce\")\n",
    "        bad = int(df[\"ds\"].isna().sum())\n",
    "        if bad > 0:\n",
    "            raise data_error(\n",
    "                \"DATA_INVALID_DS\",\n",
    "                \"Some ds values could not be parsed as datetime.\",\n",
    "                context={\"bad_rows\": bad},\n",
    "            )\n",
    "\n",
    "    return df\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368b79d",
   "metadata": {},
   "source": [
    "## codecell（pytest）/mnt/e/env/ts/tslib/tests/test_dataset_loader_unit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4345cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_dataset_loader_unit.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tp = Path(\"/mnt/e/env/ts/tslib/tests/test_dataset_loader_unit.py\")\n",
    "tp.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tcode = r'''\\\n",
    "import pandas as pd\n",
    "import pytest\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from nf_app.dataset_loader import load_dataframe\n",
    "from nf_app.errors import NFError\n",
    "\n",
    "\n",
    "def test_dataset_loader_returns_dataframe_with_required_cols_and_extra_cols():\n",
    "    # in-memory SQLite で「列増減に壊れない」を検証（DB資格情報不要）\n",
    "    eng = create_engine(\"sqlite+pysqlite:///:memory:\")\n",
    "\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE joined(\n",
    "                unique_id TEXT NOT NULL,\n",
    "                ds TEXT NOT NULL,\n",
    "                y REAL NULL,\n",
    "                hist_x INTEGER NULL\n",
    "            );\n",
    "        \"\"\"))\n",
    "        conn.execute(\n",
    "            text(\"INSERT INTO joined(unique_id, ds, y, hist_x) VALUES (:u, :ds, :y, :hx)\"),\n",
    "            {\"u\": \"A\", \"ds\": \"2020-01-02\", \"y\": 123.0, \"hx\": 7},\n",
    "        )\n",
    "\n",
    "    df = load_dataframe(\n",
    "        eng,\n",
    "        sql=\"SELECT unique_id, ds, y, hist_x FROM joined\",\n",
    "        required_cols=(\"unique_id\", \"ds\", \"y\"),\n",
    "    )\n",
    "\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    assert set([\"unique_id\", \"ds\", \"y\"]).issubset(df.columns)\n",
    "    # 追加列も落とさない（列増減に強い）\n",
    "    assert \"hist_x\" in df.columns\n",
    "    # ds が datetime に正規化される\n",
    "    assert pd.api.types.is_datetime64_any_dtype(df[\"ds\"])\n",
    "\n",
    "\n",
    "def test_dataset_loader_raises_when_required_missing():\n",
    "    eng = create_engine(\"sqlite+pysqlite:///:memory:\")\n",
    "\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE joined(\n",
    "                unique_id TEXT NOT NULL,\n",
    "                ds TEXT NOT NULL\n",
    "            );\n",
    "        \"\"\"))\n",
    "        conn.execute(\n",
    "            text(\"INSERT INTO joined(unique_id, ds) VALUES (:u, :ds)\"),\n",
    "            {\"u\": \"A\", \"ds\": \"2020-01-02\"},\n",
    "        )\n",
    "\n",
    "    with pytest.raises(NFError):\n",
    "        # y が無いので落ちる\n",
    "        load_dataframe(eng, sql=\"SELECT unique_id, ds FROM joined\", required_cols=(\"unique_id\", \"ds\", \"y\"))\n",
    "'''\n",
    "tp.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac711c1",
   "metadata": {},
   "source": [
    "## codecell（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1ef1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m______________ ERROR collecting tests/test_dataset_loader_unit.py ______________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/mnt/e/env/ts/tslib/tests/test_dataset_loader_unit.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_dataset_loader_unit.py\u001b[0m:6: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mdataset_loader\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m load_dataframe\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   ModuleNotFoundError: No module named 'nf_app.dataset_loader'\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/test_dataset_loader_unit.py\n",
      "!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m\u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.17s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_dataset_loader_unit.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac602c",
   "metadata": {},
   "source": [
    "## ✅ codecell 1：まず存在確認（ここで9割決まる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db0c63ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf_app path: ['/mnt/e/env/ts/tslib/src/nf_app']\n",
      "dataset_loader.py exists?: False -> /mnt/e/env/ts/tslib/src/nf_app/dataset_loader.py\n",
      "modules under nf_app: ['config', 'db', 'errors', 'introspect', 'logging', 'migrations', 'query_builder', 'run_plans']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pkgutil\n",
    "import nf_app\n",
    "\n",
    "print(\"nf_app path:\", list(getattr(nf_app, \"__path__\", [])))\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/dataset_loader.py\")\n",
    "print(\"dataset_loader.py exists?:\", p.exists(), \"->\", p)\n",
    "\n",
    "print(\"modules under nf_app:\", [m.name for m in pkgutil.iter_modules(nf_app.__path__)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105f9c8",
   "metadata": {},
   "source": [
    "## ✅ codecell 2：dataset_loader.py を フルパスで確実に作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88fdbf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/dataset_loader.py size: 2066\n",
      "\\\n",
      "from __future__ import annotations\n",
      "\n",
      "from typing import Any, Mapping, Optional, Sequence\n",
      "\n",
      "import pandas as pd\n",
      "from sqlalchemy import text\n",
      "from sqlalchemy.engine import Engine\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "\n",
      "from nf_app.errors import db_error, data_error\n",
      "\n",
      "\n",
      "def load_dataframe(\n",
      "    engine: Engine,\n",
      "    *,\n",
      "    sql: str,\n",
      "    params: Optional[Mapping[str, Any]] = None,\n",
      "    required_cols: Seq\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/dataset_loader.py\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Mapping, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error, data_error\n",
    "\n",
    "\n",
    "def load_dataframe(\n",
    "    engine: Engine,\n",
    "    *,\n",
    "    sql: str,\n",
    "    params: Optional[Mapping[str, Any]] = None,\n",
    "    required_cols: Sequence[str] = (\"unique_id\", \"ds\", \"y\"),\n",
    "    parse_dates: Sequence[str] = (\"ds\",),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    SQLをpandas.read_sqlでDataFrame化して返す（列増減に強い）\n",
    "    - required_cols が存在しない場合は data_error を投げる\n",
    "    - ds は datetime に正規化（date/str混在を吸収）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            df = pd.read_sql_query(\n",
    "                text(sql),\n",
    "                con=conn,\n",
    "                params=dict(params or {}),\n",
    "                parse_dates=list(parse_dates) if parse_dates else None,\n",
    "            )\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_READ_SQL_FAILED\",\n",
    "            \"Failed to read SQL into DataFrame.\",\n",
    "            context={\"sql_head\": sql[:200]},\n",
    "            cause=e,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise data_error(\n",
    "            \"DATA_READ_SQL_FAILED\",\n",
    "            \"Failed to read SQL into DataFrame.\",\n",
    "            context={\"sql_head\": sql[:200]},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise data_error(\n",
    "            \"DATA_MISSING_REQUIRED_COLUMNS\",\n",
    "            f\"Missing required columns: {missing}\",\n",
    "            context={\"missing\": missing, \"columns\": list(df.columns)},\n",
    "        )\n",
    "\n",
    "    if \"ds\" in df.columns:\n",
    "        df[\"ds\"] = pd.to_datetime(df[\"ds\"], errors=\"coerce\")\n",
    "        bad = int(df[\"ds\"].isna().sum())\n",
    "        if bad > 0:\n",
    "            raise data_error(\n",
    "                \"DATA_INVALID_DS\",\n",
    "                \"Some ds values could not be parsed as datetime.\",\n",
    "                context={\"bad_rows\": bad},\n",
    "            )\n",
    "\n",
    "    return df\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p, \"size:\", p.stat().st_size)\n",
    "print(p.read_text(encoding=\"utf-8\")[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2f15a",
   "metadata": {},
   "source": [
    "## ✅ codecell 3：pytest 再実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d676cb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                       [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.10s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_dataset_loader_unit.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41889fb6",
   "metadata": {},
   "source": [
    "## ID 023 prefix_rules取得（DB→メモリ辞書）\n",
    "\n",
    "目的:\n",
    "- model DB（schema: neuralforecast）の feature_prefix_rules を取得し、メモリ上のルール集合へ変換する\n",
    "- ルール変更が即反映される（=キャッシュせず毎回DBを読む）\n",
    "\n",
    "出力（例）:\n",
    "- rules: List[FeaturePrefixRule]（priority昇順・prefix長降順で安定）\n",
    "- dict: { \"hist_\": \"hist\", \"futr_\": \"futr\", \"stat_\": \"stat\" } など（下流の自動分類で使用）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a82e1",
   "metadata": {},
   "source": [
    "## ✅ codecell（実装）/mnt/e/env/ts/tslib/src/nf_app/feature_rules.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9ba324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/feature_rules.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/feature_rules.py\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "from sqlalchemy import inspect, text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from nf_app.errors import db_error\n",
    "\n",
    "DEFAULT_SCHEMA = \"neuralforecast\"\n",
    "DEFAULT_TABLE = \"feature_prefix_rules\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FeaturePrefixRule:\n",
    "    \"\"\"\n",
    "    prefix: 例 'hist_' / 'futr_' / 'stat_'\n",
    "    exog_kind: NeuralForecastの分類用（例 'hist' / 'futr' / 'stat'）\n",
    "    enabled: 有効フラグ\n",
    "    priority: 小さいほど優先\n",
    "    \"\"\"\n",
    "    prefix: str\n",
    "    exog_kind: str\n",
    "    enabled: bool = True\n",
    "    priority: int = 100\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"prefix\": self.prefix,\n",
    "            \"exog_kind\": self.exog_kind,\n",
    "            \"enabled\": self.enabled,\n",
    "            \"priority\": self.priority,\n",
    "        }\n",
    "\n",
    "\n",
    "def _full_table(schema: Optional[str], table: str) -> str:\n",
    "    return f\"{schema}.{table}\" if schema else table\n",
    "\n",
    "\n",
    "def _detect_columns(engine: Engine, *, schema: Optional[str], table: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    SQLAlchemy Inspector で列名を取得（DB方言に依存しにくい）\n",
    "    \"\"\"\n",
    "    insp = inspect(engine)\n",
    "    cols = insp.get_columns(table, schema=schema)\n",
    "    return [c[\"name\"] for c in cols]\n",
    "\n",
    "\n",
    "def load_prefix_rules(\n",
    "    engine: Engine,\n",
    "    *,\n",
    "    schema: Optional[str] = DEFAULT_SCHEMA,\n",
    "    table: str = DEFAULT_TABLE,\n",
    "    enabled_only: bool = True,\n",
    ") -> List[FeaturePrefixRule]:\n",
    "    \"\"\"\n",
    "    feature_prefix_rules をDBから取得して返す（※キャッシュしない＝即反映）\n",
    "    - ORDER: priority ASC, prefix length DESC, prefix ASC で安定\n",
    "    - 列名が多少変わっても検出して読む（拡張耐性）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cols = _detect_columns(engine, schema=schema, table=table)\n",
    "    except Exception as e:\n",
    "        raise db_error(\n",
    "            \"DB_PREFIX_RULES_INTROSPECT_FAILED\",\n",
    "            \"Failed to introspect feature_prefix_rules columns.\",\n",
    "            context={\"schema\": schema, \"table\": table},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "    # できるだけ“揺れ”に耐える（移行や拡張を想定）\n",
    "    prefix_col_candidates = [\"prefix\", \"feature_prefix\", \"col_prefix\"]\n",
    "    kind_col_candidates = [\"exog_kind\", \"exog_type\", \"kind\", \"feature_type\", \"group_name\"]\n",
    "    enabled_col_candidates = [\"enabled\", \"is_enabled\"]\n",
    "    priority_col_candidates = [\"priority\", \"order\", \"rank\"]\n",
    "\n",
    "    def pick(cands: Sequence[str], *, required: bool) -> Optional[str]:\n",
    "        for c in cands:\n",
    "            if c in cols:\n",
    "                return c\n",
    "        if required:\n",
    "            raise db_error(\n",
    "                \"DB_PREFIX_RULES_SCHEMA_INVALID\",\n",
    "                \"feature_prefix_rules schema is missing required columns.\",\n",
    "                context={\"missing_candidates\": list(cands), \"found_columns\": cols},\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    prefix_col = pick(prefix_col_candidates, required=True)\n",
    "    kind_col = pick(kind_col_candidates, required=True)\n",
    "    enabled_col = pick(enabled_col_candidates, required=False)\n",
    "    priority_col = pick(priority_col_candidates, required=False)\n",
    "\n",
    "    full = _full_table(schema, table)\n",
    "\n",
    "    where = \"\"\n",
    "    if enabled_only and enabled_col:\n",
    "        where = f\"WHERE {enabled_col} = TRUE\"\n",
    "\n",
    "    # SQLite等で enabled_col がない場合も許容（enabled_only=trueでも“全取得”になる）\n",
    "    order_by = []\n",
    "    if priority_col:\n",
    "        order_by.append(f\"{priority_col} ASC\")\n",
    "    order_by.append(f\"LENGTH({prefix_col}) DESC\")\n",
    "    order_by.append(f\"{prefix_col} ASC\")\n",
    "    order_sql = \"ORDER BY \" + \", \".join(order_by)\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT\n",
    "            {prefix_col} AS prefix,\n",
    "            {kind_col}   AS exog_kind\n",
    "            {\",\" + enabled_col + \" AS enabled\" if enabled_col else \"\"}\n",
    "            {\",\" + priority_col + \" AS priority\" if priority_col else \"\"}\n",
    "        FROM {full}\n",
    "        {where}\n",
    "        {order_sql};\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            rows = conn.execute(text(sql)).mappings().all()\n",
    "    except SQLAlchemyError as e:\n",
    "        raise db_error(\n",
    "            \"DB_PREFIX_RULES_SELECT_FAILED\",\n",
    "            \"Failed to select feature_prefix_rules.\",\n",
    "            context={\"schema\": schema, \"table\": table},\n",
    "            cause=e,\n",
    "        )\n",
    "\n",
    "    out: List[FeaturePrefixRule] = []\n",
    "    for r in rows:\n",
    "        out.append(\n",
    "            FeaturePrefixRule(\n",
    "                prefix=str(r[\"prefix\"]),\n",
    "                exog_kind=str(r[\"exog_kind\"]),\n",
    "                enabled=bool(r[\"enabled\"]) if \"enabled\" in r and r[\"enabled\"] is not None else True,\n",
    "                priority=int(r[\"priority\"]) if \"priority\" in r and r[\"priority\"] is not None else 100,\n",
    "            )\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "def prefix_rules_dict(\n",
    "    engine: Engine,\n",
    "    *,\n",
    "    schema: Optional[str] = DEFAULT_SCHEMA,\n",
    "    table: str = DEFAULT_TABLE,\n",
    "    enabled_only: bool = True,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    使いやすい辞書形（prefix -> exog_kind）\n",
    "    ※同一prefixが複数あり得る場合は、load_prefix_rulesの先頭（優先）を採用\n",
    "    \"\"\"\n",
    "    rules = load_prefix_rules(engine, schema=schema, table=table, enabled_only=enabled_only)\n",
    "    d: Dict[str, str] = {}\n",
    "    for r in rules:\n",
    "        if r.prefix not in d:\n",
    "            d[r.prefix] = r.exog_kind\n",
    "    return d\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5578d6d",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest）/mnt/e/env/ts/tslib/tests/test_feature_rules.py\n",
    "\n",
    "DB資格情報不要で回るように、SQLite(in-memory)で「UPDATE→再取得→即反映」を検証します（=キャッシュしてたら落ちる）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60927e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_feature_rules.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tp = Path(\"/mnt/e/env/ts/tslib/tests/test_feature_rules.py\")\n",
    "tp.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tcode = r'''\\\n",
    "import pytest\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from nf_app.feature_rules import load_prefix_rules, prefix_rules_dict\n",
    "\n",
    "\n",
    "def test_prefix_rules_reflects_updates_immediately_no_cache():\n",
    "    eng = create_engine(\"sqlite+pysqlite:///:memory:\")\n",
    "\n",
    "    # schemaはSQLiteだと無いので schema=None で呼ぶ（本番は neuralforecast）\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE feature_prefix_rules(\n",
    "                prefix TEXT PRIMARY KEY,\n",
    "                exog_kind TEXT NOT NULL,\n",
    "                enabled BOOLEAN NOT NULL DEFAULT 1,\n",
    "                priority INTEGER NOT NULL DEFAULT 100\n",
    "            );\n",
    "        \"\"\"))\n",
    "        conn.execute(\n",
    "            text(\"INSERT INTO feature_prefix_rules(prefix, exog_kind, enabled, priority) VALUES (:p, :k, :e, :pri)\"),\n",
    "            {\"p\": \"hist_\", \"k\": \"hist\", \"e\": True, \"pri\": 10},\n",
    "        )\n",
    "\n",
    "    d1 = prefix_rules_dict(eng, schema=None, table=\"feature_prefix_rules\")\n",
    "    assert d1[\"hist_\"] == \"hist\"\n",
    "\n",
    "    # ルール変更（UPDATE）\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(\n",
    "            text(\"UPDATE feature_prefix_rules SET exog_kind=:k WHERE prefix=:p\"),\n",
    "            {\"p\": \"hist_\", \"k\": \"futr\"},\n",
    "        )\n",
    "\n",
    "    # 再取得で即反映（キャッシュしてたらここが失敗する）\n",
    "    d2 = prefix_rules_dict(eng, schema=None, table=\"feature_prefix_rules\")\n",
    "    assert d2[\"hist_\"] == \"futr\"\n",
    "\n",
    "\n",
    "def test_prefix_rules_order_is_stable_priority_then_prefixlen():\n",
    "    eng = create_engine(\"sqlite+pysqlite:///:memory:\")\n",
    "\n",
    "    with eng.begin() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE feature_prefix_rules(\n",
    "                prefix TEXT PRIMARY KEY,\n",
    "                exog_kind TEXT NOT NULL,\n",
    "                enabled BOOLEAN NOT NULL DEFAULT 1,\n",
    "                priority INTEGER NOT NULL DEFAULT 100\n",
    "            );\n",
    "        \"\"\"))\n",
    "        conn.execute(text(\"INSERT INTO feature_prefix_rules(prefix, exog_kind, enabled, priority) VALUES ('h_', 'hist', 1, 10)\"))\n",
    "        conn.execute(text(\"INSERT INTO feature_prefix_rules(prefix, exog_kind, enabled, priority) VALUES ('hist_', 'hist', 1, 10)\"))\n",
    "        conn.execute(text(\"INSERT INTO feature_prefix_rules(prefix, exog_kind, enabled, priority) VALUES ('stat_', 'stat', 1, 20)\"))\n",
    "\n",
    "    rules = load_prefix_rules(eng, schema=None, table=\"feature_prefix_rules\")\n",
    "    # 同priorityなら prefix長が長い方が先（衝突回避の基本）\n",
    "    assert rules[0].prefix == \"hist_\"\n",
    "    assert rules[1].prefix == \"h_\"\n",
    "'''\n",
    "tp.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106e42d",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "386736c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                       [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.09s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_feature_rules.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d10381",
   "metadata": {},
   "source": [
    "## ID 024 列を役割に分類（hist/futr/stat/meta/ignore）\n",
    "\n",
    "入力:\n",
    "- DataFrame列一覧（例: ds, unique_id, y, hist_*, futr_*, stat_* ...）\n",
    "- prefix_rules（DBから取得した prefix -> exog_kind）\n",
    "\n",
    "出力（roles）:\n",
    "- meta_cols: ['unique_id','ds','y']（必須）\n",
    "- hist_exog_cols / futr_exog_cols / stat_exog_cols（外生変数）\n",
    "- ignore_cols: 学習に使わない（id, exec_ts など）\n",
    "- unknown_cols: どれにも当てはまらない列（運用で気づけるように残す）\n",
    "\n",
    "設計:\n",
    "- prefixは「長い一致を優先」（衝突回避）\n",
    "- ルールや列増減に強い（列が増えても分類するだけで落ちない）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d02568",
   "metadata": {},
   "source": [
    "## ✅ codecell（実装）/mnt/e/env/ts/tslib/src/nf_app/feature_router.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0dc5781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/feature_router.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/src/nf_app/feature_router.py\")\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Set, Tuple\n",
    "\n",
    "META_DEFAULT = (\"unique_id\", \"ds\", \"y\")\n",
    "\n",
    "# 運用上よくある「学習に不要」列。増えても害はない（ignoreに落ちるだけ）。\n",
    "IGNORE_DEFAULT_PREFIXES = (\n",
    "    \"exec_\", \"updated_\", \"created_\", \"proc_\", \"run_\", \"job_\",\n",
    ")\n",
    "IGNORE_DEFAULT_COLS = (\n",
    "    \"id\",\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RoutedFeatures:\n",
    "    meta_cols: Tuple[str, ...]\n",
    "    hist_exog_cols: Tuple[str, ...]\n",
    "    futr_exog_cols: Tuple[str, ...]\n",
    "    stat_exog_cols: Tuple[str, ...]\n",
    "    ignore_cols: Tuple[str, ...]\n",
    "    unknown_cols: Tuple[str, ...]\n",
    "\n",
    "\n",
    "def _sorted_prefixes(prefix_to_kind: Dict[str, str]) -> List[str]:\n",
    "    # 長いprefixを先に（衝突回避）\n",
    "    return sorted(prefix_to_kind.keys(), key=lambda s: (-len(s), s))\n",
    "\n",
    "\n",
    "def route_columns(\n",
    "    columns: Sequence[str],\n",
    "    *,\n",
    "    prefix_to_kind: Dict[str, str],\n",
    "    meta_cols: Sequence[str] = META_DEFAULT,\n",
    "    ignore_prefixes: Sequence[str] = IGNORE_DEFAULT_PREFIXES,\n",
    "    ignore_cols: Sequence[str] = IGNORE_DEFAULT_COLS,\n",
    ") -> RoutedFeatures:\n",
    "    \"\"\"\n",
    "    列名を役割に分類する（列増減で壊れない）\n",
    "    - prefix_to_kind: {\"hist_\":\"hist\",\"futr_\":\"futr\",\"stat_\":\"stat\", ...}\n",
    "    - meta_cols: 必須（unique_id/ds/y）\n",
    "    - ignore: 運用列（id/exec_ts等）を学習対象から除外\n",
    "    \"\"\"\n",
    "    cols = list(columns)\n",
    "    meta_set: Set[str] = set(meta_cols)\n",
    "\n",
    "    # 必須メタ列チェック（ここは落とす：学習以前の前提が崩れるため）\n",
    "    missing_meta = [c for c in meta_cols if c not in cols]\n",
    "    if missing_meta:\n",
    "        raise ValueError(f\"Missing required meta columns: {missing_meta}\")\n",
    "\n",
    "    prefixes = _sorted_prefixes(prefix_to_kind)\n",
    "\n",
    "    hist: List[str] = []\n",
    "    futr: List[str] = []\n",
    "    stat: List[str] = []\n",
    "    ignore: List[str] = []\n",
    "    unknown: List[str] = []\n",
    "\n",
    "    for c in cols:\n",
    "        if c in meta_set:\n",
    "            continue\n",
    "\n",
    "        # ignore（完全一致）\n",
    "        if c in ignore_cols:\n",
    "            ignore.append(c)\n",
    "            continue\n",
    "\n",
    "        # ignore（prefix一致）\n",
    "        if any(c.startswith(pfx) for pfx in ignore_prefixes):\n",
    "            ignore.append(c)\n",
    "            continue\n",
    "\n",
    "        # prefixルール（長い一致を優先）\n",
    "        matched = False\n",
    "        for pfx in prefixes:\n",
    "            if c.startswith(pfx):\n",
    "                kind = prefix_to_kind[pfx]\n",
    "                if kind == \"hist\":\n",
    "                    hist.append(c)\n",
    "                elif kind == \"futr\":\n",
    "                    futr.append(c)\n",
    "                elif kind == \"stat\":\n",
    "                    stat.append(c)\n",
    "                else:\n",
    "                    # 将来のkind拡張：未知kindはunknown扱いにして監視可能に\n",
    "                    unknown.append(c)\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if matched:\n",
    "            continue\n",
    "\n",
    "        # どれにも当てはまらない列\n",
    "        unknown.append(c)\n",
    "\n",
    "    # 安定化（順序は入力列順を尊重しつつ、テストしやすいようtuple化）\n",
    "    return RoutedFeatures(\n",
    "        meta_cols=tuple(meta_cols),\n",
    "        hist_exog_cols=tuple(hist),\n",
    "        futr_exog_cols=tuple(futr),\n",
    "        stat_exog_cols=tuple(stat),\n",
    "        ignore_cols=tuple(ignore),\n",
    "        unknown_cols=tuple(unknown),\n",
    "    )\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b3814",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest）/mnt/e/env/ts/tslib/tests/test_feature_router.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "924bc347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_feature_router.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tp = Path(\"/mnt/e/env/ts/tslib/tests/test_feature_router.py\")\n",
    "tp.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tcode = r'''\\\n",
    "import pytest\n",
    "from nf_app.feature_router import route_columns\n",
    "\n",
    "\n",
    "def test_feature_router_classifies_prefixes_correctly():\n",
    "    cols = [\n",
    "        \"unique_id\", \"ds\", \"y\",\n",
    "        \"hist_pn1\", \"hist_pm1\",\n",
    "        \"futr_year\", \"futr_month\",\n",
    "        \"stat_mean_7\",\n",
    "        \"id\", \"exec_ts\",\n",
    "        \"other_col\",\n",
    "    ]\n",
    "    prefix_to_kind = {\"hist_\": \"hist\", \"futr_\": \"futr\", \"stat_\": \"stat\"}\n",
    "\n",
    "    routed = route_columns(cols, prefix_to_kind=prefix_to_kind)\n",
    "\n",
    "    assert set(routed.hist_exog_cols) == {\"hist_pn1\", \"hist_pm1\"}\n",
    "    assert set(routed.futr_exog_cols) == {\"futr_year\", \"futr_month\"}\n",
    "    assert set(routed.stat_exog_cols) == {\"stat_mean_7\"}\n",
    "\n",
    "    # ignore: id はデフォルトで ignore_cols、exec_ts は ignore_prefixes により ignore\n",
    "    assert \"id\" in routed.ignore_cols\n",
    "    assert \"exec_ts\" in routed.ignore_cols\n",
    "\n",
    "    # unknown: ルールにない列は unknown\n",
    "    assert \"other_col\" in routed.unknown_cols\n",
    "\n",
    "\n",
    "def test_feature_router_prefers_longest_prefix_to_avoid_collisions():\n",
    "    cols = [\"unique_id\", \"ds\", \"y\", \"hist_x\", \"h_x\"]\n",
    "    # 衝突しやすいケース：h_ と hist_\n",
    "    prefix_to_kind = {\"h_\": \"hist\", \"hist_\": \"futr\"}  # わざと違うkindにする\n",
    "    routed = route_columns(cols, prefix_to_kind=prefix_to_kind)\n",
    "\n",
    "    # hist_x は hist_ が優先（長いprefix）\n",
    "    assert \"hist_x\" in routed.futr_exog_cols\n",
    "    # h_x は h_ にしか一致しない\n",
    "    assert \"h_x\" in routed.hist_exog_cols\n",
    "\n",
    "\n",
    "def test_feature_router_requires_meta_columns():\n",
    "    cols = [\"unique_id\", \"ds\", \"hist_x\"]\n",
    "    prefix_to_kind = {\"hist_\": \"hist\"}\n",
    "    with pytest.raises(ValueError):\n",
    "        route_columns(cols, prefix_to_kind=prefix_to_kind)\n",
    "'''\n",
    "tp.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e2fd6c",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae027a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                      [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_feature_router.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba422a37",
   "metadata": {},
   "source": [
    "## ID 025 `df`生成（unique_id/ds/y＋hist外生）\n",
    "\n",
    "目的:\n",
    "- join済みの巨大DataFrameから、NeuralForecast入力の df を生成する\n",
    "- df = [unique_id, ds, y] + hist_exog_cols（feature_routerで分類されたhist_*のみ）\n",
    "- dsはdatetimeへ正規化、（推奨）unique_idをstrへ正規化\n",
    "- 安定運用のため、(unique_id, ds)でソート可能にする\n",
    "\n",
    "注意:\n",
    "- futr_/stat_ はここでは入れない（次工程で future/static として扱えるように分離）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2465364",
   "metadata": {},
   "source": [
    "## ✅ codecell（実装）/mnt/e/env/ts/tslib/src/nf_app/builders/df_builder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aeaaaf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/builders/df_builder.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# package dir\n",
    "pkg = Path(\"/mnt/e/env/ts/tslib/src/nf_app/builders\")\n",
    "pkg.mkdir(parents=True, exist_ok=True)\n",
    "(pkg / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "p = pkg / \"df_builder.py\"\n",
    "\n",
    "code = r'''\\\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable, List, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nf_app.feature_router import RoutedFeatures\n",
    "\n",
    "\n",
    "def build_training_df(\n",
    "    joined_df: pd.DataFrame,\n",
    "    *,\n",
    "    routed: RoutedFeatures,\n",
    "    sort: bool = True,\n",
    "    coerce_unique_id_str: bool = True,\n",
    "    coerce_ds_datetime: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    NeuralForecast学習用 df を生成:\n",
    "      df = meta_cols(unique_id/ds/y) + hist_exog_cols\n",
    "\n",
    "    - joined_df: dataset結合結果（巨大でもOK）\n",
    "    - routed: route_columns() の結果（hist/futr/stat/ignore/unknownなど）\n",
    "    - sort: (unique_id, ds) でソート（再現性/学習安定性のため推奨）\n",
    "    \"\"\"\n",
    "    if joined_df is None:\n",
    "        raise ValueError(\"joined_df is None\")\n",
    "\n",
    "    # 必須メタ列\n",
    "    meta_cols = list(routed.meta_cols)\n",
    "    for c in meta_cols:\n",
    "        if c not in joined_df.columns:\n",
    "            raise ValueError(f\"Missing required meta column in joined_df: {c}\")\n",
    "\n",
    "    # hist外生（存在しない列が混ざっていたら、それは上流の不整合なので落とす）\n",
    "    hist_cols = list(routed.hist_exog_cols)\n",
    "    missing_hist = [c for c in hist_cols if c not in joined_df.columns]\n",
    "    if missing_hist:\n",
    "        raise ValueError(f\"Missing hist_exog columns in joined_df: {missing_hist}\")\n",
    "\n",
    "    cols = meta_cols + hist_cols\n",
    "\n",
    "    df = joined_df.loc[:, cols].copy()\n",
    "\n",
    "    # 型正規化\n",
    "    if coerce_unique_id_str and \"unique_id\" in df.columns:\n",
    "        df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n",
    "\n",
    "    if coerce_ds_datetime and \"ds\" in df.columns:\n",
    "        # DATE/文字列/Timestampなど混在に耐える\n",
    "        df[\"ds\"] = pd.to_datetime(df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    if sort:\n",
    "        # NeuralForecast系は (unique_id, ds) ソートが基本的に安全\n",
    "        df = df.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "'''\n",
    "p.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aedd11b",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest）/mnt/e/env/ts/tslib/tests/test_df_builder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f1512e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_df_builder.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tp = Path(\"/mnt/e/env/ts/tslib/tests/test_df_builder.py\")\n",
    "\n",
    "tcode = r'''\\\n",
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "from nf_app.feature_router import route_columns\n",
    "from nf_app.builders.df_builder import build_training_df\n",
    "\n",
    "\n",
    "def test_df_builder_columns_are_spec():\n",
    "    joined = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [2, 1],\n",
    "            \"ds\": [\"2020-01-02\", \"2020-01-01\"],\n",
    "            \"y\": [10, 20],\n",
    "            \"hist_a\": [0.1, 0.2],\n",
    "            \"hist_b\": [1, 2],\n",
    "            \"futr_x\": [9, 9],\n",
    "            \"stat_s\": [7, 7],\n",
    "            \"id\": [100, 101],\n",
    "            \"other\": [\"x\", \"y\"],\n",
    "        }\n",
    "    )\n",
    "    prefix_to_kind = {\"hist_\": \"hist\", \"futr_\": \"futr\", \"stat_\": \"stat\"}\n",
    "    routed = route_columns(list(joined.columns), prefix_to_kind=prefix_to_kind)\n",
    "\n",
    "    df = build_training_df(joined, routed=routed, sort=False)\n",
    "\n",
    "    # dfは meta + hist だけ\n",
    "    assert list(df.columns) == [\"unique_id\", \"ds\", \"y\", \"hist_a\", \"hist_b\"]\n",
    "\n",
    "    # dsはdatetime化\n",
    "    assert pd.api.types.is_datetime64_any_dtype(df[\"ds\"])\n",
    "\n",
    "    # futr/stat/other/idは混ざらない\n",
    "    assert \"futr_x\" not in df.columns\n",
    "    assert \"stat_s\" not in df.columns\n",
    "    assert \"other\" not in df.columns\n",
    "    assert \"id\" not in df.columns\n",
    "\n",
    "\n",
    "def test_df_builder_sorts_by_unique_id_and_ds():\n",
    "    joined = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [2, 1, 1],\n",
    "            \"ds\": [\"2020-01-02\", \"2020-01-03\", \"2020-01-01\"],\n",
    "            \"y\": [10, 30, 20],\n",
    "            \"hist_a\": [0.1, 0.3, 0.2],\n",
    "        }\n",
    "    )\n",
    "    routed = route_columns(list(joined.columns), prefix_to_kind={\"hist_\": \"hist\"})\n",
    "\n",
    "    df = build_training_df(joined, routed=routed, sort=True)\n",
    "\n",
    "    # ソート順を確認\n",
    "    assert df[\"unique_id\"].tolist() == [\"1\", \"1\", \"2\"]\n",
    "    assert df[\"ds\"].dt.strftime(\"%Y-%m-%d\").tolist() == [\"2020-01-01\", \"2020-01-03\", \"2020-01-02\"]\n",
    "\n",
    "\n",
    "def test_router_raises_when_meta_missing():\n",
    "    \"\"\"\n",
    "    meta列の欠落は、df_builderではなく feature_router（route_columns）が落とす設計。\n",
    "    pipeline上の前提（unique_id/ds/y）を満たさない場合は早期に止める。\n",
    "    \"\"\"\n",
    "    joined = pd.DataFrame({\"unique_id\": [\"a\"], \"ds\": [\"2020-01-01\"], \"hist_a\": [1]})\n",
    "    with pytest.raises(ValueError):\n",
    "        route_columns(list(joined.columns), prefix_to_kind={\"hist_\": \"hist\"})\n",
    "'''\n",
    "tp.write_text(tcode, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fc3d5",
   "metadata": {},
   "source": [
    "## ✅ codecell（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3cb244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                      [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q tests/test_df_builder.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266b0e0",
   "metadata": {},
   "source": [
    "### 3. Build futr_df（unique_id/ds＋futr外生）\n",
    "\n",
    "- joined（JOIN済みDataFrame）から、各 series（unique_id）ごとに「学習終了日（yが最後に存在するds）」以降の行を future とみなし、\n",
    "  予測ホライズン `h` 本ぶんの `futr_df` を作る。\n",
    "- `futr_df` の列は `unique_id`, `ds`, `futr_` 接頭辞列のみ。\n",
    "- 将来行が `h` 本未満なら ValueError（「予測ホライズン分を満たす」保証）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c5ecb",
   "metadata": {},
   "source": [
    "## ✅ Codeセル 1（ディレクトリ作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48fd0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /mnt/e/env/ts/tslib/src/nf_app/builders\n",
    "!mkdir -p /mnt/e/env/ts/tslib/tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d31cda",
   "metadata": {},
   "source": [
    "## ✅ Codeセル 2（実装：futr_builder.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e6a31c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/e/env/ts/tslib/src/nf_app/builders/futr_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/e/env/ts/tslib/src/nf_app/builders/futr_builder.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nf_app.feature_router import RoutedFeatures\n",
    "\n",
    "\n",
    "def _ensure_datetime(df: pd.DataFrame, col: str = \"ds\") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[col] = pd.to_datetime(out[col], errors=\"raise\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _last_train_ds(g: pd.DataFrame) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    学習終了日（train end）を推定:\n",
    "    - y列があれば「yが非nullの最大ds」\n",
    "    - y列がなければ「最大ds」（=全部学習扱い）\n",
    "    \"\"\"\n",
    "    if \"y\" in g.columns:\n",
    "        g2 = g[g[\"y\"].notna()]\n",
    "        if len(g2) > 0:\n",
    "            return pd.to_datetime(g2[\"ds\"]).max()\n",
    "    return pd.to_datetime(g[\"ds\"]).max()\n",
    "\n",
    "\n",
    "def build_futr_df(\n",
    "    joined: pd.DataFrame,\n",
    "    routed: RoutedFeatures,\n",
    "    *,\n",
    "    horizon: int,\n",
    "    require_full_horizon: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    futr_df（future exogenous）を生成する。\n",
    "\n",
    "    入力:\n",
    "      - joined: JOIN済みDataFrame（最低 unique_id, ds は必須。通常 y も含む）\n",
    "      - routed: route_columns の結果（routed.futr を利用）\n",
    "      - horizon: 予測ホライズン（h）\n",
    "    出力:\n",
    "      - columns: ['unique_id', 'ds'] + futr_cols\n",
    "      - rows: 各 unique_id ごとに「学習終了日以降」の先頭 horizon 行\n",
    "\n",
    "    設計方針（拡張性）:\n",
    "      - 列増減に強い: futr列は routed.futr を参照するだけ\n",
    "      - テーブル増減に強い: JOIN結果の列セットが変わっても routed に従う\n",
    "    \"\"\"\n",
    "    if horizon <= 0:\n",
    "        raise ValueError(\"horizon must be positive\")\n",
    "\n",
    "    for c in (\"unique_id\", \"ds\"):\n",
    "        if c not in joined.columns:\n",
    "            raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "    futr_cols: List[str] = list(getattr(routed, \"futr\", []))\n",
    "    # futr_cols が空でも futr_df 自体は作れる（モデル側で futr_exog を使わないケース）\n",
    "    base_cols = [\"unique_id\", \"ds\"]\n",
    "    keep_cols = base_cols + futr_cols\n",
    "\n",
    "    df = _ensure_datetime(joined, \"ds\")\n",
    "    # 不要列があっても落ちないように、存在する列だけ抜く\n",
    "    existing_keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "    df = df[existing_keep_cols].copy()\n",
    "\n",
    "    out_parts: List[pd.DataFrame] = []\n",
    "    for uid, g in df.groupby(\"unique_id\", sort=False):\n",
    "        g = g.sort_values(\"ds\")\n",
    "        # 学習終了日推定には joined 全体（y等）も見たいので、元joinedから取る\n",
    "        g_full = joined[joined[\"unique_id\"] == uid].copy()\n",
    "        g_full = _ensure_datetime(g_full, \"ds\").sort_values(\"ds\")\n",
    "        last_ds = _last_train_ds(g_full)\n",
    "\n",
    "        future = g[g[\"ds\"] > last_ds]\n",
    "        if require_full_horizon and len(future) < horizon:\n",
    "            raise ValueError(\n",
    "                f\"Insufficient future rows for unique_id={uid}: \"\n",
    "                f\"need horizon={horizon}, but got {len(future)}\"\n",
    "            )\n",
    "        future = future.head(horizon)\n",
    "        out_parts.append(future)\n",
    "\n",
    "    if not out_parts:\n",
    "        # unique_id が空など\n",
    "        return pd.DataFrame(columns=keep_cols)\n",
    "\n",
    "    out = pd.concat(out_parts, ignore_index=True)\n",
    "    # 列順を安定化\n",
    "    out = out[[c for c in keep_cols if c in out.columns]].copy()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243e61c",
   "metadata": {},
   "source": [
    "## ✅ Codeセル 3（pytest：test_futr_builder.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "06a188d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_futr_builder.py\n",
      "import pandas as pd\n",
      "import pytest\n",
      "\n",
      "from nf_app.feature_router import route_columns\n",
      "from nf_app.builders.futr_builder import build_futr_df\n",
      "\n",
      "\n",
      "def test_build_futr_df_picks_horizon_rows_per_series():\n",
      "    joined = pd.DataFrame(\n",
      "        {\n",
      "            \"unique_id\": [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\"],\n",
      "            \"ds\": [\"2020-01-01\",\"2020-01-02\",\"2020-01-03\",\"2020-01-01\",\"2020-01-02\",\"2020-01-03\"],\n",
      "            \"y\":\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/e/env/ts/tslib/tests/test_futr_builder.py\")\n",
    "p.write_text(\n",
    "\"\"\"\\\n",
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "from nf_app.feature_router import route_columns\n",
    "from nf_app.builders.futr_builder import build_futr_df\n",
    "\n",
    "\n",
    "def test_build_futr_df_picks_horizon_rows_per_series():\n",
    "    joined = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\"],\n",
    "            \"ds\": [\"2020-01-01\",\"2020-01-02\",\"2020-01-03\",\"2020-01-01\",\"2020-01-02\",\"2020-01-03\"],\n",
    "            \"y\": [1, 2, None, 10, 20, None],  # 3日目が未来（yなし）\n",
    "            \"hist_x\": [0.1,0.2,0.3, 1.1,1.2,1.3],\n",
    "            \"futr_x\": [100,101,102, 200,201,202],\n",
    "            \"stat_z\": [9,9,9, 8,8,8],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    routed = route_columns(\n",
    "        list(joined.columns),\n",
    "        prefix_to_kind={\"hist_\":\"hist\",\"futr_\":\"futr\",\"stat_\":\"stat\"},\n",
    "    )\n",
    "\n",
    "    futr_df = build_futr_df(joined, routed, horizon=1)\n",
    "\n",
    "    # 列: unique_id/ds + futr_* のみ（stat_ は含めない）\n",
    "    assert list(futr_df.columns) == [\"unique_id\", \"ds\", \"futr_x\"]\n",
    "\n",
    "    # 行: seriesごとに horizon 行（ここでは未来の1行＝2020-01-03が選ばれる）\n",
    "    assert len(futr_df) == 2\n",
    "    # ds は datetime に変換されている想定\n",
    "    ds_str = futr_df[\"ds\"].dt.strftime(\"%Y-%m-%d\").tolist()\n",
    "    assert ds_str == [\"2020-01-03\", \"2020-01-03\"]\n",
    "\n",
    "    # futr_x が残る\n",
    "    assert futr_df[\"futr_x\"].tolist() == [102, 202]\n",
    "\n",
    "\n",
    "def test_build_futr_df_raises_when_not_enough_future_rows():\n",
    "    joined = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\",\"a\",\"a\"],\n",
    "            \"ds\": [\"2020-01-01\",\"2020-01-02\",\"2020-01-03\"],\n",
    "            \"y\": [1, 2, None],     # 未来は1行しかない\n",
    "            \"futr_x\": [100,101,102],\n",
    "        }\n",
    "    )\n",
    "    routed = route_columns(list(joined.columns), prefix_to_kind={\"futr_\":\"futr\"})\n",
    "    with pytest.raises(ValueError):\n",
    "        build_futr_df(joined, routed, horizon=2)\n",
    "\n",
    "\n",
    "def test_build_futr_df_rejects_non_positive_horizon():\n",
    "    joined = pd.DataFrame({\"unique_id\":[\"a\"], \"ds\":[\"2020-01-01\"], \"y\":[1], \"futr_x\":[100]})\n",
    "    routed = route_columns(list(joined.columns), prefix_to_kind={\"futr_\":\"futr\"})\n",
    "    with pytest.raises(ValueError):\n",
    "        build_futr_df(joined, routed, horizon=0)\n",
    "\"\"\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(\"Wrote:\", p)\n",
    "print(p.read_text(encoding=\"utf-8\")[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125139d",
   "metadata": {},
   "source": [
    "## ✅ Codeセル 4（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "18c3f872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-8.3.5, pluggy-1.6.0 -- /home/az/miniconda3/envs/ts/bin/python3.11\n",
      "cachedir: .pytest_cache\n",
      "metadata: {'Python': '3.11.14', 'Platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'Packages': {'pytest': '8.3.5', 'pluggy': '1.6.0'}, 'Plugins': {'typeguard': '2.13.3', 'metadata': '3.1.1', 'anyio': '4.12.1', 'dash': '3.3.0', 'fugue': '0.9.4', 'env': '1.1.5', 'html': '4.1.1', 'jaxtyping': '0.2.29', 'hydra-core': '1.3.0'}}\n",
      "Fugue tests will be initialized with options:\n",
      "rootdir: /mnt/e/env/ts/tslib\n",
      "configfile: pytest.ini\n",
      "plugins: typeguard-2.13.3, metadata-3.1.1, anyio-4.12.1, dash-3.3.0, fugue-0.9.4, env-1.1.5, html-4.1.1, jaxtyping-0.2.29, hydra-core-1.3.0\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "<Dir tslib>\n",
      "  <Dir tests>\n",
      "    <Module test_futr_builder.py>\n",
      "      <Function test_build_futr_df_picks_horizon_rows_per_series>\n",
      "      <Function test_build_futr_df_raises_when_not_enough_future_rows>\n",
      "      <Function test_build_futr_df_rejects_non_positive_horizon>\n",
      "\n",
      "\u001b[32m========================== \u001b[32m3 tests collected\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q --collect-only -vv tests/test_futr_builder.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2ad64c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                      [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -q tests/test_futr_builder.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87db9d",
   "metadata": {},
   "source": [
    "## 3. Build static_df（ID 027）\n",
    "\n",
    "`joined`（dataset結合結果）から `stat_` 接頭辞の列（= static features）を抽出し、\n",
    "`unique_id` 単位で 1行に集約した `static_df` を生成する。\n",
    "\n",
    "仕様:\n",
    "- 列: unique_id + stat_* のみ\n",
    "- 行: 1 unique_id = 1 行（重複しない）\n",
    "- routed が dict/dataclass/任意オブジェクトでも stat 列を吸収できる\n",
    "- routed が無い/空でも stat_ 接頭辞でフォールバック（列増減に強い）\n",
    "- stat が行方向に揺れていても、ds昇順で「最後に観測できた値（last non-null）」を採用する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b5f3af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/src/nf_app/builders/static_builder.py\n",
      "# /mnt/e/env/ts/tslib/src/nf_app/builders/static_builder.py\n",
      "from __future__ import annotations\n",
      "\n",
      "from typing import Any, Dict, List, Sequence\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "def _as_list(x: Any) -> List[str]:\n",
      "    if x is None:\n",
      "        return []\n",
      "    if isinstance(x, list):\n",
      "        return [str(v) for v in x]\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "impl = Path(\"/mnt/e/env/ts/tslib/src/nf_app/builders/static_builder.py\")\n",
    "impl.write_text(\n",
    "\"\"\"\\\n",
    "# /mnt/e/env/ts/tslib/src/nf_app/builders/static_builder.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _as_list(x: Any) -> List[str]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(v) for v in x]\n",
    "    if isinstance(x, tuple):\n",
    "        return [str(v) for v in x]\n",
    "    if isinstance(x, set):\n",
    "        return [str(v) for v in x]\n",
    "    if isinstance(x, str):\n",
    "        return [x]\n",
    "    try:\n",
    "        return [str(v) for v in list(x)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def _get_routed_list(routed: Any, candidate_names: Sequence[str]) -> List[str]:\n",
    "    \\\"\\\"\\\"routed が dict / dataclass / 任意オブジェクトでも stat列を拾えるようにする\\\"\\\"\\\"\n",
    "    if routed is None:\n",
    "        return []\n",
    "\n",
    "    if isinstance(routed, dict):\n",
    "        for name in candidate_names:\n",
    "            if name in routed:\n",
    "                out = _as_list(routed.get(name))\n",
    "                if out:\n",
    "                    return out\n",
    "\n",
    "    for name in candidate_names:\n",
    "        if hasattr(routed, name):\n",
    "            out = _as_list(getattr(routed, name))\n",
    "            if out:\n",
    "                return out\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def build_static_df(\n",
    "    joined: pd.DataFrame,\n",
    "    routed: Any,\n",
    "    *,\n",
    "    unique_id_col: str = \"unique_id\",\n",
    "    ds_col: str = \"ds\",\n",
    "    stat_prefix: str = \"stat_\",\n",
    ") -> pd.DataFrame:\n",
    "    \\\"\\\"\\\"\n",
    "    static_df を生成する（NeuralForecast の static features 想定）\n",
    "\n",
    "    仕様:\n",
    "    - 列: unique_id + stat_* のみ\n",
    "    - 行: 1 unique_id = 1 行\n",
    "    - 集約: ds昇順で最後に観測できた値（last non-null）を採用\n",
    "    - routed が無い/空でも stat_ 接頭辞でフォールバック\n",
    "    \\\"\\\"\\\"\n",
    "    if unique_id_col not in joined.columns:\n",
    "        raise ValueError(f\"Missing required column: {unique_id_col}\")\n",
    "\n",
    "    df = joined.copy()\n",
    "\n",
    "    # ds が無いケースも運用上あり得るので、無ければ「入力順の最後」を使う\n",
    "    has_ds = ds_col in df.columns\n",
    "    if has_ds:\n",
    "        df[ds_col] = pd.to_datetime(df[ds_col], errors=\"coerce\")\n",
    "\n",
    "    # stat列抽出（routed の揺れに強く）\n",
    "    stat_cols = _get_routed_list(\n",
    "        routed,\n",
    "        candidate_names=[\n",
    "            \"stat\",\n",
    "            \"stat_cols\",\n",
    "            \"stat_exog\",\n",
    "            \"stat_exog_cols\",\n",
    "            \"stat_exog_list\",\n",
    "            \"static\",\n",
    "            \"static_cols\",\n",
    "            \"static_features\",\n",
    "            \"static_features_cols\",\n",
    "        ],\n",
    "    )\n",
    "    if not stat_cols:\n",
    "        stat_cols = [c for c in df.columns if c.startswith(stat_prefix)]\n",
    "\n",
    "    # stat列がゼロでも、空の static features として unique_id だけ返す（拡張に強い）\n",
    "    if not stat_cols:\n",
    "        out = df[[unique_id_col]].drop_duplicates().reset_index(drop=True)\n",
    "        return out\n",
    "\n",
    "    missing = [c for c in stat_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Some stat columns are missing in joined df: {missing}\")\n",
    "\n",
    "    # 並び替え（dsがあれば ds 昇順、無ければ入力順を尊重）\n",
    "    if has_ds:\n",
    "        # NaT は末尾に寄せたいので stable sort で\n",
    "        df = df.sort_values([unique_id_col, ds_col], kind=\"stable\")\n",
    "\n",
    "    # last non-null を取る関数\n",
    "    def _last_non_null(s: pd.Series):\n",
    "        nn = s.dropna()\n",
    "        if nn.empty:\n",
    "            return None\n",
    "        return nn.iloc[-1]\n",
    "\n",
    "    # unique_id 単位で集約\n",
    "    agg: Dict[str, Any] = {c: _last_non_null for c in stat_cols}\n",
    "    out = df.groupby(unique_id_col, dropna=False, sort=False).agg(agg).reset_index()\n",
    "\n",
    "    # 1 unique_id = 1 行を保証（groupby なので基本成立）\n",
    "    if out[unique_id_col].duplicated().any():\n",
    "        raise ValueError(\"static_df has duplicated unique_id rows (unexpected).\")\n",
    "\n",
    "    # 列順を固定\n",
    "    out = out[[unique_id_col] + stat_cols]\n",
    "    return out\n",
    "\"\"\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(\"Wrote:\", impl)\n",
    "print(impl.read_text(encoding=\"utf-8\")[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bd9f2",
   "metadata": {},
   "source": [
    "## codecell：pytest（テスト作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9f19ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/e/env/ts/tslib/tests/test_static_builder.py\n",
      "import pandas as pd\n",
      "\n",
      "from nf_app.feature_router import route_columns\n",
      "from nf_app.builders.static_builder import build_static_df\n",
      "\n",
      "\n",
      "def test_build_static_df_aggregates_stat_per_unique_id():\n",
      "    joined = pd.DataFrame(\n",
      "        {\n",
      "            \"unique_id\": [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\"],\n",
      "            \"ds\": [\"202\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "t = Path(\"/mnt/e/env/ts/tslib/tests/test_static_builder.py\")\n",
    "t.write_text(\n",
    "\"\"\"\\\n",
    "import pandas as pd\n",
    "\n",
    "from nf_app.feature_router import route_columns\n",
    "from nf_app.builders.static_builder import build_static_df\n",
    "\n",
    "\n",
    "def test_build_static_df_aggregates_stat_per_unique_id():\n",
    "    joined = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\"],\n",
    "            \"ds\": [\"2020-01-01\",\"2020-01-02\",\"2020-01-03\",\"2020-01-01\",\"2020-01-02\",\"2020-01-03\"],\n",
    "            \"y\": [1, 2, None, 10, 20, None],\n",
    "            \"hist_x\": [0.1,0.2,0.3, 1.1,1.2,1.3],\n",
    "            \"futr_x\": [100,101,102, 200,201,202],\n",
    "            \"stat_z\": [9,9,9, 8,8,8],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    routed = route_columns(\n",
    "        list(joined.columns),\n",
    "        prefix_to_kind={\"hist_\":\"hist\",\"futr_\":\"futr\",\"stat_\":\"stat\"},\n",
    "    )\n",
    "\n",
    "    static_df = build_static_df(joined, routed)\n",
    "\n",
    "    assert list(static_df.columns) == [\"unique_id\", \"stat_z\"]\n",
    "    assert len(static_df) == 2\n",
    "    assert static_df[\"unique_id\"].is_unique\n",
    "\n",
    "    # 値（unique_id順は保証しないので dict で確認）\n",
    "    got = dict(zip(static_df[\"unique_id\"].tolist(), static_df[\"stat_z\"].tolist()))\n",
    "    assert got == {\"a\": 9, \"b\": 8}\n",
    "\n",
    "\n",
    "def test_build_static_df_uses_last_non_null_when_stat_varies():\n",
    "    joined = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\",\"a\",\"a\"],\n",
    "            \"ds\": [\"2020-01-01\",\"2020-01-02\",\"2020-01-03\"],\n",
    "            \"y\": [1, 2, None],\n",
    "            \"stat_z\": [1, None, 3],  # last non-null は 3\n",
    "        }\n",
    "    )\n",
    "    routed = route_columns(list(joined.columns), prefix_to_kind={\"stat_\":\"stat\"})\n",
    "    static_df = build_static_df(joined, routed)\n",
    "    assert static_df.loc[0, \"unique_id\"] == \"a\"\n",
    "    assert static_df.loc[0, \"stat_z\"] == 3\n",
    "\n",
    "\n",
    "def test_build_static_df_returns_unique_id_only_when_no_stat_cols():\n",
    "    joined = pd.DataFrame(\n",
    "        {\"unique_id\": [\"a\",\"a\",\"b\"], \"ds\": [\"2020-01-01\",\"2020-01-02\",\"2020-01-01\"], \"y\":[1,2,3]}\n",
    "    )\n",
    "    routed = route_columns(list(joined.columns), prefix_to_kind={\"hist_\":\"hist\",\"futr_\":\"futr\",\"stat_\":\"stat\"})\n",
    "    static_df = build_static_df(joined, routed)\n",
    "    assert list(static_df.columns) == [\"unique_id\"]\n",
    "    assert len(static_df) == 2\n",
    "    assert set(static_df[\"unique_id\"].tolist()) == {\"a\",\"b\"}\n",
    "\"\"\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(\"Wrote:\", t)\n",
    "print(t.read_text(encoding=\"utf-8\")[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148340a",
   "metadata": {},
   "source": [
    "## codecell：収集確認 → 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "20b57436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/env/ts/tslib\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-8.3.5, pluggy-1.6.0 -- /home/az/miniconda3/envs/ts/bin/python3.11\n",
      "cachedir: .pytest_cache\n",
      "metadata: {'Python': '3.11.14', 'Platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'Packages': {'pytest': '8.3.5', 'pluggy': '1.6.0'}, 'Plugins': {'typeguard': '2.13.3', 'metadata': '3.1.1', 'anyio': '4.12.1', 'dash': '3.3.0', 'fugue': '0.9.4', 'env': '1.1.5', 'html': '4.1.1', 'jaxtyping': '0.2.29', 'hydra-core': '1.3.0'}}\n",
      "Fugue tests will be initialized with options:\n",
      "rootdir: /mnt/e/env/ts/tslib\n",
      "configfile: pytest.ini\n",
      "plugins: typeguard-2.13.3, metadata-3.1.1, anyio-4.12.1, dash-3.3.0, fugue-0.9.4, env-1.1.5, html-4.1.1, jaxtyping-0.2.29, hydra-core-1.3.0\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "<Dir tslib>\n",
      "  <Dir tests>\n",
      "    <Module test_static_builder.py>\n",
      "      <Function test_build_static_df_aggregates_stat_per_unique_id>\n",
      "      <Function test_build_static_df_uses_last_non_null_when_stat_varies>\n",
      "      <Function test_build_static_df_returns_unique_id_only_when_no_stat_cols>\n",
      "\n",
      "\u001b[32m========================== \u001b[32m3 tests collected\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/e/env/ts/tslib\n",
    "!pytest -q --collect-only -vv tests/test_static_builder.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d7e14",
   "metadata": {},
   "source": [
    "### 3. Build static_df（stat_ を unique_id で 1行に集約）\n",
    "- joined から stat_（静的特徴量）だけを抜き出し、unique_id 単位で 1 行にする\n",
    "- routed（分類結果）が dict/dataclass/任意オブジェクトでも動く\n",
    "- stat_ が時系列で揺れている場合は「最後の非NULL値」を採用（運用で破綻しにくい）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a6faa98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "mkdir -p /mnt/e/env/ts/tslib/src/nf_app/builders\n",
    "\n",
    "cat > /mnt/e/env/ts/tslib/src/nf_app/builders/static_builder.py <<'PY'\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _as_list(x: Any) -> List[str]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(v) for v in x]\n",
    "    if isinstance(x, tuple):\n",
    "        return [str(v) for v in x]\n",
    "    if isinstance(x, set):\n",
    "        return [str(v) for v in x]\n",
    "    if isinstance(x, str):\n",
    "        return [x]\n",
    "    try:\n",
    "        return [str(v) for v in list(x)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def _get_routed_list(routed: Any, candidate_names: Sequence[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    routed が dict / dataclass / 任意オブジェクトでも「それっぽい属性名」からリスト抽出する。\n",
    "    \"\"\"\n",
    "    if routed is None:\n",
    "        return []\n",
    "\n",
    "    if isinstance(routed, dict):\n",
    "        for name in candidate_names:\n",
    "            if name in routed:\n",
    "                out = _as_list(routed.get(name))\n",
    "                if out:\n",
    "                    return out\n",
    "\n",
    "    for name in candidate_names:\n",
    "        if hasattr(routed, name):\n",
    "            out = _as_list(getattr(routed, name))\n",
    "            if out:\n",
    "                return out\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def build_static_df(\n",
    "    joined: pd.DataFrame,\n",
    "    routed: Any,\n",
    "    *,\n",
    "    unique_id_col: str = \"unique_id\",\n",
    "    ds_col: str = \"ds\",\n",
    "    stat_prefix: str = \"stat_\",\n",
    "    validate_constant: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    static_df を生成（NeuralForecast の static_features 用）\n",
    "\n",
    "    仕様:\n",
    "    - 1 unique_id = 1 行\n",
    "    - 列: unique_id + stat_ 列（stat_ が無ければ unique_id のみ）\n",
    "    - stat_ が時系列で変化しても「最後の非NULL値」を採用（安定運用寄り）\n",
    "    - validate_constant=True の場合、unique_id 内で stat_ が複数値なら例外（品質ゲート）\n",
    "    \"\"\"\n",
    "    if unique_id_col not in joined.columns:\n",
    "        raise ValueError(f\"Missing required column: {unique_id_col}\")\n",
    "\n",
    "    df = joined.copy()\n",
    "\n",
    "    if ds_col in df.columns:\n",
    "        df[ds_col] = pd.to_datetime(df[ds_col], errors=\"coerce\")\n",
    "        if df[ds_col].isna().any():\n",
    "            bad = df[df[ds_col].isna()].head(5)\n",
    "            raise ValueError(f\"{ds_col} contains unparseable values. sample=\\n{bad}\")\n",
    "\n",
    "    stat_cols = _get_routed_list(\n",
    "        routed,\n",
    "        candidate_names=[\n",
    "            \"stat\",\n",
    "            \"stat_cols\",\n",
    "            \"static\",\n",
    "            \"static_cols\",\n",
    "            \"static_features\",\n",
    "            \"static_features_cols\",\n",
    "        ],\n",
    "    )\n",
    "    if not stat_cols:\n",
    "        stat_cols = [c for c in df.columns if c.startswith(stat_prefix)]\n",
    "\n",
    "    # stat_ が無い場合も破綻しない（unique_id のみ返す）\n",
    "    if not stat_cols:\n",
    "        return df[[unique_id_col]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    missing = [c for c in stat_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Some stat columns are missing in joined df: {missing}\")\n",
    "\n",
    "    # unique_idごとに最後の行（dsがあればds順、なければ出現順）を取って stat を確定させる\n",
    "    if ds_col in df.columns:\n",
    "        df = df.sort_values([unique_id_col, ds_col], kind=\"stable\")\n",
    "    else:\n",
    "        df = df.sort_values([unique_id_col], kind=\"stable\")\n",
    "\n",
    "    last = df.groupby(unique_id_col, dropna=False, sort=False).tail(1)\n",
    "\n",
    "    out = last[[unique_id_col] + stat_cols].copy()\n",
    "\n",
    "    # 最後の行が NULL の場合に備え、列ごとに「最後の非NULL値」を補完（series内で最後を優先）\n",
    "    for c in stat_cols:\n",
    "        if out[c].isna().any():\n",
    "            # seriesごとに最後の非NULL\n",
    "            s = (\n",
    "                df[[unique_id_col, c]]\n",
    "                .dropna(subset=[c])\n",
    "                .groupby(unique_id_col, dropna=False, sort=False)[c]\n",
    "                .tail(1)\n",
    "            )\n",
    "            # s は index が元のdfの行indexなので、unique_id→値に変換して map\n",
    "            s_map = df.loc[s.index, [unique_id_col, c]].drop_duplicates(subset=[unique_id_col], keep=\"last\")\n",
    "            out[c] = out[c].where(out[c].notna(), out[unique_id_col].map(dict(zip(s_map[unique_id_col], s_map[c]))))\n",
    "\n",
    "    if validate_constant:\n",
    "        # unique_id内で複数値（NA除外）があるなら落とす\n",
    "        bad_cols: List[str] = []\n",
    "        for c in stat_cols:\n",
    "            nun = (\n",
    "                df[[unique_id_col, c]]\n",
    "                .dropna(subset=[c])\n",
    "                .groupby(unique_id_col, dropna=False)[c]\n",
    "                .nunique()\n",
    "            )\n",
    "            if (nun > 1).any():\n",
    "                bad_cols.append(c)\n",
    "        if bad_cols:\n",
    "            raise ValueError(f\"static features not constant within series: {bad_cols}\")\n",
    "\n",
    "    return out.drop_duplicates(subset=[unique_id_col], keep=\"last\").reset_index(drop=True)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3c2f7",
   "metadata": {},
   "source": [
    "## ✅ Codeセル（pytest作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7f57add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "cat > /mnt/e/env/ts/tslib/tests/test_static_builder.py <<'PY'\n",
    "import pandas as pd\n",
    "\n",
    "from nf_app.builders.static_builder import build_static_df\n",
    "from nf_app.feature_router import route_columns\n",
    "\n",
    "\n",
    "def test_build_static_df_one_row_per_unique_id():\n",
    "    joined = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\", \"a\", \"b\", \"b\"],\n",
    "            \"ds\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-01\", \"2020-01-02\"],\n",
    "            \"y\": [1, 2, 10, 20],\n",
    "            \"stat_z\": [9, 9, 8, 8],\n",
    "            \"stat_w\": [1.5, 1.5, 2.5, 2.5],\n",
    "        }\n",
    "    )\n",
    "    routed = route_columns(\n",
    "        list(joined.columns),\n",
    "        prefix_to_kind={\"hist_\": \"hist\", \"futr_\": \"futr\", \"stat_\": \"stat\"},\n",
    "    )\n",
    "\n",
    "    static_df = build_static_df(joined, routed)\n",
    "    assert list(static_df.columns) == [\"unique_id\", \"stat_z\", \"stat_w\"]\n",
    "    assert static_df[\"unique_id\"].nunique() == len(static_df) == 2\n",
    "    # a は stat_z=9, b は stat_z=8\n",
    "    m = dict(zip(static_df[\"unique_id\"], static_df[\"stat_z\"]))\n",
    "    assert m[\"a\"] == 9\n",
    "    assert m[\"b\"] == 8\n",
    "\n",
    "\n",
    "def test_build_static_df_uses_last_non_null_when_last_row_has_null():\n",
    "    joined = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\", \"a\", \"a\"],\n",
    "            \"ds\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"],\n",
    "            \"y\": [1, 2, 3],\n",
    "            \"stat_z\": [9, 9, None],  # 最終行が欠損\n",
    "        }\n",
    "    )\n",
    "    routed = route_columns(\n",
    "        list(joined.columns),\n",
    "        prefix_to_kind={\"stat_\": \"stat\"},\n",
    "    )\n",
    "    static_df = build_static_df(joined, routed)\n",
    "    assert static_df.loc[0, \"stat_z\"] == 9\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940406d1",
   "metadata": {},
   "source": [
    "## ✅ Codeセル（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3ee9f126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                       [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /mnt/e/env/ts/tslib\n",
    "pytest -q tests/test_static_builder.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868db29",
   "metadata": {},
   "source": [
    "### 3. Dtypes（NeuralForecast投入用の型整形）\n",
    "- unique_id: str（文字列）\n",
    "- ds: datetime64[ns]（タイムゾーン付きなら tz を落として naive 化）\n",
    "- y: float（欠損は NaN）\n",
    "- 外生変数（hist_/futr_/stat_ など）は基本 float に寄せる（torch で落ちにくくする）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0471b",
   "metadata": {},
   "source": [
    "## ✅ Codeセル（実装ファイル作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "89e046d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "cat > /mnt/e/env/ts/tslib/src/nf_app/transforms.py <<'PY'\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Iterable, List, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DtypeSpec:\n",
    "    unique_id_col: str = \"unique_id\"\n",
    "    ds_col: str = \"ds\"\n",
    "    y_col: str = \"y\"\n",
    "\n",
    "\n",
    "def _to_datetime_naive(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    # tz-aware を naive に戻す（NeuralForecast/torch で扱いやすい）\n",
    "    try:\n",
    "        if hasattr(dt.dt, \"tz\") and dt.dt.tz is not None:\n",
    "            dt = dt.dt.tz_convert(None)\n",
    "    except Exception:\n",
    "        # dt が datetime じゃないケースなど\n",
    "        pass\n",
    "    return dt\n",
    "\n",
    "\n",
    "def normalize_main_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    spec: DtypeSpec = DtypeSpec(),\n",
    "    exog_cols: Optional[Sequence[str]] = None,\n",
    "    coerce_exog_numeric: bool = True,\n",
    "    copy: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    学習用 df（unique_id/ds/y + hist外生）を NeuralForecast に入れやすい型へ正規化。\n",
    "    \"\"\"\n",
    "    if copy:\n",
    "        out = df.copy()\n",
    "    else:\n",
    "        out = df\n",
    "\n",
    "    for c in (spec.unique_id_col, spec.ds_col, spec.y_col):\n",
    "        if c not in out.columns:\n",
    "            raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "    out[spec.unique_id_col] = out[spec.unique_id_col].astype(str)\n",
    "    out[spec.ds_col] = _to_datetime_naive(out[spec.ds_col])\n",
    "    if out[spec.ds_col].isna().any():\n",
    "        bad = out[out[spec.ds_col].isna()].head(5)\n",
    "        raise ValueError(f\"{spec.ds_col} contains unparseable values. sample=\\n{bad}\")\n",
    "\n",
    "    out[spec.y_col] = pd.to_numeric(out[spec.y_col], errors=\"coerce\").astype(float)\n",
    "\n",
    "    if exog_cols is None:\n",
    "        # meta以外を外生候補として扱う（ただし文字列列が混ざる場合もあるので coercion を条件化）\n",
    "        exog_cols = [c for c in out.columns if c not in (spec.unique_id_col, spec.ds_col, spec.y_col)]\n",
    "\n",
    "    if coerce_exog_numeric:\n",
    "        for c in exog_cols:\n",
    "            if c in out.columns:\n",
    "                out[c] = pd.to_numeric(out[c], errors=\"coerce\").astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def normalize_futr_df(\n",
    "    futr_df: pd.DataFrame,\n",
    "    *,\n",
    "    spec: DtypeSpec = DtypeSpec(),\n",
    "    exog_cols: Optional[Sequence[str]] = None,\n",
    "    coerce_exog_numeric: bool = True,\n",
    "    copy: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    予測用 futr_df（unique_id/ds + futr外生）を正規化。\n",
    "    \"\"\"\n",
    "    if copy:\n",
    "        out = futr_df.copy()\n",
    "    else:\n",
    "        out = futr_df\n",
    "\n",
    "    for c in (spec.unique_id_col, spec.ds_col):\n",
    "        if c not in out.columns:\n",
    "            raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "    out[spec.unique_id_col] = out[spec.unique_id_col].astype(str)\n",
    "    out[spec.ds_col] = _to_datetime_naive(out[spec.ds_col])\n",
    "    if out[spec.ds_col].isna().any():\n",
    "        bad = out[out[spec.ds_col].isna()].head(5)\n",
    "        raise ValueError(f\"{spec.ds_col} contains unparseable values. sample=\\n{bad}\")\n",
    "\n",
    "    if exog_cols is None:\n",
    "        exog_cols = [c for c in out.columns if c not in (spec.unique_id_col, spec.ds_col)]\n",
    "\n",
    "    if coerce_exog_numeric:\n",
    "        for c in exog_cols:\n",
    "            if c in out.columns:\n",
    "                out[c] = pd.to_numeric(out[c], errors=\"coerce\").astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def normalize_static_df(\n",
    "    static_df: pd.DataFrame,\n",
    "    *,\n",
    "    spec: DtypeSpec = DtypeSpec(),\n",
    "    feature_cols: Optional[Sequence[str]] = None,\n",
    "    coerce_numeric: bool = True,\n",
    "    copy: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    static_df（unique_id + stat特徴）を正規化。\n",
    "    \"\"\"\n",
    "    if copy:\n",
    "        out = static_df.copy()\n",
    "    else:\n",
    "        out = static_df\n",
    "\n",
    "    if spec.unique_id_col not in out.columns:\n",
    "        raise ValueError(f\"Missing required column: {spec.unique_id_col}\")\n",
    "\n",
    "    out[spec.unique_id_col] = out[spec.unique_id_col].astype(str)\n",
    "\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in out.columns if c != spec.unique_id_col]\n",
    "\n",
    "    if coerce_numeric:\n",
    "        for c in feature_cols:\n",
    "            if c in out.columns:\n",
    "                out[c] = pd.to_numeric(out[c], errors=\"coerce\").astype(float)\n",
    "\n",
    "    return out\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e09fca7",
   "metadata": {},
   "source": [
    "## ✅ Codeセル（pytest作成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1962ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "cat > /mnt/e/env/ts/tslib/tests/test_transforms.py <<'PY'\n",
    "import pandas as pd\n",
    "\n",
    "from nf_app.transforms import normalize_main_df, normalize_futr_df, normalize_static_df, DtypeSpec\n",
    "\n",
    "\n",
    "def test_normalize_main_df_casts_types():\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [1, 1, 2],\n",
    "            \"ds\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-01\"],\n",
    "            \"y\": [10, \"20\", None],\n",
    "            \"hist_x\": [1, 2, 3],\n",
    "        }\n",
    "    )\n",
    "    out = normalize_main_df(df, exog_cols=[\"hist_x\"])\n",
    "    assert out[\"unique_id\"].dtype == object  # str化\n",
    "    assert pd.api.types.is_datetime64_any_dtype(out[\"ds\"])\n",
    "    assert out[\"y\"].dtype == float\n",
    "    assert out[\"hist_x\"].dtype == float\n",
    "\n",
    "\n",
    "def test_normalize_futr_df_casts_types():\n",
    "    futr = pd.DataFrame(\n",
    "        {\"unique_id\": [\"a\", \"a\"], \"ds\": [\"2020-01-03\", \"2020-01-04\"], \"futr_x\": [100, \"101\"]}\n",
    "    )\n",
    "    out = normalize_futr_df(futr, exog_cols=[\"futr_x\"])\n",
    "    assert pd.api.types.is_datetime64_any_dtype(out[\"ds\"])\n",
    "    assert out[\"futr_x\"].dtype == float\n",
    "\n",
    "\n",
    "def test_normalize_static_df_casts_features():\n",
    "    st = pd.DataFrame({\"unique_id\": [\"a\", \"b\"], \"stat_z\": [\"9\", 8]})\n",
    "    out = normalize_static_df(st, feature_cols=[\"stat_z\"])\n",
    "    assert out[\"stat_z\"].dtype == float\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8362a3c",
   "metadata": {},
   "source": [
    "## ✅ Codeセル（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3be3caf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                      [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /mnt/e/env/ts/tslib\n",
    "pytest -q tests/test_transforms.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebcd73",
   "metadata": {},
   "source": [
    "### 3. Missing Policy（欠損戦略 drop / impute / flag をRunPlan化）\n",
    "- RunPlan(params_json) に `missing_policy` を持たせ、欠損処理の再現性を担保する\n",
    "- y（目的変数）は学習上欠損不可 → 原則 drop（例外的に impute も選べる）\n",
    "- 外生変数（exog）は drop / impute / flag を切替可能\n",
    "- flag は「欠損フラグ列を追加しつつ、値は一定値で埋める」方式（モデルが欠損情報を学習できる）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6ade4",
   "metadata": {},
   "source": [
    "## 🧩 Codeセル（実装：/mnt/e/env/ts/tslib/src/nf_app/missing.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ad73f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "cat > /mnt/e/env/ts/tslib/src/nf_app/missing.py <<'PY'\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MissingPolicy:\n",
    "    \"\"\"\n",
    "    欠損戦略（RunPlan化する前提）\n",
    "\n",
    "    mode:\n",
    "      - \"drop\"   : 欠損がある行を落とす（主に exog 欠損に対して）\n",
    "      - \"impute\" : 欠損を埋める\n",
    "      - \"flag\"   : 欠損フラグ列を追加し、値は埋める（通常 constant=0）\n",
    "    y_action:\n",
    "      - \"drop\"   : y 欠損行は落とす（デフォルト）\n",
    "      - \"impute\" : y も埋める（推奨しないが再現性は担保する）\n",
    "    impute_method（mode=impute/flag のとき）:\n",
    "      - \"zero\" | \"mean\" | \"median\" | \"constant\" | \"ffill\" | \"bfill\"\n",
    "      ※ ffill/bfill は series（unique_id）内・ds順で処理（必要なら）\n",
    "    constant_value:\n",
    "      - \"constant\" / \"flag\" の埋め値（デフォルト 0.0）\n",
    "    add_flags:\n",
    "      - True の場合、mode=\"impute\" でもフラグ列を追加できる\n",
    "    \"\"\"\n",
    "    mode: str = \"impute\"\n",
    "    y_action: str = \"drop\"\n",
    "    impute_method: str = \"zero\"\n",
    "    constant_value: float = 0.0\n",
    "    add_flags: bool = False\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(d: Optional[Dict[str, Any]]) -> \"MissingPolicy\":\n",
    "        d = d or {}\n",
    "        return MissingPolicy(\n",
    "            mode=str(d.get(\"mode\", \"impute\")).upper().lower(),  # safety: normalize\n",
    "            y_action=str(d.get(\"y_action\", \"drop\")).upper().lower(),\n",
    "            impute_method=str(d.get(\"impute_method\", \"zero\")).upper().lower(),\n",
    "            constant_value=float(d.get(\"constant_value\", 0.0)),\n",
    "            add_flags=bool(d.get(\"add_flags\", False)),\n",
    "        )\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"mode\": self.mode,\n",
    "            \"y_action\": self.y_action,\n",
    "            \"impute_method\": self.impute_method,\n",
    "            \"constant_value\": self.constant_value,\n",
    "            \"add_flags\": self.add_flags,\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MissingReport:\n",
    "    dropped_rows: int\n",
    "    imputed_counts: Dict[str, int]\n",
    "    flags_added: List[str]\n",
    "\n",
    "\n",
    "def load_missing_policy_from_runplan_params(params_json: Optional[Dict[str, Any]]) -> MissingPolicy:\n",
    "    \"\"\"\n",
    "    run_plans.params_json から missing_policy を取得する想定。\n",
    "    例:\n",
    "      params_json = {\"missing_policy\": {\"mode\":\"flag\",\"impute_method\":\"constant\",\"constant_value\":0}}\n",
    "    \"\"\"\n",
    "    params_json = params_json or {}\n",
    "    mp = params_json.get(\"missing_policy\")\n",
    "    if isinstance(mp, dict):\n",
    "        return MissingPolicy.from_dict(mp)\n",
    "    return MissingPolicy()  # default\n",
    "\n",
    "\n",
    "def _ensure_cols_exist(df: pd.DataFrame, cols: Sequence[str], *, label: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{label}: missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "def _numericize(df: pd.DataFrame, cols: Sequence[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _compute_fill_value(s: pd.Series, method: str, constant_value: float) -> float:\n",
    "    # すべて欠損のときも deterministic に落とす（NaN を返さない）\n",
    "    nonnull = s.dropna()\n",
    "    if method == \"zero\":\n",
    "        return 0.0\n",
    "    if method == \"constant\":\n",
    "        return float(constant_value)\n",
    "    if nonnull.empty:\n",
    "        # mean/median のときに NaN で返さない\n",
    "        return float(constant_value if method in (\"mean\", \"median\") else 0.0)\n",
    "    if method == \"mean\":\n",
    "        return float(nonnull.mean())\n",
    "    if method == \"median\":\n",
    "        return float(nonnull.median())\n",
    "    # ffill/bfill は値じゃなく処理なのでここには来ない\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def apply_missing_policy(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    policy: MissingPolicy,\n",
    "    unique_id_col: str = \"unique_id\",\n",
    "    ds_col: str = \"ds\",\n",
    "    y_col: Optional[str] = \"y\",\n",
    "    exog_cols: Optional[Sequence[str]] = None,\n",
    "    serieswise_fill: bool = True,\n",
    ") -> Tuple[pd.DataFrame, MissingReport]:\n",
    "    \"\"\"\n",
    "    欠損処理の本体。\n",
    "    - df: 学習df / futr_df / static_df いずれにも使える\n",
    "    - y_col=None の場合は y 処理を行わない（futr_df や static_df 用）\n",
    "    - exog_cols が None の場合、(unique_id, ds, y) 以外を exog とみなす\n",
    "    \"\"\"\n",
    "    if policy.mode not in (\"drop\", \"impute\", \"flag\"):\n",
    "        raise ValueError(f\"Unknown policy.mode: {policy.mode}\")\n",
    "    if policy.y_action not in (\"drop\", \"impute\"):\n",
    "        raise ValueError(f\"Unknown policy.y_action: {policy.y_action}\")\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    _ensure_cols_exist(out, [unique_id_col], label=\"apply_missing_policy\")\n",
    "    if ds_col in out.columns:\n",
    "        out[ds_col] = pd.to_datetime(out[ds_col], errors=\"coerce\")\n",
    "        if out[ds_col].isna().any():\n",
    "            bad = out[out[ds_col].isna()].head(5)\n",
    "            raise ValueError(f\"{ds_col} contains unparseable values. sample=\\n{bad}\")\n",
    "\n",
    "    if exog_cols is None:\n",
    "        base = {unique_id_col}\n",
    "        if ds_col in out.columns:\n",
    "            base.add(ds_col)\n",
    "        if y_col is not None:\n",
    "            base.add(y_col)\n",
    "        exog_cols = [c for c in out.columns if c not in base]\n",
    "\n",
    "    # 欠損フラグ（必要なら）\n",
    "    flags_added: List[str] = []\n",
    "    want_flags = (policy.mode == \"flag\") or policy.add_flags\n",
    "    if want_flags:\n",
    "        for c in exog_cols:\n",
    "            if c in out.columns:\n",
    "                flag_col = f\"{c}__isna\"\n",
    "                out[flag_col] = out[c].isna().astype(int)\n",
    "                flags_added.append(flag_col)\n",
    "\n",
    "    # numericに寄せる（mean/medianなどのため）\n",
    "    out = _numericize(out, [c for c in exog_cols if c in out.columns])\n",
    "    if y_col is not None and y_col in out.columns:\n",
    "        out[y_col] = pd.to_numeric(out[y_col], errors=\"coerce\")\n",
    "\n",
    "    dropped_rows = 0\n",
    "    imputed_counts: Dict[str, int] = {}\n",
    "\n",
    "    # 1) y の処理（学習df向け）\n",
    "    if y_col is not None and y_col in out.columns:\n",
    "        if policy.y_action == \"drop\":\n",
    "            before = len(out)\n",
    "            out = out[out[y_col].notna()].copy()\n",
    "            dropped_rows += before - len(out)\n",
    "        else:  # impute\n",
    "            # y は serieswise_fill の恩恵が薄いので全体で deterministic に埋める\n",
    "            n = int(out[y_col].isna().sum())\n",
    "            if n > 0:\n",
    "                fillv = _compute_fill_value(out[y_col], policy.impute_method, policy.constant_value)\n",
    "                out[y_col] = out[y_col].fillna(fillv)\n",
    "                imputed_counts[y_col] = n\n",
    "\n",
    "    # 2) exog の処理\n",
    "    if not exog_cols:\n",
    "        return out.reset_index(drop=True), MissingReport(dropped_rows, imputed_counts, flags_added)\n",
    "\n",
    "    if policy.mode == \"drop\":\n",
    "        # exog のどれかが欠損なら行drop（必要なら列を絞って運用する余地はあるが、まずは明快に）\n",
    "        cols = [c for c in exog_cols if c in out.columns]\n",
    "        if cols:\n",
    "            before = len(out)\n",
    "            out = out.dropna(subset=cols).copy()\n",
    "            dropped_rows += before - len(out)\n",
    "        return out.reset_index(drop=True), MissingReport(dropped_rows, imputed_counts, flags_added)\n",
    "\n",
    "    # impute / flag は埋める\n",
    "    method = policy.impute_method\n",
    "\n",
    "    cols = [c for c in exog_cols if c in out.columns]\n",
    "    if not cols:\n",
    "        return out.reset_index(drop=True), MissingReport(dropped_rows, imputed_counts, flags_added)\n",
    "\n",
    "    # ffill/bfill は series内・ds順でやるのが安全（予測の時間方向を壊しにくい）\n",
    "    if method in (\"ffill\", \"bfill\"):\n",
    "        if serieswise_fill and ds_col in out.columns:\n",
    "            out = out.sort_values([unique_id_col, ds_col], kind=\"stable\")\n",
    "            g = out.groupby(unique_id_col, dropna=False, sort=False)\n",
    "            for c in cols:\n",
    "                n = int(out[c].isna().sum())\n",
    "                if n == 0:\n",
    "                    continue\n",
    "                if method == \"ffill\":\n",
    "                    out[c] = g[c].ffill()\n",
    "                else:\n",
    "                    out[c] = g[c].bfill()\n",
    "                # 残る欠損は constant で最後に埋める（端の欠損対策）\n",
    "                rem = int(out[c].isna().sum())\n",
    "                if rem > 0:\n",
    "                    out[c] = out[c].fillna(float(policy.constant_value))\n",
    "                imputed_counts[c] = n\n",
    "        else:\n",
    "            # dsがない/serieswiseでない → 全体で fill\n",
    "            for c in cols:\n",
    "                n = int(out[c].isna().sum())\n",
    "                if n == 0:\n",
    "                    continue\n",
    "                out[c] = out[c].ffill() if method == \"ffill\" else out[c].bfill()\n",
    "                rem = int(out[c].isna().sum())\n",
    "                if rem > 0:\n",
    "                    out[c] = out[c].fillna(float(policy.constant_value))\n",
    "                imputed_counts[c] = n\n",
    "\n",
    "        return out.reset_index(drop=True), MissingReport(dropped_rows, imputed_counts, flags_added)\n",
    "\n",
    "    # mean/median/zero/constant\n",
    "    for c in cols:\n",
    "        n = int(out[c].isna().sum())\n",
    "        if n == 0:\n",
    "            continue\n",
    "        fillv = _compute_fill_value(out[c], method, policy.constant_value)\n",
    "        out[c] = out[c].fillna(fillv)\n",
    "        imputed_counts[c] = n\n",
    "\n",
    "    # mode=flag のとき、埋め方法は上で決まる（典型は constant/zero）\n",
    "    return out.reset_index(drop=True), MissingReport(dropped_rows, imputed_counts, flags_added)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e62f8",
   "metadata": {},
   "source": [
    "## 🧪 Codeセル（pytest：/mnt/e/env/ts/tslib/tests/test_missing.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3ee88229",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "cat > /mnt/e/env/ts/tslib/tests/test_missing.py <<'PY'\n",
    "import pandas as pd\n",
    "\n",
    "from nf_app.missing import MissingPolicy, apply_missing_policy, load_missing_policy_from_runplan_params\n",
    "\n",
    "\n",
    "def test_load_missing_policy_from_runplan_params_defaults():\n",
    "    p = load_missing_policy_from_runplan_params({})\n",
    "    assert isinstance(p, MissingPolicy)\n",
    "    assert p.mode in (\"drop\", \"impute\", \"flag\")\n",
    "\n",
    "\n",
    "def test_drop_policy_drops_rows_with_missing_y_and_exog():\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\", \"a\", \"a\"],\n",
    "            \"ds\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"],\n",
    "            \"y\": [1.0, None, 3.0],\n",
    "            \"hist_x\": [10.0, 20.0, None],\n",
    "        }\n",
    "    )\n",
    "    policy = MissingPolicy(mode=\"drop\", y_action=\"drop\")\n",
    "    out, rep = apply_missing_policy(df, policy=policy, exog_cols=[\"hist_x\"])\n",
    "    # y欠損行(1行) + exog欠損行(1行) が落ちて 1行だけ残る\n",
    "    assert len(out) == 1\n",
    "    assert rep.dropped_rows == 2\n",
    "\n",
    "\n",
    "def test_impute_mean_is_deterministic():\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\", \"a\", \"b\", \"b\"],\n",
    "            \"ds\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-01\", \"2020-01-02\"],\n",
    "            \"y\": [1.0, 2.0, 3.0, 4.0],\n",
    "            \"hist_x\": [10.0, None, 30.0, None],\n",
    "        }\n",
    "    )\n",
    "    policy = MissingPolicy(mode=\"impute\", y_action=\"drop\", impute_method=\"mean\", constant_value=0.0)\n",
    "    out1, rep1 = apply_missing_policy(df, policy=policy, exog_cols=[\"hist_x\"])\n",
    "    out2, rep2 = apply_missing_policy(df, policy=policy, exog_cols=[\"hist_x\"])\n",
    "    pd.testing.assert_frame_equal(out1, out2)\n",
    "    assert rep1.imputed_counts == rep2.imputed_counts\n",
    "    # mean(10,30)=20 を入れるので欠損2個が20になる\n",
    "    assert (out1[\"hist_x\"] == 20.0).sum() == 2\n",
    "\n",
    "\n",
    "def test_flag_adds_indicator_and_fills_constant():\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"unique_id\": [\"a\", \"a\", \"b\"],\n",
    "            \"ds\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-01\"],\n",
    "            \"y\": [1.0, 2.0, 3.0],\n",
    "            \"hist_x\": [None, 5.0, None],\n",
    "        }\n",
    "    )\n",
    "    policy = MissingPolicy(mode=\"flag\", y_action=\"drop\", impute_method=\"constant\", constant_value=0.0)\n",
    "    out, rep = apply_missing_policy(df, policy=policy, exog_cols=[\"hist_x\"])\n",
    "    assert \"hist_x__isna\" in out.columns\n",
    "    # 欠損だったところが0で埋まる\n",
    "    assert out.loc[out[\"hist_x__isna\"] == 1, \"hist_x\"].eq(0.0).all()\n",
    "    # フラグは欠損の数だけ1\n",
    "    assert int(out[\"hist_x__isna\"].sum()) == 2\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18222ea",
   "metadata": {},
   "source": [
    "## ▶️ Codeセル（pytest実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fc8fcfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                     [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /mnt/e/env/ts/tslib\n",
    "pytest -q tests/test_missing.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7b8d4488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m_________________ ERROR collecting tests/test_basic_checks.py __________________\u001b[0m\n",
      "\u001b[31m\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/python.py\u001b[0m:493: in importtestmodule\n",
      "    \u001b[0mmod = import_path(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/pathlib.py\u001b[0m:587: in import_path\n",
      "    \u001b[0mimportlib.import_module(module_name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1204: in _gcd_import\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1176: in _find_and_load\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1147: in _find_and_load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:690: in _load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/assertion/rewrite.py\u001b[0m:185: in exec_module\n",
      "    \u001b[0mexec(co, module.\u001b[91m__dict__\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_basic_checks.py\u001b[0m:5: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mquality\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbasic_checks\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m BasicQualityOptions, validate_basic_quality\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE     File \"/mnt/e/env/ts/tslib/src/nf_app/quality/basic_checks.py\", line 20\u001b[0m\n",
      "\u001b[1m\u001b[31mE       return ValueError(f\"{code}: {message}{ctx}\") from cause\u001b[0m\n",
      "\u001b[1m\u001b[31mE                                                    ^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE   SyntaxError: invalid syntax\u001b[0m\u001b[0m\n",
      "\u001b[31m\u001b[1m__________________ ERROR collecting tests/test_leak_audit.py ___________________\u001b[0m\n",
      "\u001b[31m\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/python.py\u001b[0m:493: in importtestmodule\n",
      "    \u001b[0mmod = import_path(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/pathlib.py\u001b[0m:587: in import_path\n",
      "    \u001b[0mimportlib.import_module(module_name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1204: in _gcd_import\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1176: in _find_and_load\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1147: in _find_and_load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:690: in _load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/assertion/rewrite.py\u001b[0m:185: in exec_module\n",
      "    \u001b[0mexec(co, module.\u001b[91m__dict__\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_leak_audit.py\u001b[0m:5: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mquality\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mleak_audit\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE     File \"/mnt/e/env/ts/tslib/src/nf_app/quality/leak_audit.py\", line 15\u001b[0m\n",
      "\u001b[1m\u001b[31mE       return ValueError(f\"{code}: {message}{ctx}\") from cause\u001b[0m\n",
      "\u001b[1m\u001b[31mE                                                    ^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE   SyntaxError: invalid syntax\u001b[0m\u001b[0m\n",
      "\u001b[31m\u001b[1m___________________ ERROR collecting tests/test_preflight.py ___________________\u001b[0m\n",
      "\u001b[31m\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/python.py\u001b[0m:493: in importtestmodule\n",
      "    \u001b[0mmod = import_path(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/pathlib.py\u001b[0m:587: in import_path\n",
      "    \u001b[0mimportlib.import_module(module_name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1204: in _gcd_import\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1176: in _find_and_load\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1147: in _find_and_load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:690: in _load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/assertion/rewrite.py\u001b[0m:185: in exec_module\n",
      "    \u001b[0mexec(co, module.\u001b[91m__dict__\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_preflight.py\u001b[0m:5: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mpreflight\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m run_preflight\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31msrc/nf_app/preflight.py\u001b[0m:9: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mquality\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbasic_checks\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m BasicQualityOptions, validate_basic_quality\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE     File \"/mnt/e/env/ts/tslib/src/nf_app/quality/basic_checks.py\", line 20\u001b[0m\n",
      "\u001b[1m\u001b[31mE       return ValueError(f\"{code}: {message}{ctx}\") from cause\u001b[0m\n",
      "\u001b[1m\u001b[31mE                                                    ^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE   SyntaxError: invalid syntax\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/test_basic_checks.py\n",
      "\u001b[31mERROR\u001b[0m tests/test_leak_audit.py\n",
      "\u001b[31mERROR\u001b[0m tests/test_preflight.py\n",
      "!!!!!!!!!!!!!!!!!!! Interrupted: 3 errors during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m\u001b[31m\u001b[1m3 errors\u001b[0m\u001b[31m in 0.26s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cd /mnt/e/env/ts/tslib\\n\\npytest -q tests/test_basic_checks.py tests/test_leak_audit.py tests/test_signature.py tests/test_preflight.py\\n'' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcd /mnt/e/env/ts/tslib\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mpytest -q tests/test_basic_checks.py tests/test_leak_audit.py tests/test_signature.py tests/test_preflight.py\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ts/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ts/lib/python3.11/site-packages/IPython/core/magics/script.py:160\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ts/lib/python3.11/site-packages/IPython/core/magics/script.py:348\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    345\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    346\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    347\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'cd /mnt/e/env/ts/tslib\\n\\npytest -q tests/test_basic_checks.py tests/test_leak_audit.py tests/test_signature.py tests/test_preflight.py\\n'' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /mnt/e/env/ts/tslib\n",
    "\n",
    "pytest -q tests/test_basic_checks.py tests/test_leak_audit.py tests/test_signature.py tests/test_preflight.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef380e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cb00dae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m_________________ ERROR collecting tests/test_basic_checks.py __________________\u001b[0m\n",
      "\u001b[31m\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/python.py\u001b[0m:493: in importtestmodule\n",
      "    \u001b[0mmod = import_path(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/pathlib.py\u001b[0m:587: in import_path\n",
      "    \u001b[0mimportlib.import_module(module_name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1204: in _gcd_import\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1176: in _find_and_load\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1147: in _find_and_load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:690: in _load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/assertion/rewrite.py\u001b[0m:185: in exec_module\n",
      "    \u001b[0mexec(co, module.\u001b[91m__dict__\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_basic_checks.py\u001b[0m:5: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mquality\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbasic_checks\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m BasicQualityOptions, validate_basic_quality\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE     File \"/mnt/e/env/ts/tslib/src/nf_app/quality/basic_checks.py\", line 20\u001b[0m\n",
      "\u001b[1m\u001b[31mE       return ValueError(f\"{code}: {message}{ctx}\") from cause\u001b[0m\n",
      "\u001b[1m\u001b[31mE                                                    ^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE   SyntaxError: invalid syntax\u001b[0m\u001b[0m\n",
      "\u001b[31m\u001b[1m__________________ ERROR collecting tests/test_leak_audit.py ___________________\u001b[0m\n",
      "\u001b[31m\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/python.py\u001b[0m:493: in importtestmodule\n",
      "    \u001b[0mmod = import_path(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/pathlib.py\u001b[0m:587: in import_path\n",
      "    \u001b[0mimportlib.import_module(module_name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1204: in _gcd_import\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1176: in _find_and_load\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1147: in _find_and_load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:690: in _load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/assertion/rewrite.py\u001b[0m:185: in exec_module\n",
      "    \u001b[0mexec(co, module.\u001b[91m__dict__\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_leak_audit.py\u001b[0m:5: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mquality\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mleak_audit\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE     File \"/mnt/e/env/ts/tslib/src/nf_app/quality/leak_audit.py\", line 15\u001b[0m\n",
      "\u001b[1m\u001b[31mE       return ValueError(f\"{code}: {message}{ctx}\") from cause\u001b[0m\n",
      "\u001b[1m\u001b[31mE                                                    ^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE   SyntaxError: invalid syntax\u001b[0m\u001b[0m\n",
      "\u001b[31m\u001b[1m___________________ ERROR collecting tests/test_preflight.py ___________________\u001b[0m\n",
      "\u001b[31m\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/python.py\u001b[0m:493: in importtestmodule\n",
      "    \u001b[0mmod = import_path(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/pathlib.py\u001b[0m:587: in import_path\n",
      "    \u001b[0mimportlib.import_module(module_name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1204: in _gcd_import\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1176: in _find_and_load\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1147: in _find_and_load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:690: in _load_unlocked\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/home/az/miniconda3/envs/ts/lib/python3.11/site-packages/_pytest/assertion/rewrite.py\u001b[0m:185: in exec_module\n",
      "    \u001b[0mexec(co, module.\u001b[91m__dict__\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_preflight.py\u001b[0m:5: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mpreflight\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m run_preflight\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31msrc/nf_app/preflight.py\u001b[0m:9: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mnf_app\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mquality\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbasic_checks\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m BasicQualityOptions, validate_basic_quality\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE     File \"/mnt/e/env/ts/tslib/src/nf_app/quality/basic_checks.py\", line 20\u001b[0m\n",
      "\u001b[1m\u001b[31mE       return ValueError(f\"{code}: {message}{ctx}\") from cause\u001b[0m\n",
      "\u001b[1m\u001b[31mE                                                    ^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE   SyntaxError: invalid syntax\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/test_basic_checks.py\n",
      "\u001b[31mERROR\u001b[0m tests/test_leak_audit.py\n",
      "\u001b[31mERROR\u001b[0m tests/test_preflight.py\n",
      "!!!!!!!!!!!!!!!!!!! Interrupted: 3 errors during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m\u001b[31m\u001b[1m3 errors\u001b[0m\u001b[31m in 0.26s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cd /mnt/e/env/ts/tslib\\n\\npytest -q tests/test_basic_checks.py tests/test_leak_audit.py tests/test_signature.py tests/test_preflight.py\\n'' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcd /mnt/e/env/ts/tslib\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mpytest -q tests/test_basic_checks.py tests/test_leak_audit.py tests/test_signature.py tests/test_preflight.py\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ts/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ts/lib/python3.11/site-packages/IPython/core/magics/script.py:160\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ts/lib/python3.11/site-packages/IPython/core/magics/script.py:348\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    345\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    346\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    347\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'cd /mnt/e/env/ts/tslib\\n\\npytest -q tests/test_basic_checks.py tests/test_leak_audit.py tests/test_signature.py tests/test_preflight.py\\n'' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /mnt/e/env/ts/tslib\n",
    "\n",
    "pytest -q tests/test_basic_checks.py tests/test_leak_audit.py tests/test_signature.py tests/test_preflight.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
